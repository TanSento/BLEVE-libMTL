{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import timeit\n",
    "#import shap\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read-in and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tank Failure Pressure (bar)</th>\n",
       "      <th>Liquid Ratio (%)</th>\n",
       "      <th>Tank Width (m)</th>\n",
       "      <th>Tank Length (m)</th>\n",
       "      <th>Tank Height (m)</th>\n",
       "      <th>Height of BLEVE (m)</th>\n",
       "      <th>Tank Height with Gas (m)</th>\n",
       "      <th>Vapour Temerature (K)</th>\n",
       "      <th>Liquid Temerature (K)</th>\n",
       "      <th>Status</th>\n",
       "      <th>Stand-off Distance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.010208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.012350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.014577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.016878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.019250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Tank Failure Pressure (bar)  Liquid Ratio (%)  Tank Width (m)  \\\n",
       "0  B1                         24.5          0.519805             2.2   \n",
       "1  B1                         24.5          0.519805             2.2   \n",
       "2  B1                         24.5          0.519805             2.2   \n",
       "3  B1                         24.5          0.519805             2.2   \n",
       "4  B1                         24.5          0.519805             2.2   \n",
       "\n",
       "   Tank Length (m)  Tank Height (m)  Height of BLEVE (m)  \\\n",
       "0              6.0              1.0                  1.6   \n",
       "1              6.0              1.0                  1.6   \n",
       "2              6.0              1.0                  1.6   \n",
       "3              6.0              1.0                  1.6   \n",
       "4              6.0              1.0                  1.6   \n",
       "\n",
       "   Tank Height with Gas (m)   Vapour Temerature (K)   Liquid Temerature (K)  \\\n",
       "0                       0.4                   307.8                   339.0   \n",
       "1                       0.4                   307.8                   339.0   \n",
       "2                       0.4                   307.8                   339.0   \n",
       "3                       0.4                   307.8                   339.0   \n",
       "4                       0.4                   307.8                   339.0   \n",
       "\n",
       "      Status  Stand-off Distance    Target  \n",
       "0  Subcooled                 5.0  0.010208  \n",
       "1  Subcooled                 6.0  0.012350  \n",
       "2  Subcooled                 7.0  0.014577  \n",
       "3  Subcooled                 8.0  0.016878  \n",
       "4  Subcooled                 9.0  0.019250  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"data/arrival_time_COMPLETE.csv\")\n",
    "df2 = pd.read_csv(\"data/negative_duration_COMPLETE.csv\")\n",
    "df3 = pd.read_csv(\"data/negative_peak_time_COMPLETE.csv\")\n",
    "df4 = pd.read_csv(\"data/negative_pressure_COMPLETE.csv\")\n",
    "df5 = pd.read_csv(\"data/positive_duration_COMPLETE.csv\")\n",
    "df6 = pd.read_csv(\"data/positive_impulse_COMPLETE.csv\")\n",
    "df7 = pd.read_csv(\"data/positive_peak_time_COMPLETE.csv\")\n",
    "df8 = pd.read_csv(\"data/positive_pressure_COMPLETE.csv\")\n",
    "\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tank Failure Pressure (bar)</th>\n",
       "      <th>Liquid Ratio (%)</th>\n",
       "      <th>Tank Width (m)</th>\n",
       "      <th>Tank Length (m)</th>\n",
       "      <th>Tank Height (m)</th>\n",
       "      <th>Height of BLEVE (m)</th>\n",
       "      <th>Tank Height with Gas (m)</th>\n",
       "      <th>Vapour Temerature (K)</th>\n",
       "      <th>Liquid Temerature (K)</th>\n",
       "      <th>Status</th>\n",
       "      <th>Stand-off Distance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.006817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.007816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.008326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.008817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35995</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35996</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35997</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35998</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Tank Failure Pressure (bar)  Liquid Ratio (%)  Tank Width (m)  \\\n",
       "0        B1                     24.50000          0.519805             2.2   \n",
       "1        B1                     24.50000          0.519805             2.2   \n",
       "2        B1                     24.50000          0.519805             2.2   \n",
       "3        B1                     24.50000          0.519805             2.2   \n",
       "4        B1                     24.50000          0.519805             2.2   \n",
       "...     ...                          ...               ...             ...   \n",
       "35995  P500                     11.40239          0.442321             1.4   \n",
       "35996  P500                     11.40239          0.442321             1.4   \n",
       "35997  P500                     11.40239          0.442321             1.4   \n",
       "35998  P500                     11.40239          0.442321             1.4   \n",
       "35999  P500                     11.40239          0.442321             1.4   \n",
       "\n",
       "       Tank Length (m)  Tank Height (m)  Height of BLEVE (m)  \\\n",
       "0                  6.0              1.0                  1.6   \n",
       "1                  6.0              1.0                  1.6   \n",
       "2                  6.0              1.0                  1.6   \n",
       "3                  6.0              1.0                  1.6   \n",
       "4                  6.0              1.0                  1.6   \n",
       "...                ...              ...                  ...   \n",
       "35995              2.8              1.2                  1.4   \n",
       "35996              2.8              1.2                  1.4   \n",
       "35997              2.8              1.2                  1.4   \n",
       "35998              2.8              1.2                  1.4   \n",
       "35999              2.8              1.2                  1.4   \n",
       "\n",
       "       Tank Height with Gas (m)   Vapour Temerature (K)  \\\n",
       "0                           0.4                   307.8   \n",
       "1                           0.4                   307.8   \n",
       "2                           0.4                   307.8   \n",
       "3                           0.4                   307.8   \n",
       "4                           0.4                   307.8   \n",
       "...                         ...                     ...   \n",
       "35995                       0.8                   388.2   \n",
       "35996                       0.8                   388.2   \n",
       "35997                       0.8                   388.2   \n",
       "35998                       0.8                   388.2   \n",
       "35999                       0.8                   388.2   \n",
       "\n",
       "        Liquid Temerature (K)  Status  Stand-off Distance    Target  \n",
       "0                       339.0       0                 5.0  0.006817  \n",
       "1                       339.0       0                 6.0  0.007302  \n",
       "2                       339.0       0                 7.0  0.007816  \n",
       "3                       339.0       0                 8.0  0.008326  \n",
       "4                       339.0       0                 9.0  0.008817  \n",
       "...                       ...     ...                 ...       ...  \n",
       "35995                   366.7       1                36.0       NaN  \n",
       "35996                   366.7       1                37.0       NaN  \n",
       "35997                   366.7       1                38.0       NaN  \n",
       "35998                   366.7       1                39.0       NaN  \n",
       "35999                   366.7       1                40.0       NaN  \n",
       "\n",
       "[36000 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding 'Status' feature into 0 and 1 \n",
    "# 0 for Subcooled and 1 for Superheated\n",
    "# Doing Similarly for ID (Do we need dummy encoding ??)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()\n",
    "\n",
    "df5['Status'] = LE.fit_transform(df5['Status'])\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tank Failure Pressure (bar)</th>\n",
       "      <th>Liquid Ratio (%)</th>\n",
       "      <th>Tank Width (m)</th>\n",
       "      <th>Tank Length (m)</th>\n",
       "      <th>Tank Height (m)</th>\n",
       "      <th>Height of BLEVE (m)</th>\n",
       "      <th>Tank Height with Gas (m)</th>\n",
       "      <th>Vapour Temerature (K)</th>\n",
       "      <th>Liquid Temerature (K)</th>\n",
       "      <th>Status</th>\n",
       "      <th>Stand-off Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28795</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28796</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28797</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28798</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28799</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28800 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tank Failure Pressure (bar)  Liquid Ratio (%)  Tank Width (m)  \\\n",
       "0                         24.50000          0.519805             2.2   \n",
       "1                         24.50000          0.519805             2.2   \n",
       "2                         24.50000          0.519805             2.2   \n",
       "3                         24.50000          0.519805             2.2   \n",
       "4                         24.50000          0.519805             2.2   \n",
       "...                            ...               ...             ...   \n",
       "28795                     33.17377          0.372041             1.0   \n",
       "28796                     33.17377          0.372041             1.0   \n",
       "28797                     33.17377          0.372041             1.0   \n",
       "28798                     33.17377          0.372041             1.0   \n",
       "28799                     33.17377          0.372041             1.0   \n",
       "\n",
       "       Tank Length (m)  Tank Height (m)  Height of BLEVE (m)  \\\n",
       "0                  6.0              1.0                  1.6   \n",
       "1                  6.0              1.0                  1.6   \n",
       "2                  6.0              1.0                  1.6   \n",
       "3                  6.0              1.0                  1.6   \n",
       "4                  6.0              1.0                  1.6   \n",
       "...                ...              ...                  ...   \n",
       "28795              2.2              0.6                  0.2   \n",
       "28796              2.2              0.6                  0.2   \n",
       "28797              2.2              0.6                  0.2   \n",
       "28798              2.2              0.6                  0.2   \n",
       "28799              2.2              0.6                  0.2   \n",
       "\n",
       "       Tank Height with Gas (m)   Vapour Temerature (K)  \\\n",
       "0                           0.4                   307.8   \n",
       "1                           0.4                   307.8   \n",
       "2                           0.4                   307.8   \n",
       "3                           0.4                   307.8   \n",
       "4                           0.4                   307.8   \n",
       "...                         ...                     ...   \n",
       "28795                       0.4                   312.7   \n",
       "28796                       0.4                   312.7   \n",
       "28797                       0.4                   312.7   \n",
       "28798                       0.4                   312.7   \n",
       "28799                       0.4                   312.7   \n",
       "\n",
       "        Liquid Temerature (K)  Status  Stand-off Distance  \n",
       "0                       339.0       0                 5.0  \n",
       "1                       339.0       0                 6.0  \n",
       "2                       339.0       0                 7.0  \n",
       "3                       339.0       0                 8.0  \n",
       "4                       339.0       0                 9.0  \n",
       "...                       ...     ...                 ...  \n",
       "28795                   318.2       0                36.0  \n",
       "28796                   318.2       0                37.0  \n",
       "28797                   318.2       0                38.0  \n",
       "28798                   318.2       0                39.0  \n",
       "28799                   318.2       0                40.0  \n",
       "\n",
       "[28800 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df5.drop(['ID','Target'], axis=1)[:28800]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.006817\n",
       "1        0.007302\n",
       "2        0.007816\n",
       "3        0.008326\n",
       "4        0.008817\n",
       "           ...   \n",
       "28795    0.012178\n",
       "28796    0.012275\n",
       "28797    0.012374\n",
       "28798    0.012477\n",
       "28799    0.012573\n",
       "Name: Target, Length: 28800, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y5 = df5['Target'][:28800]\n",
    "y5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df1['Target'][:28800]\n",
    "y2 = df2['Target'][:28800]\n",
    "y3 = df3['Target'][:28800]\n",
    "y4 = df4['Target'][:28800]\n",
    "y6 = df6['Target'][:28800]\n",
    "y7 = df7['Target'][:28800]\n",
    "y8 = df8['Target'][:28800]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 11)\n",
      "(7200, 11)\n"
     ]
    }
   ],
   "source": [
    "X_traindf, X_testdf, y1_train, y1_test = train_test_split(X, y1, test_size=0.25, random_state=42)\n",
    "print(X_traindf.shape)\n",
    "print(X_testdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_train, y2_test = train_test_split(y2, test_size=0.25, random_state=42)\n",
    "y3_train, y3_test = train_test_split(y3, test_size=0.25, random_state=42)\n",
    "y4_train, y4_test = train_test_split(y4, test_size=0.25, random_state=42)\n",
    "y5_train, y5_test = train_test_split(y5, test_size=0.25, random_state=42)\n",
    "y6_train, y6_test = train_test_split(y6, test_size=0.25, random_state=42)\n",
    "y7_train, y7_test = train_test_split(y7, test_size=0.25, random_state=42)\n",
    "y8_train, y8_test = train_test_split(y8, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01284534, 0.01444022, 0.10035855, ..., 0.10251021, 0.07990494,\n",
       "       0.01143898])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y7_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1268162 , 0.62633157, 0.04595783, ..., 0.02130297, 0.13602383,\n",
       "       2.3361142 ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y8_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((y1_train.values.reshape(-1,1), y2_train.values.reshape(-1,1), y3_train.values.reshape(-1,1), \n",
    "                          y4_train.values.reshape(-1,1), y5_train.values.reshape(-1,1), y6_train.values.reshape(-1,1),\n",
    "                          y7_train.values.reshape(-1,1), y8_train.values.reshape(-1,1)), axis=1)\n",
    "\n",
    "y_test = np.concatenate((y1_test.values.reshape(-1,1), y2_test.values.reshape(-1,1), y3_test.values.reshape(-1,1), \n",
    "                          y4_test.values.reshape(-1,1), y5_test.values.reshape(-1,1), y6_test.values.reshape(-1,1),\n",
    "                          y7_test.values.reshape(-1,1), y8_test.values.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 8)\n",
      "(7200, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.11694350e-02, 1.56393350e-02, 2.44314220e-02, ...,\n",
       "        2.86203890e+02, 1.28453400e-02, 1.12681620e+00],\n",
       "       [1.24119570e-02, 1.88029450e-02, 2.82499930e-02, ...,\n",
       "        1.80425900e+02, 1.44402250e-02, 6.26331570e-01],\n",
       "       [9.50448220e-02, 1.51018300e-02, 1.09446822e-01, ...,\n",
       "        1.49908640e+01, 1.00358550e-01, 4.59578340e-02],\n",
       "       ...,\n",
       "       [9.67233260e-02, 9.91223000e-03, 1.08929812e-01, ...,\n",
       "        7.44885020e+00, 1.02510210e-01, 2.13029660e-02],\n",
       "       [7.58939240e-02, 2.09730370e-02, 9.80206378e-02, ...,\n",
       "        6.19014550e+01, 7.99049360e-02, 1.36023830e-01],\n",
       "       [1.00712810e-02, 3.38855160e-02, 3.30022184e-02, ...,\n",
       "        7.96478820e+02, 1.14389770e-02, 2.33611420e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.36676340e-02, 1.85481800e-02, 1.16362342e-01, ...,\n",
       "        3.87540700e+01, 9.85093640e-02, 8.02679810e-02],\n",
       "       [1.88341180e-02, 1.49853250e-02, 3.32267860e-02, ...,\n",
       "        1.10768800e+02, 2.12428740e-02, 3.99188070e-01],\n",
       "       [4.82065450e-02, 2.26087420e-02, 7.29268908e-02, ...,\n",
       "        9.14076770e+01, 5.16740420e-02, 1.84897880e-01],\n",
       "       ...,\n",
       "       [7.48656170e-02, 1.68266440e-02, 9.27517236e-02, ...,\n",
       "        3.51032030e+01, 7.93257060e-02, 9.26706940e-02],\n",
       "       [1.48923780e-02, 2.82288120e-02, 3.50396958e-02, ...,\n",
       "        3.61880250e+02, 1.65773410e-02, 1.17031230e+00],\n",
       "       [4.29282640e-02, 7.86490400e-03, 5.24556696e-02, ...,\n",
       "        1.25165350e+01, 4.73894250e-02, 4.77122370e-02]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization and Power Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing both X_train and X_test using standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_traindf)\n",
    "X_test = scaler.transform(X_testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check if it succeeded\n",
    "# df_stdscal = pd.DataFrame(X_train)\n",
    "# df_stdscal.hist(figsize = (20,20), bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "y_train_normal = quantile.fit_transform(y_train)\n",
    "y_test_normal = quantile.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0197092 ,  0.33428349, -0.71517926,  2.24232066,  1.86359402,\n",
       "       -0.74523355, -0.4648274 , -0.2268882 , -0.75649043, -2.09079023,\n",
       "       -0.55463441,  0.75038709, -0.03604441,  0.25850177,  1.95536698,\n",
       "        0.14818542,  0.53428356, -0.76673673,  0.53672479, -0.23066606,\n",
       "       -0.14865688,  0.41007522,  1.43976542,  0.18550623, -0.37067758,\n",
       "       -0.82626052,  1.35012437, -1.40802512, -2.21909141, -0.44441688,\n",
       "       -0.75462019,  1.19340742,  0.80011561,  0.54253131,  0.33034992,\n",
       "        1.98874773,  1.62697118, -0.53849423,  0.35250048,  2.31339542])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_normal[1:41, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39918807, 0.18489788, 0.07076792, 1.603528  , 1.0806105 ,\n",
       "       0.06908134, 0.08818348, 0.10897039, 0.06826332, 0.01860524,\n",
       "       0.08134046, 0.28987974, 0.12975076, 0.17156978, 1.2115381 ,\n",
       "       0.15436813, 0.22705919, 0.06763056, 0.22765867, 0.10868868,\n",
       "       0.11713055, 0.19952142, 0.66621888, 0.15975925, 0.09596858,\n",
       "       0.06414993, 0.59720933, 0.03660972, 0.01633277, 0.08981635,\n",
       "       0.06839496, 0.49123496, 0.30692583, 0.2290165 , 0.18374079,\n",
       "       1.2546521 , 0.83790588, 0.08270955, 0.18826026, 1.7111793 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_check = quantile.inverse_transform(y_test_normal)\n",
    "y_test_check[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39918807, 0.18489788, 0.07076792, 1.603528  , 1.0806105 ,\n",
       "       0.06908134, 0.08818348, 0.10897039, 0.06826332, 0.01860524,\n",
       "       0.08134046, 0.28987974, 0.12975076, 0.17156978, 1.2115381 ,\n",
       "       0.15436813, 0.22705919, 0.06763056, 0.22765867, 0.10868868,\n",
       "       0.11713055, 0.19952142, 0.66621888, 0.15975925, 0.09596858,\n",
       "       0.06414993, 0.59720933, 0.03660972, 0.01633277, 0.08981635,\n",
       "       0.06839496, 0.49123496, 0.30692583, 0.2290165 , 0.18374079,\n",
       "       1.2546521 , 0.83790588, 0.08270955, 0.18826026, 1.7111793 ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check if it succeeded\n",
    "# df_stdscal = pd.DataFrame(y_train)\n",
    "# df_stdscal.hist(figsize = (20,20), bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.backends.cudnn.version() , torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1781, -0.9747, -1.5140,  ..., -0.7230, -0.7424, -1.4851],\n",
      "        [-0.5207,  0.5219,  0.0684,  ..., -0.3853,  1.3470, -1.4851],\n",
      "        [ 1.3911,  1.5275, -0.7228,  ...,  1.1182, -0.7424,  1.3998],\n",
      "        ...,\n",
      "        [-1.6514,  0.5764, -1.2503,  ...,  0.7199,  1.3470,  1.3998],\n",
      "        [ 0.9835,  0.0136, -0.7228,  ..., -0.4908, -0.7424,  0.9190],\n",
      "        [ 1.3715,  0.0745,  0.3321,  ...,  0.5537,  1.3470, -1.4851]])\n"
     ]
    }
   ],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "X_train_torch = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test_torch = torch.from_numpy(X_test.astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "y_train_torch = torch.from_numpy(y_train_normal.astype(np.float32))\n",
    "y_test_torch = torch.from_numpy(y_test_normal.astype(np.float32))\n",
    "\n",
    "\n",
    "print(X_train_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7290, -0.1825, -1.7822,  ...,  1.7153, -1.7731,  1.8945],\n",
       "        [-1.5605,  0.3168, -1.4323,  ...,  1.2409, -1.5663,  1.3875],\n",
       "        [ 1.4714, -0.2850,  1.1074,  ..., -1.2251,  1.4889, -1.1817],\n",
       "        ...,\n",
       "        [ 1.5913, -1.5754,  1.0800,  ..., -1.9514,  1.6349, -1.9590],\n",
       "        [ 0.6395,  0.5629,  0.6927,  ...,  0.1485,  0.6236,  0.0151],\n",
       "        [-1.9274,  2.0287, -1.1576,  ...,  2.8862, -2.0276,  2.6981]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3816,  0.2827,  1.4829,  ..., -0.3075,  1.3684, -0.5699],\n",
       "        [-1.0928, -0.3107, -1.1451,  ...,  0.7363, -1.0969,  1.0197],\n",
       "        [-0.1113,  0.7614,  0.0336,  ...,  0.5406, -0.1180,  0.3343],\n",
       "        ...,\n",
       "        [ 0.6086,  0.0336,  0.5384,  ..., -0.4086,  0.6064, -0.4105],\n",
       "        [-1.3440,  1.4611, -1.0647,  ...,  1.9779, -1.3778,  1.9265],\n",
       "        [-0.2490, -2.1335, -0.4783,  ..., -1.3961, -0.2252, -1.1426]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(X_train_torch)):\n",
    "   train_data.append([X_train_torch[i],\n",
    "                      y_train_torch[i] \n",
    "                     ])\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(X_test_torch)):\n",
    "   test_data.append([X_test_torch[i], \n",
    "                     y_test_torch[i]\n",
    "                     ])\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=512,               # batch_size could be smaller\n",
    "    num_workers=0)                                                                   # Increasing num_workers slow down the training because it does not use GPU at all\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=512,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7290, -0.1825, -1.7822,  ...,  1.7153, -1.7731,  1.8945],\n",
      "        [-1.5605,  0.3168, -1.4323,  ...,  1.2409, -1.5663,  1.3875],\n",
      "        [ 1.4714, -0.2850,  1.1074,  ..., -1.2251,  1.4889, -1.1817],\n",
      "        ...,\n",
      "        [ 0.8358, -0.0945,  0.8934,  ..., -0.7314,  0.8505, -1.0485],\n",
      "        [-0.8206,  0.8322, -0.6906,  ...,  1.3263, -0.8403,  1.2101],\n",
      "        [ 0.2861,  1.0838,  0.4119,  ...,  0.6527,  0.2623,  0.4181]])\n",
      "tensor([[-0.5300, -2.0834, -0.7918,  ..., -0.8972, -0.5132, -0.4180],\n",
      "        [-0.9009, -1.6717, -1.1690,  ..., -0.1349, -0.8938,  0.5568],\n",
      "        [ 1.4466, -0.0538,  1.5691,  ..., -0.8411,  1.4696, -1.2969],\n",
      "        ...,\n",
      "        [ 1.3805,  0.5386,  1.6099,  ..., -0.4543,  1.3940, -0.9163],\n",
      "        [ 2.4348, -0.6237,  1.6922,  ..., -2.1980,  2.4790, -2.2859],\n",
      "        [-0.1717,  0.7099, -0.0818,  ...,  0.8580, -0.1884,  0.6773]])\n",
      "tensor([[ 0.2020, -0.7115,  0.1271,  ..., -0.5994,  0.2110, -0.5918],\n",
      "        [-0.0059, -0.2115, -0.0668,  ..., -0.0512, -0.0085,  0.0518],\n",
      "        [-1.1355, -0.5121, -1.2322,  ...,  0.3827, -1.1245,  0.7504],\n",
      "        ...,\n",
      "        [-0.9340, -0.2810, -1.0251,  ...,  1.0621, -0.9535,  1.3148],\n",
      "        [ 0.1063,  0.2683,  0.1440,  ..., -0.0209,  0.1085, -0.2021],\n",
      "        [-2.6043,  0.3627, -2.1220,  ...,  1.7895, -2.6160,  2.1416]])\n",
      "tensor([[-0.3388,  1.8408, -0.1090,  ...,  1.3557, -0.3607,  0.9889],\n",
      "        [ 1.3040,  1.9205,  3.1201,  ...,  0.7790,  1.2458,  0.1796],\n",
      "        [-0.6991,  1.7344, -0.4734,  ...,  1.8379, -0.7287,  1.5371],\n",
      "        ...,\n",
      "        [ 1.0664,  0.9639,  1.3039,  ...,  0.1256,  1.0458, -0.1783],\n",
      "        [-1.6152,  0.5406, -1.4470,  ...,  1.2873, -1.6243,  1.5087],\n",
      "        [-0.8205, -1.0797, -0.9529,  ..., -0.1304, -0.8038,  0.2629]])\n",
      "tensor([[-0.1871,  0.1163, -0.2030,  ...,  0.3484, -0.1953,  0.4197],\n",
      "        [-1.9282, -1.7085, -2.7503,  ..., -0.2147, -1.8141,  0.5657],\n",
      "        [ 1.6561, -1.7856,  1.1190,  ..., -2.4656,  1.7176, -2.5107],\n",
      "        ...,\n",
      "        [ 0.1240, -0.1066,  0.0865,  ..., -0.5805,  0.1322, -0.5656],\n",
      "        [-1.1004, -0.5606, -1.3509,  ..., -0.8110, -1.0557, -0.2400],\n",
      "        [-2.3294, -1.0607, -2.6494,  ...,  0.9331, -2.2621,  1.4495]])\n",
      "tensor([[-1.2725, -1.3560, -1.4715,  ..., -0.4136, -1.2265,  0.2324],\n",
      "        [ 0.2726, -0.1055,  0.2604,  ..., -0.3200,  0.2767, -0.4037],\n",
      "        [ 0.0784, -0.8434, -0.0943,  ..., -1.1949,  0.0973, -1.0764],\n",
      "        ...,\n",
      "        [ 1.2081,  0.9339,  1.4344,  ..., -0.0339,  1.1875, -0.3515],\n",
      "        [ 1.1289,  0.6457,  1.3153,  ..., -0.1511,  1.1182, -0.5080],\n",
      "        [-1.1617,  1.2852, -0.9418,  ...,  2.0244, -1.2031,  1.9376]])\n",
      "tensor([[ 1.3987, -0.1906,  1.2403,  ..., -0.6114,  1.3924, -0.7222],\n",
      "        [-1.6078, -1.1778, -2.0383,  ...,  0.6294, -1.5989,  1.2185],\n",
      "        [ 0.4551, -0.6526,  0.3639,  ..., -0.7559,  0.4651, -0.7714],\n",
      "        ...,\n",
      "        [-0.3549,  0.6266, -0.2641,  ...,  0.3552, -0.3575,  0.3595],\n",
      "        [ 0.6362, -0.3029,  0.6720,  ..., -0.6820,  0.6502, -0.9269],\n",
      "        [ 2.4475, -0.4124,  2.3141,  ..., -1.0381,  2.4183, -1.3041]])\n",
      "tensor([[ 2.9108, -0.6557,  1.9890,  ..., -1.8382,  2.9180, -1.9071],\n",
      "        [ 0.1885,  0.1365,  0.2005,  ..., -0.1354,  0.1922, -0.2667],\n",
      "        [ 0.1587, -1.0545,  0.0785,  ..., -0.7173,  0.1676, -0.5293],\n",
      "        ...,\n",
      "        [ 1.4645,  0.9325,  1.8647,  ...,  0.1921,  1.4132, -0.1190],\n",
      "        [ 1.2970, -1.3034,  0.9128,  ..., -1.5339,  1.3310, -1.4931],\n",
      "        [-2.5630,  0.0574, -2.1333,  ...,  2.0786, -2.5857,  2.3142]])\n",
      "tensor([[ 1.6956, -0.4829,  1.4646,  ..., -0.9308,  1.6994, -1.0523],\n",
      "        [-0.1497,  0.5208, -0.0723,  ...,  0.1572, -0.1504,  0.0776],\n",
      "        [ 0.8126,  0.7227,  0.9107,  ...,  0.1571,  0.7949, -0.0750],\n",
      "        ...,\n",
      "        [-1.4106,  0.5814, -1.2664,  ...,  1.4496, -1.4338,  1.5557],\n",
      "        [-0.4861,  0.0064, -0.5092,  ...,  0.5575, -0.4952,  0.6921],\n",
      "        [-0.0593,  1.3126,  0.1377,  ...,  0.7906, -0.0698,  0.4424]])\n",
      "tensor([[ 0.2499,  1.2286,  0.4382,  ...,  0.7003,  0.2346,  0.3597],\n",
      "        [ 0.0554,  0.3985,  0.0756,  ...,  0.2358,  0.0460,  0.2188],\n",
      "        [ 0.3039,  0.7025,  0.3991,  ...,  0.2370,  0.2965,  0.0300],\n",
      "        ...,\n",
      "        [ 2.4251, -0.3371,  1.7513,  ..., -1.8156,  2.4546, -1.9481],\n",
      "        [-1.1340, -1.8389, -1.4767,  ...,  0.0348, -1.1220,  0.7326],\n",
      "        [-0.3463,  0.8507, -0.2517,  ...,  0.7581, -0.3583,  0.6651]])\n",
      "tensor([[ 0.8210, -0.8412,  0.5857,  ..., -1.6604,  0.8473, -1.6533],\n",
      "        [-0.1879,  1.2487, -0.0198,  ...,  0.8996, -0.2003,  0.6589],\n",
      "        [-2.0655, -0.8292, -2.3056,  ...,  0.7633, -1.9969,  1.2946],\n",
      "        ...,\n",
      "        [-1.6783,  0.4750, -1.5093,  ...,  1.5783, -1.7044,  1.7084],\n",
      "        [-0.1056, -0.3117, -0.1796,  ..., -0.1739, -0.1070,  0.0093],\n",
      "        [ 0.1554, -0.1702,  0.2061,  ..., -0.4836,  0.1682, -0.7610]])\n",
      "tensor([[-0.3311,  0.0352, -0.3874,  ...,  0.1207, -0.3339,  0.3569],\n",
      "        [-0.7734, -0.4677, -0.8631,  ...,  0.1763, -0.7687,  0.4988],\n",
      "        [-0.3564, -0.3715, -0.4202,  ...,  0.1869, -0.3576,  0.3620],\n",
      "        ...,\n",
      "        [ 0.5621, -0.4899,  0.5723,  ..., -0.7196,  0.5749, -0.9512],\n",
      "        [ 1.2769,  2.4338,  3.3132,  ...,  0.7417,  1.2302,  0.0962],\n",
      "        [ 0.9675,  0.2587,  1.1096,  ..., -0.5231,  0.9795, -0.9729]])\n",
      "tensor([[-1.3526, -0.8438, -1.5422,  ...,  1.0014, -1.3545,  1.3262],\n",
      "        [-0.9778, -0.0806, -0.9928,  ...,  0.6604, -0.9795,  0.9031],\n",
      "        [ 1.4065, -1.0322,  1.0220,  ..., -2.4085,  1.4624, -2.4920],\n",
      "        ...,\n",
      "        [ 0.1681,  0.0612,  0.1415,  ...,  0.1231,  0.1593,  0.1570],\n",
      "        [ 1.3938,  1.6960,  2.3638,  ...,  0.4070,  1.3454, -0.0662],\n",
      "        [-0.0358, -0.6146, -0.1862,  ..., -0.9617, -0.0163, -0.8670]])\n",
      "tensor([[-0.2956,  0.5126, -0.2530,  ...,  0.6693, -0.3073,  0.6376],\n",
      "        [ 0.2885,  0.9838,  0.4047,  ...,  0.3309,  0.2779,  0.0893],\n",
      "        [-0.6847, -1.2670, -0.8223,  ..., -0.1359, -0.6732,  0.3087],\n",
      "        ...,\n",
      "        [ 1.1546,  1.3612,  1.7817,  ...,  0.2849,  1.1379, -0.2696],\n",
      "        [ 1.0155,  0.5902,  1.1100,  ...,  0.0091,  0.9966, -0.2061],\n",
      "        [ 0.7617,  0.2805,  0.7904,  ..., -0.2251,  0.7570, -0.4078]])\n",
      "tensor([[ 0.3645,  1.6741,  0.5935,  ...,  0.6998,  0.3454,  0.3029],\n",
      "        [-1.4216,  0.9653, -1.1827,  ...,  1.8926, -1.4608,  1.8802],\n",
      "        [-0.6218, -0.6597, -0.7169,  ...,  0.2009, -0.6208,  0.5263],\n",
      "        ...,\n",
      "        [ 0.9638,  0.1302,  1.0033,  ..., -0.3687,  0.9630, -0.6595],\n",
      "        [-1.0725,  0.4626, -0.9924,  ...,  1.1670, -1.0820,  1.2211],\n",
      "        [-1.3247, -1.8796, -1.7083,  ...,  0.5978, -1.3119,  1.1039]])\n",
      "tensor([[ 0.0109, -0.5515, -0.1451,  ..., -0.8986,  0.0241, -0.7559],\n",
      "        [-0.6323,  0.2752, -0.6247,  ...,  0.5706, -0.6372,  0.6941],\n",
      "        [-0.0937, -0.5692, -0.2472,  ..., -1.4528, -0.0692, -1.3582],\n",
      "        ...,\n",
      "        [-0.5550, -0.4353, -0.6315,  ...,  0.4876, -0.5656,  0.7338],\n",
      "        [ 0.1660,  0.5654,  0.2624,  ...,  0.1290,  0.1676, -0.1125],\n",
      "        [ 1.1155,  2.6541,  2.1414,  ...,  0.8451,  1.0611,  0.3034]])\n",
      "tensor([[-1.3536, -2.3127, -2.0706,  ..., -0.2477, -1.3160,  0.5329],\n",
      "        [-0.6874,  0.3041, -0.6077,  ...,  0.2375, -0.6780,  0.2910],\n",
      "        [-0.0970, -0.5311, -0.1746,  ..., -0.1782, -0.0989,  0.0345],\n",
      "        ...,\n",
      "        [ 1.0591,  0.2038,  1.0540,  ..., -0.4803,  1.0622, -0.6976],\n",
      "        [-0.7811,  0.1883, -0.7849,  ...,  0.9757, -0.7939,  1.0631],\n",
      "        [ 0.5676,  0.6880,  0.7632,  ...,  0.1852,  0.5619, -0.2042]])\n",
      "tensor([[-2.7449,  1.0714, -1.6729,  ...,  2.5175, -2.7883,  2.5617],\n",
      "        [-0.7358,  0.6982, -0.5924,  ...,  0.8409, -0.7414,  0.7893],\n",
      "        [ 2.2284, -1.8601,  1.3913,  ..., -1.8968,  2.2570, -1.9007],\n",
      "        ...,\n",
      "        [-1.0721, -0.2398, -1.0838,  ...,  0.2576, -1.0570,  0.6189],\n",
      "        [-0.0409, -1.4045, -0.1648,  ..., -0.7006, -0.0356, -0.3591],\n",
      "        [-0.5866, -0.4511, -0.7580,  ..., -1.1064, -0.5595, -0.7678]])\n",
      "tensor([[-1.8517, -1.5220, -2.3642,  ...,  1.1435, -1.8639,  1.6400],\n",
      "        [-0.4773, -0.8146, -0.5913,  ...,  0.0347, -0.4807,  0.4003],\n",
      "        [-0.0383,  0.6963,  0.0240,  ...,  0.6592, -0.0541,  0.5716],\n",
      "        ...,\n",
      "        [ 0.8727,  0.8063,  1.0690,  ...,  0.0034,  0.8672, -0.3919],\n",
      "        [-1.2411, -0.3331, -1.3086,  ...,  0.6920, -1.2432,  1.0278],\n",
      "        [-0.1524, -1.9586, -0.3770,  ..., -0.9631, -0.1365, -0.5815]])\n",
      "tensor([[-1.9343,  0.0310, -1.9219,  ...,  0.8769, -1.9070,  1.3280],\n",
      "        [ 0.6895, -0.0666,  0.6209,  ..., -0.5262,  0.6915, -0.6318],\n",
      "        [ 0.3113, -0.0434,  0.3006,  ..., -0.2133,  0.3138, -0.2939],\n",
      "        ...,\n",
      "        [-0.2140, -0.5177, -0.2119,  ..., -0.7401, -0.1942, -0.8144],\n",
      "        [-0.3057, -0.6042, -0.3980,  ..., -0.2793, -0.3018, -0.0045],\n",
      "        [ 0.3184, -1.0200,  0.2079,  ..., -0.8826,  0.3288, -0.8324]])\n",
      "tensor([[-1.6011, -1.0743, -2.0459,  ...,  1.1845, -1.6279,  1.6971],\n",
      "        [ 0.7208, -0.3472,  0.5241,  ..., -0.9735,  0.7260, -0.7814],\n",
      "        [-0.2238, -0.7610, -0.2214,  ..., -0.8552, -0.2003, -0.9458],\n",
      "        ...,\n",
      "        [ 0.5886, -0.2630,  0.5064,  ..., -0.5731,  0.5918, -0.6273],\n",
      "        [-1.4981, -0.5277, -1.9166,  ..., -0.5201, -1.4292,  0.2520],\n",
      "        [-0.8545,  0.1763, -0.8291,  ...,  0.7497, -0.8625,  0.9123]])\n",
      "tensor([[-0.0228, -0.0502,  0.0348,  ..., -0.6179, -0.0030, -0.9238],\n",
      "        [ 0.9924,  0.3997,  1.0289,  ..., -0.3290,  0.9839, -0.5059],\n",
      "        [ 1.2853,  0.2660,  1.2340,  ..., -0.5157,  1.2885, -0.7205],\n",
      "        ...,\n",
      "        [-1.9209, -0.4898, -2.0058,  ...,  1.1784, -1.9006,  1.4399],\n",
      "        [-1.0356, -1.4830, -1.2831,  ...,  0.3070, -1.0342,  0.8982],\n",
      "        [ 1.1225,  1.1753,  1.5593,  ...,  0.3579,  1.0869, -0.0088]])\n",
      "tensor([[ 0.1340,  1.0373,  0.2574,  ...,  0.5575,  0.1213,  0.3411],\n",
      "        [-0.7614,  3.0974, -0.3176,  ...,  2.3485, -0.7968,  1.7527],\n",
      "        [ 2.5236,  0.2355,  1.9583,  ..., -1.5412,  2.5268, -1.7183],\n",
      "        ...,\n",
      "        [-0.7123, -0.3394, -0.7584,  ..., -0.0350, -0.7026,  0.2960],\n",
      "        [-0.7385, -0.2835, -0.7326,  ...,  0.1151, -0.7303,  0.3781],\n",
      "        [-0.8367,  1.5030, -0.5805,  ...,  1.4370, -0.8522,  1.1938]])\n",
      "tensor([[ 0.0234, -1.4986, -0.1748,  ..., -2.8211,  0.0491, -2.7541],\n",
      "        [-0.5151, -0.1739, -0.4993,  ...,  0.0301, -0.5029,  0.0813],\n",
      "        [ 0.2624, -0.4660,  0.1022,  ..., -1.9270,  0.2889, -1.8378],\n",
      "        ...,\n",
      "        [-0.1180,  0.6770, -0.0487,  ...,  0.7993, -0.1330,  0.6738],\n",
      "        [-0.7001,  0.0368, -0.7140,  ...,  0.7437, -0.7092,  0.8648],\n",
      "        [-0.7296,  1.3291, -0.5445,  ...,  1.5523, -0.7542,  1.3461]])\n",
      "tensor([[ 1.2966, -0.4327,  0.9659,  ..., -1.2194,  1.3122, -1.1380],\n",
      "        [-0.5392,  0.6357, -0.4519,  ...,  0.7883, -0.5476,  0.7229],\n",
      "        [-0.8394, -0.7186, -0.9089,  ...,  0.0401, -0.8276,  0.4103],\n",
      "        ...,\n",
      "        [ 1.2299,  0.0705,  1.1343,  ..., -0.5635,  1.2614, -0.6939],\n",
      "        [-1.3373, -0.5387, -1.3918,  ..., -0.3802, -1.2787,  0.1003],\n",
      "        [-0.4754,  0.7026, -0.3592,  ...,  0.7078, -0.4837,  0.6531]])\n",
      "tensor([[-0.4533,  0.7089, -0.3542,  ...,  1.0188, -0.4684,  0.8601],\n",
      "        [ 0.6210,  0.3359,  0.4800,  ..., -0.9799,  0.6345, -0.9340],\n",
      "        [-1.9078,  1.2272, -1.4182,  ...,  2.6316, -2.0394,  2.8650],\n",
      "        ...,\n",
      "        [ 0.0984, -0.1531,  0.0377,  ..., -0.3018,  0.1020, -0.2506],\n",
      "        [-0.9077, -0.7722, -1.0331,  ...,  0.3385, -0.9059,  0.7163],\n",
      "        [ 1.9929,  0.4469,  2.4168,  ..., -0.2497,  1.9340, -0.5447]])\n",
      "tensor([[-0.6588,  0.3586, -0.6072,  ...,  0.6019, -0.6587,  0.6200],\n",
      "        [-0.9232, -1.0259, -1.0210,  ..., -0.5158, -0.8933, -0.0904],\n",
      "        [ 0.4760,  1.9392,  0.8902,  ...,  1.0221,  0.4517,  0.4648],\n",
      "        ...,\n",
      "        [ 0.7508, -0.8698,  0.6291,  ..., -0.9854,  0.7667, -1.0532],\n",
      "        [ 0.8975,  0.2540,  0.8455,  ..., -0.2783,  0.8838, -0.3669],\n",
      "        [-1.3066,  0.1631, -1.3178,  ...,  1.0679, -1.3182,  1.3275]])\n",
      "tensor([[-1.1455,  2.5219, -0.7272,  ...,  2.2976, -1.1905,  2.0308],\n",
      "        [ 1.5139, -0.1603,  1.1722,  ..., -1.4231,  1.5599, -1.6342],\n",
      "        [-0.3655,  0.4244, -0.3028,  ...,  0.5995, -0.3748,  0.5775],\n",
      "        ...,\n",
      "        [-1.1985, -1.1113, -1.3541,  ...,  0.2917, -1.1777,  0.7361],\n",
      "        [ 0.7978,  0.5354,  0.8492,  ...,  0.1760,  0.7736,  0.0314],\n",
      "        [-0.0095,  1.7559,  0.2022,  ...,  1.0479, -0.0313,  0.7121]])\n",
      "tensor([[ 1.2398,  1.4567,  1.8545,  ...,  0.5062,  1.1894,  0.1060],\n",
      "        [-0.3397, -0.5375, -0.3296,  ..., -0.3499, -0.3271, -0.3199],\n",
      "        [ 0.3544, -1.4756,  0.1293,  ..., -1.1578,  0.3634, -0.9354],\n",
      "        ...,\n",
      "        [ 0.0645,  0.6035,  0.1616,  ...,  0.2600,  0.0579,  0.0604],\n",
      "        [-1.0365,  0.1375, -1.0373,  ...,  1.0754, -1.0503,  1.2267],\n",
      "        [ 1.9399, -0.2856,  1.3993,  ..., -1.2783,  1.9348, -1.2506]])\n",
      "tensor([[-0.3776,  1.8972, -0.1188,  ...,  1.4409, -0.3984,  1.0265],\n",
      "        [ 0.5235,  0.2052,  0.5647,  ..., -0.4502,  0.5271, -0.6387],\n",
      "        [ 2.2399, -1.4905,  1.4250,  ..., -1.7124,  2.2563, -1.6925],\n",
      "        ...,\n",
      "        [ 0.4465, -0.2122,  0.4591,  ..., -0.5625,  0.4559, -0.7951],\n",
      "        [ 1.3515, -0.3922,  1.0018,  ..., -1.0877,  1.3511, -0.9489],\n",
      "        [-1.9645,  0.5690, -1.6747,  ...,  1.7252, -2.0140,  1.9365]])\n",
      "tensor([[-1.2568,  0.9292, -1.0722,  ...,  1.8521, -1.2925,  1.8287],\n",
      "        [-0.0837, -1.3824, -0.1914,  ..., -0.8636, -0.0679, -0.7247],\n",
      "        [ 0.1163, -1.6361, -0.0875,  ..., -1.5605,  0.1400, -1.4112],\n",
      "        ...,\n",
      "        [ 0.7445,  0.9029,  0.8845,  ...,  0.1472,  0.7316, -0.0863],\n",
      "        [ 0.2592,  1.3346,  0.4509,  ...,  0.6333,  0.2426,  0.3510],\n",
      "        [ 0.9952,  1.1861,  1.3537,  ...,  0.5582,  0.9552,  0.1854]])\n",
      "tensor([[-1.1037,  2.3122, -0.6687,  ...,  2.1339, -1.1344,  1.7435],\n",
      "        [-1.0701,  0.1375, -1.0019,  ..., -0.1202, -1.0372,  0.1925],\n",
      "        [-0.1166, -0.5381, -0.2060,  ...,  0.0236, -0.1229,  0.3116],\n",
      "        ...,\n",
      "        [ 1.2650,  0.7816,  1.6290,  ...,  0.1509,  1.2365, -0.2484],\n",
      "        [-2.0744,  0.3092, -1.7118,  ...,  1.0170, -1.9608,  1.2683],\n",
      "        [-0.7960, -2.2102, -1.1113,  ..., -2.4022, -0.7546, -2.0563]])\n",
      "tensor([[ 0.9125, -1.0888,  0.6487,  ..., -2.6770,  0.9461, -2.6940],\n",
      "        [-0.4845, -0.6106, -0.5481,  ..., -0.4050, -0.4691, -0.2535],\n",
      "        [ 0.0830,  0.7796,  0.2321,  ...,  0.2735,  0.0791, -0.0334],\n",
      "        ...,\n",
      "        [ 0.3656,  1.3322,  0.5781,  ...,  0.5506,  0.3508,  0.1735],\n",
      "        [-0.2872, -1.4899, -0.3979,  ..., -0.9053, -0.2647, -0.7499],\n",
      "        [ 0.9368,  0.3363,  1.0423,  ..., -0.1402,  0.9327, -0.4787]])\n",
      "tensor([[ 0.5664, -0.1955,  0.5055,  ..., -0.6010,  0.5725, -0.7416],\n",
      "        [-0.0591, -1.1481, -0.1753,  ..., -0.3044, -0.0618,  0.0561],\n",
      "        [-0.3047, -0.1803, -0.3050,  ...,  0.0794, -0.3019,  0.0889],\n",
      "        ...,\n",
      "        [-0.5771, -0.5558, -0.7503,  ..., -0.8883, -0.5506, -0.5896],\n",
      "        [ 0.1995,  1.0592,  0.3328,  ...,  0.6990,  0.1807,  0.4536],\n",
      "        [-0.8628,  1.6318, -0.6090,  ...,  1.6820, -0.8879,  1.4327]])\n",
      "tensor([[ 0.8998,  0.2506,  0.8363,  ..., -0.5348,  0.8962, -0.5768],\n",
      "        [ 1.3773,  1.3054,  2.2117,  ...,  0.3177,  1.3346, -0.1334],\n",
      "        [-0.5534, -0.9646, -0.7432,  ..., -0.9317, -0.5337, -0.5247],\n",
      "        ...,\n",
      "        [-1.3392,  1.7097, -1.0125,  ...,  2.1613, -1.3771,  2.0542],\n",
      "        [ 0.6205, -0.0532,  0.5608,  ..., -0.2318,  0.6152, -0.2900],\n",
      "        [-2.8540, -1.3572, -3.2568,  ...,  1.2722, -2.8256,  1.8692]])\n",
      "tensor([[-0.8764,  0.4935, -0.7919,  ...,  1.2241, -0.8902,  1.1860],\n",
      "        [ 0.2573,  0.0419,  0.2198,  ..., -0.5134,  0.2641, -0.5961],\n",
      "        [-0.0150, -1.0052, -0.1912,  ..., -1.1593,  0.0020, -0.9339],\n",
      "        ...,\n",
      "        [ 0.0489, -0.8514, -0.1199,  ..., -0.9514,  0.0574, -0.6525],\n",
      "        [-0.2718,  0.5988, -0.1724,  ...,  0.5222, -0.2743,  0.3662],\n",
      "        [-1.2116,  1.4211, -0.9190,  ...,  1.9372, -1.2408,  1.7365]])\n",
      "tensor([[-1.3093, -0.4090, -1.4663,  ...,  1.4815, -1.3480,  1.7895],\n",
      "        [ 0.4268,  0.6519,  0.4957,  ...,  0.3520,  0.4104,  0.1859],\n",
      "        [ 0.2061, -0.9045,  0.0264,  ..., -1.0531,  0.2179, -0.8175],\n",
      "        ...,\n",
      "        [ 1.4597, -2.3226,  0.9530,  ..., -2.2896,  1.5098, -2.2860],\n",
      "        [-0.6347, -1.5841, -0.8085,  ...,  0.0487, -0.6353,  0.5653],\n",
      "        [ 1.2377, -0.3113,  1.3093,  ..., -0.6831,  1.2499, -1.0037]])\n",
      "tensor([[ 1.7941, -1.2752,  1.2149,  ..., -1.6552,  1.8181, -1.6271],\n",
      "        [ 0.8221, -0.1986,  0.7246,  ..., -0.6402,  0.8259, -0.6652],\n",
      "        [ 0.1412,  0.7628,  0.2843,  ...,  0.3068,  0.1378, -0.0271],\n",
      "        ...,\n",
      "        [ 0.3226, -1.9781,  0.0926,  ..., -1.3986,  0.3391, -1.2430],\n",
      "        [ 0.4296, -0.9098,  0.2255,  ..., -1.0690,  0.4376, -0.8466],\n",
      "        [ 0.5813, -0.1950,  0.5029,  ..., -0.5004,  0.5831, -0.5570]])\n",
      "tensor([[ 0.5487, -2.4819,  0.2845,  ..., -2.0599,  0.5756, -1.9891],\n",
      "        [ 1.6829,  0.6002,  2.0312,  ..., -0.4140,  1.6756, -0.8192],\n",
      "        [-0.5033,  2.4722, -0.1735,  ...,  1.4921, -0.5228,  1.0430],\n",
      "        ...,\n",
      "        [-0.6985,  1.0842, -0.5634,  ...,  1.3962, -0.7203,  1.2662],\n",
      "        [ 0.8270, -0.1216,  0.7350,  ..., -0.6816,  0.8357, -0.8037],\n",
      "        [-1.2039, -1.3831, -1.4681,  ...,  0.2342, -1.1931,  0.8187]])\n",
      "tensor([[ 0.5172,  0.4825,  0.5659,  ..., -0.1118,  0.5136, -0.3302],\n",
      "        [-1.4118,  0.4935, -1.2884,  ...,  1.2864, -1.4243,  1.4130],\n",
      "        [-1.7290,  0.6060, -1.5117,  ...,  1.2689, -1.7532,  1.5413],\n",
      "        ...,\n",
      "        [-1.0737, -0.2738, -1.1690,  ...,  1.1212, -1.0937,  1.3801],\n",
      "        [-1.4428, -1.8967, -2.1150,  ..., -0.8998, -1.3694, -0.2367],\n",
      "        [-2.7787,  0.1651, -2.2327,  ...,  1.9535, -2.7555,  2.1851]])\n",
      "tensor([[-1.3787, -1.4015, -1.7786,  ...,  0.5996, -1.3804,  1.1872],\n",
      "        [-1.6492,  1.5471, -1.1289,  ...,  1.8873, -1.6714,  1.7568],\n",
      "        [ 0.7320,  1.8953,  1.1017,  ...,  0.7631,  0.7024,  0.3117],\n",
      "        ...,\n",
      "        [-0.4444, -0.3008, -0.4911,  ...,  0.0877, -0.4443,  0.3666],\n",
      "        [ 2.0255, -0.0047,  1.8959,  ..., -0.7214,  2.0004, -0.9669],\n",
      "        [-0.9216, -1.7003, -1.1419,  ...,  0.2963, -0.9207,  0.8213]])\n",
      "tensor([[ 0.7164,  1.4127,  1.0518,  ...,  0.6468,  0.6891,  0.1912],\n",
      "        [-0.5452, -0.4068, -0.5827,  ...,  0.0358, -0.5423,  0.2622],\n",
      "        [-0.3901,  1.3668, -0.1542,  ...,  1.0264, -0.4009,  0.7055],\n",
      "        ...,\n",
      "        [-1.0855,  2.0898, -0.6992,  ...,  2.5755, -1.1208,  1.7760],\n",
      "        [ 1.7430, -0.0639,  1.6182,  ..., -0.7429,  1.7413, -0.9866],\n",
      "        [-0.1266, -2.0464, -0.3607,  ..., -0.8513, -0.1182, -0.4176]])\n",
      "tensor([[ 4.5677e-01, -6.9398e-01,  2.5792e-01,  8.0979e-01, -1.1577e+00,\n",
      "         -8.9502e-01,  4.6070e-01, -6.0003e-01],\n",
      "        [ 5.2010e-01,  3.5702e-02,  4.5809e-01,  5.3690e-01, -2.1315e-02,\n",
      "         -2.8098e-01,  5.1039e-01, -2.3048e-01],\n",
      "        [-3.1671e-01, -1.0489e+00, -4.2814e-01,  1.8315e-02, -4.2988e-01,\n",
      "         -3.1246e-01, -3.1237e-01, -4.0536e-02],\n",
      "        [-6.6989e-03,  2.7926e-01,  3.4704e-02, -3.5242e-01,  5.6999e-01,\n",
      "          1.4662e-01, -1.1299e-02,  9.3962e-02],\n",
      "        [ 3.0266e-01,  1.3209e+00,  5.2324e-01, -1.8917e-01,  1.6419e+00,\n",
      "          6.3988e-01,  2.8538e-01,  3.0161e-01],\n",
      "        [ 1.0967e+00,  1.3132e+00,  1.4393e+00,  2.3541e-01,  1.4193e+00,\n",
      "          3.3972e-01,  1.0606e+00, -8.6063e-03],\n",
      "        [ 1.4151e+00, -1.2904e+00,  1.0057e+00,  2.4269e+00, -6.1804e-01,\n",
      "         -2.2916e+00,  1.4670e+00, -2.3415e+00],\n",
      "        [-7.5863e-01,  7.4229e-01, -6.6182e-01, -1.0842e+00, -7.1677e-02,\n",
      "          1.2046e+00, -7.7546e-01,  1.1430e+00],\n",
      "        [ 1.0832e+00,  1.0059e+00,  1.3334e+00, -4.5899e-03,  1.3012e+00,\n",
      "          3.8832e-01,  1.0474e+00,  7.5263e-02],\n",
      "        [ 9.9803e-01,  1.1793e+00,  1.3653e+00,  3.9332e-01,  1.8998e+00,\n",
      "          3.3666e-01,  9.7631e-01, -1.3315e-01],\n",
      "        [ 4.7782e-01,  2.5527e-01,  4.7854e-01, -2.9001e-03,  4.5866e-01,\n",
      "          1.5188e-02,  4.6952e-01, -4.8617e-02],\n",
      "        [-9.4761e-01,  3.0227e-01, -9.0911e-01, -1.0402e+00, -3.8079e-01,\n",
      "          8.2134e-01, -9.5359e-01,  9.7767e-01],\n",
      "        [ 5.7313e-01,  1.3354e-01,  5.8162e-01,  3.4236e-01,  6.5250e-01,\n",
      "         -2.7462e-01,  5.6993e-01, -4.2213e-01],\n",
      "        [-4.6885e-01,  1.5412e+00, -2.5681e-01, -8.6594e-01,  8.3650e-01,\n",
      "          1.3328e+00, -4.8769e-01,  1.0134e+00],\n",
      "        [ 9.0237e-03,  1.4182e-01,  8.0167e-02,  9.7758e-02,  1.0044e+00,\n",
      "         -1.5446e-01,  1.2997e-02, -2.6410e-01],\n",
      "        [ 1.4736e+00,  1.7490e+00,  2.6832e+00,  5.3921e-01,  2.1248e+00,\n",
      "          4.4374e-01,  1.4281e+00, -1.0633e-01],\n",
      "        [ 2.0386e-01, -9.7458e-01,  1.5794e-02,  9.5013e-01, -1.2632e+00,\n",
      "         -1.0699e+00,  2.1469e-01, -8.1701e-01],\n",
      "        [-1.5466e+00, -1.5345e+00, -1.9479e+00, -7.5007e-01, -1.5480e+00,\n",
      "          8.7531e-02, -1.5034e+00,  7.2885e-01],\n",
      "        [ 1.6512e+00,  7.8727e-01,  2.0744e+00,  7.7689e-01,  1.1446e+00,\n",
      "         -1.2837e-01,  1.6150e+00, -4.5754e-01],\n",
      "        [-3.4816e-02,  1.1704e+00,  1.2115e-01, -5.5124e-01,  9.7791e-01,\n",
      "          1.0227e+00, -5.4877e-02,  7.3635e-01],\n",
      "        [ 1.3799e+00, -3.9568e-01,  1.2046e+00,  9.8535e-01,  2.7280e-01,\n",
      "         -7.4596e-01,  1.3749e+00, -8.0333e-01],\n",
      "        [ 8.7009e-02, -1.4986e-01,  1.2416e-01,  9.6755e-02,  8.7168e-01,\n",
      "         -4.2329e-01,  9.6297e-02, -5.2436e-01],\n",
      "        [-1.3656e+00, -2.0790e+00, -2.0365e+00, -3.2605e-01, -2.6518e+00,\n",
      "         -3.9647e-01, -1.3293e+00,  3.7490e-01],\n",
      "        [ 4.4367e-01,  4.5287e-01,  5.0395e-01,  1.7611e-01,  8.3896e-01,\n",
      "         -1.2117e-01,  4.4156e-01, -3.7620e-01],\n",
      "        [ 3.6375e-01,  1.1233e+00,  5.5642e-01, -1.4983e-01,  1.5507e+00,\n",
      "          5.1547e-01,  3.4861e-01,  1.6015e-01],\n",
      "        [ 4.5886e-02,  4.7462e-01,  7.9283e-02, -3.9450e-01,  3.4825e-01,\n",
      "          4.4693e-01,  3.3865e-02,  4.0023e-01],\n",
      "        [ 5.6200e-01,  1.0938e+00,  7.5191e-01,  2.6404e-02,  1.3637e+00,\n",
      "          3.5628e-01,  5.4634e-01,  5.7363e-02],\n",
      "        [-4.3040e-01, -9.1352e-01, -5.3999e-01, -2.5547e-01, -5.1382e-01,\n",
      "         -3.9780e-02, -4.3010e-01,  3.0501e-01],\n",
      "        [-3.4716e-02,  1.0594e+00,  1.2641e-01, -3.1754e-01,  1.1405e+00,\n",
      "          5.3803e-01, -4.0233e-02,  2.2585e-01],\n",
      "        [-4.5352e-01,  3.0182e-02, -4.5563e-01, -4.3337e-01, -4.7998e-02,\n",
      "          2.4896e-01, -4.5417e-01,  3.3881e-01],\n",
      "        [ 1.2962e-01,  1.3025e-02,  1.2445e-01,  1.1503e-01,  3.8744e-01,\n",
      "         -2.0211e-01,  1.3276e-01, -2.8748e-01],\n",
      "        [ 6.4783e-01, -1.0898e+00,  5.2524e-01,  1.2556e+00,  5.2306e-02,\n",
      "         -1.1319e+00,  6.6314e-01, -1.1461e+00],\n",
      "        [ 3.3937e-01, -1.4313e+00,  1.3332e-01,  2.1497e+00, -1.0301e+00,\n",
      "         -2.1562e+00,  3.6806e-01, -2.0911e+00],\n",
      "        [ 9.5448e-01,  2.7734e-02,  8.7355e-01,  7.2505e-01,  1.7125e-01,\n",
      "         -5.1438e-01,  9.5140e-01, -6.4574e-01],\n",
      "        [-1.7688e+00, -2.3093e+00, -2.8831e+00, -4.9999e-01, -3.1940e+00,\n",
      "         -3.6694e-01, -1.6693e+00,  4.8479e-01],\n",
      "        [-7.2875e-01,  9.4665e-01, -6.0031e-01, -9.3896e-01,  5.6507e-02,\n",
      "          1.1845e+00, -7.4491e-01,  1.0949e+00],\n",
      "        [-7.5016e-01,  1.3901e+00, -5.3725e-01, -1.2284e+00,  4.1797e-01,\n",
      "          1.5036e+00, -7.7043e-01,  1.2360e+00],\n",
      "        [ 7.4391e-01,  1.0543e+00,  9.1883e-01,  1.8293e-01,  1.1681e+00,\n",
      "          2.7877e-01,  7.2786e-01, -1.2398e-02],\n",
      "        [-6.5888e-01, -1.8022e+00, -9.2036e-01,  2.3903e-01, -1.9823e+00,\n",
      "         -8.4573e-01, -6.3654e-01, -3.1762e-01],\n",
      "        [-7.4801e-01,  2.8778e-01, -7.2587e-01, -8.2124e-01, -2.6759e-01,\n",
      "          7.5646e-01, -7.5512e-01,  8.4164e-01],\n",
      "        [-8.8858e-01,  1.4873e+00, -6.4510e-01, -1.2047e+00,  3.0641e-01,\n",
      "          1.4004e+00, -9.0459e-01,  1.2410e+00],\n",
      "        [ 1.2841e+00,  4.1371e-01,  1.3366e+00,  4.4886e-01,  7.9293e-01,\n",
      "         -1.9919e-01,  1.2625e+00, -3.5229e-01],\n",
      "        [ 8.3152e-01,  6.8821e-01,  9.1449e-01,  2.2631e-01,  8.0631e-01,\n",
      "          1.2888e-01,  8.1266e-01, -6.4555e-02],\n",
      "        [ 1.5376e+00, -5.1397e-01,  1.1215e+00,  1.3568e+00, -7.3975e-01,\n",
      "         -1.2872e+00,  1.5494e+00, -1.2314e+00],\n",
      "        [ 1.2687e-01, -1.3802e+00, -7.5602e-02,  1.0280e+00, -1.3418e+00,\n",
      "         -1.1710e+00,  1.4060e-01, -9.3355e-01],\n",
      "        [-1.3577e+00, -1.0402e+00, -1.5990e+00, -8.3988e-01, -1.3741e+00,\n",
      "          2.6816e-01, -1.3365e+00,  7.9551e-01],\n",
      "        [ 5.4419e-01,  1.6842e+00,  8.1125e-01, -2.3051e-01,  1.5367e+00,\n",
      "          8.3886e-01,  5.1722e-01,  4.5769e-01],\n",
      "        [ 1.6297e-01, -1.3015e+00, -2.5531e-02,  1.4221e+00, -1.1259e+00,\n",
      "         -1.5112e+00,  1.8627e-01, -1.3853e+00],\n",
      "        [ 2.5602e-01, -9.2690e-02,  2.2313e-01,  5.2400e-01,  2.2079e-01,\n",
      "         -5.3995e-01,  2.6530e-01, -6.6651e-01],\n",
      "        [-7.0450e-01, -1.3581e+00, -9.4654e-01,  3.8938e-02, -1.9499e+00,\n",
      "         -5.5023e-01, -6.9025e-01,  5.0073e-02],\n",
      "        [ 1.7671e+00, -1.0284e-01,  1.5741e+00,  9.6764e-01,  3.1075e-01,\n",
      "         -7.4658e-01,  1.7518e+00, -9.2995e-01],\n",
      "        [ 5.8782e-01,  1.4324e+00,  8.4226e-01,  1.3505e-01,  1.6130e+00,\n",
      "          4.7678e-01,  5.7006e-01,  2.8408e-02],\n",
      "        [ 1.0448e-01, -1.0563e+00,  9.6416e-03,  7.7252e-01, -1.3609e-01,\n",
      "         -8.3911e-01,  1.1935e-01, -8.5112e-01],\n",
      "        [ 8.2317e-02, -7.8980e-01, -9.6796e-02,  1.0596e+00, -1.2749e+00,\n",
      "         -1.2489e+00,  9.9516e-02, -1.0499e+00],\n",
      "        [-1.6599e+00, -1.9897e-01, -1.7877e+00, -2.1214e+00, -1.5078e+00,\n",
      "          1.7494e+00, -1.7209e+00,  2.0341e+00],\n",
      "        [-5.6749e-01,  3.3529e-01, -5.3448e-01, -6.1577e-01, -9.9810e-02,\n",
      "          2.6760e-01, -5.6703e-01,  4.6780e-01],\n",
      "        [ 8.3826e-01,  9.2891e-01,  9.8851e-01,  4.1843e-01,  1.0226e+00,\n",
      "          1.1857e-01,  8.2549e-01, -1.2651e-01],\n",
      "        [ 6.7524e-01,  2.0787e-01,  6.5759e-01,  5.3522e-01,  4.4543e-01,\n",
      "         -1.6354e-01,  6.6444e-01, -2.4737e-01],\n",
      "        [-4.5570e-01, -1.1508e-01, -4.4907e-01, -4.2626e-01,  1.3027e-01,\n",
      "          1.7876e-01, -4.5448e-01,  2.4068e-01],\n",
      "        [ 1.9508e+00,  1.7616e-01,  1.8389e+00,  5.9048e-01,  3.7272e-01,\n",
      "         -4.4024e-01,  1.8824e+00, -6.0486e-01],\n",
      "        [-9.8918e-01,  1.0097e-01, -9.0390e-01, -5.5705e-01, -1.5766e-02,\n",
      "          1.2081e-01, -9.6811e-01,  3.7636e-01],\n",
      "        [-1.0123e+00,  6.7503e-01, -8.9640e-01, -1.3071e+00, -2.7949e-01,\n",
      "          1.4050e+00, -1.0317e+00,  1.3762e+00],\n",
      "        [ 5.1369e-01,  4.5627e-01,  5.4014e-01, -1.7272e-01,  5.2489e-01,\n",
      "          1.9949e-01,  4.9398e-01,  1.2281e-01],\n",
      "        [ 5.5838e-01,  8.8979e-01,  6.9289e-01,  2.6377e-01,  1.0323e+00,\n",
      "          5.1335e-02,  5.5438e-01, -2.5293e-01],\n",
      "        [-3.9881e-01,  6.1481e-01, -3.0639e-01, -6.5402e-01,  5.3099e-01,\n",
      "          6.5265e-01, -4.0376e-01,  5.5602e-01],\n",
      "        [ 5.3711e-01, -2.7353e-02,  5.2605e-01,  2.1365e-01,  5.2247e-01,\n",
      "         -3.1318e-01,  5.3930e-01, -4.3523e-01],\n",
      "        [-1.7713e+00,  5.3683e-01, -1.4649e+00, -1.2228e+00, -5.7902e-01,\n",
      "          9.2380e-01, -1.7484e+00,  1.2016e+00],\n",
      "        [-9.3991e-01, -1.4762e+00, -1.2031e+00, -1.0642e-01, -1.7708e+00,\n",
      "         -4.9757e-01, -9.1510e-01,  1.1391e-01],\n",
      "        [ 4.2221e-01, -3.6004e-02,  3.5615e-01,  2.8996e-01, -6.4366e-02,\n",
      "         -2.7653e-01,  4.1825e-01, -2.6571e-01],\n",
      "        [-2.4106e+00,  4.6104e-01, -1.8178e+00, -2.4587e+00, -1.0296e+00,\n",
      "          2.1159e+00, -2.4158e+00,  2.0995e+00],\n",
      "        [-4.5351e-01,  8.9187e-01, -3.0153e-01, -7.2194e-01,  7.9383e-01,\n",
      "          8.6852e-01, -4.6339e-01,  6.5697e-01],\n",
      "        [-5.2617e-01,  6.2167e-01, -3.6833e-01, -5.8624e-01,  9.5182e-01,\n",
      "          7.4980e-01, -5.3145e-01,  5.9396e-01],\n",
      "        [ 7.6784e-02,  4.5483e-01,  2.1608e-01,  5.2734e-02,  1.5907e+00,\n",
      "          2.3378e-01,  7.5307e-02, -7.7614e-02],\n",
      "        [ 3.3749e-01, -1.4072e+00,  2.0305e-01,  6.5309e-01, -1.7859e-01,\n",
      "         -7.9493e-01,  3.4338e-01, -5.4529e-01],\n",
      "        [-1.1321e+00,  1.7304e+00, -8.2377e-01, -1.5102e+00, -4.3184e-02,\n",
      "          1.8124e+00, -1.1604e+00,  1.6442e+00],\n",
      "        [-1.1606e+00, -9.4793e-01, -1.2277e+00, -2.0467e-02, -4.5813e-01,\n",
      "         -6.5955e-01, -1.1112e+00, -2.7989e-01],\n",
      "        [-2.8388e-01, -1.4027e+00, -4.2539e-01,  2.0399e-01, -5.5275e-01,\n",
      "         -4.5141e-01, -2.8227e-01,  5.1930e-02],\n",
      "        [ 8.8379e-01,  5.8791e-01,  1.0091e+00,  3.5328e-02,  1.1793e+00,\n",
      "          8.3401e-02,  8.6755e-01, -2.1636e-01],\n",
      "        [-7.6156e-01, -9.3101e-01, -9.8013e-01,  7.2625e-02, -1.8493e+00,\n",
      "         -7.3181e-01, -7.3210e-01, -2.9452e-01],\n",
      "        [-1.2753e+00,  6.1762e-01, -1.1483e+00, -1.2656e+00, -5.3623e-01,\n",
      "          1.1098e+00, -1.2814e+00,  1.2696e+00],\n",
      "        [ 6.9584e-01,  7.1788e-01,  8.1495e-01,  1.3811e-01,  1.0728e+00,\n",
      "          9.2162e-02,  6.8293e-01, -1.8176e-01],\n",
      "        [ 2.6169e-01,  1.1203e+00,  4.1734e-01, -1.1203e-02,  1.1617e+00,\n",
      "          5.4426e-01,  2.4904e-01,  2.2890e-01],\n",
      "        [-4.2936e-01,  1.4172e+00, -2.2485e-01, -7.4033e-01,  9.1333e-01,\n",
      "          9.0125e-01, -4.3782e-01,  6.3915e-01],\n",
      "        [ 5.5262e-01, -1.3453e-01,  3.9073e-01,  1.5970e+00, -9.2208e-01,\n",
      "         -1.8679e+00,  5.8021e-01, -1.8088e+00],\n",
      "        [-1.3718e+00, -1.7243e-01, -1.3584e+00, -2.9469e-01, -5.7133e-01,\n",
      "         -1.8931e-01, -1.3290e+00,  2.8647e-01],\n",
      "        [ 1.4804e+00,  4.3416e-01,  1.4255e+00,  6.9158e-01,  3.5297e-01,\n",
      "         -4.2975e-01,  1.4589e+00, -5.8146e-01],\n",
      "        [-1.2631e+00, -1.2357e+00, -1.4964e+00, -1.1178e+00, -1.2649e+00,\n",
      "          4.5718e-01, -1.2492e+00,  9.4669e-01],\n",
      "        [-1.6068e+00,  2.0615e-01, -1.5764e+00, -1.6435e+00, -1.1510e+00,\n",
      "          1.3157e+00, -1.6151e+00,  1.5520e+00],\n",
      "        [-1.1346e-01, -2.6276e+00, -3.5117e-01,  1.4414e+00, -1.4607e+00,\n",
      "         -1.7775e+00, -9.0076e-02, -1.5463e+00],\n",
      "        [-3.8826e-02, -6.5915e-01, -5.4022e-02,  1.9191e-01,  5.0152e-01,\n",
      "         -3.8564e-01, -3.2775e-02, -3.5032e-01],\n",
      "        [-3.6053e-01, -2.9205e-01, -3.7221e-01, -1.3384e-01,  1.2523e-01,\n",
      "         -1.2984e-01, -3.4953e-01, -8.9009e-02],\n",
      "        [-5.2408e-01,  9.1943e-01, -4.1225e-01, -7.6789e-01,  2.1860e-01,\n",
      "          9.2797e-01, -5.3751e-01,  8.4185e-01],\n",
      "        [ 6.8502e-01, -4.4223e-01,  5.8763e-01,  9.5449e-01,  8.3896e-02,\n",
      "         -7.6743e-01,  6.9246e-01, -8.2903e-01],\n",
      "        [ 1.5913e+00, -1.5754e+00,  1.0800e+00,  2.3575e+00, -7.1979e-01,\n",
      "         -1.9514e+00,  1.6349e+00, -1.9590e+00],\n",
      "        [ 6.3946e-01,  5.6295e-01,  6.9273e-01,  1.2682e-01,  6.7558e-01,\n",
      "          1.4851e-01,  6.2364e-01,  1.5077e-02],\n",
      "        [-1.9274e+00,  2.0287e+00, -1.1576e+00, -2.6706e+00, -4.1009e-01,\n",
      "          2.8862e+00, -2.0276e+00,  2.6981e+00]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(y)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BleveNet(\n",
      "  (fc1): Linear(in_features=11, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=8, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (mish): Mish()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Define the NN architecture\n",
    "## NN with 3 hidden layer, s=[26, 256, 256, 256, 8]\n",
    "\n",
    "class BleveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BleveNet, self).__init__()\n",
    "        # The first hidden layer has 256 neurons\n",
    "        self.fc1 = nn.Linear(X_train_torch.shape[1], 256)\n",
    "        # The second hidden layer has 256 neurons\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        # The third hidden layer has 256 neurons\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        # The final layer has 1 output neuron\n",
    "        self.fc4 = nn.Linear(256, 8)\n",
    "\n",
    "\n",
    "        # Define proportion or neurons to dropout\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Mish activation\n",
    "        self.mish = nn.Mish()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.mish(x)\n",
    "        x = self.dropout(x)\n",
    "        # add second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.mish(x)\n",
    "        x = self.dropout(x)\n",
    "        # add third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        x = self.mish(x)\n",
    "        x = self.dropout(x)\n",
    "        # add final fully connected layers\n",
    "        output = self.fc4(x)\n",
    "       \n",
    "        return output\n",
    "\n",
    "# initialize the NN\n",
    "model = BleveNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.HuberLoss()        # This is the best loss function for my model\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)     # This is the best optimizer for my model \n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500 \tTraining Loss: 0.069044\n",
      "Epoch: 2/500 \tTraining Loss: 0.022229\n",
      "Epoch: 3/500 \tTraining Loss: 0.018079\n",
      "Epoch: 4/500 \tTraining Loss: 0.017303\n",
      "Epoch: 5/500 \tTraining Loss: 0.016086\n",
      "Epoch: 6/500 \tTraining Loss: 0.014802\n",
      "Epoch: 7/500 \tTraining Loss: 0.013582\n",
      "Epoch: 8/500 \tTraining Loss: 0.013006\n",
      "Epoch: 9/500 \tTraining Loss: 0.012731\n",
      "Epoch: 10/500 \tTraining Loss: 0.012009\n",
      "Epoch: 11/500 \tTraining Loss: 0.011654\n",
      "Epoch: 12/500 \tTraining Loss: 0.011420\n",
      "Epoch: 13/500 \tTraining Loss: 0.011701\n",
      "Epoch: 14/500 \tTraining Loss: 0.011261\n",
      "Epoch: 15/500 \tTraining Loss: 0.011108\n",
      "Epoch: 16/500 \tTraining Loss: 0.010607\n",
      "Epoch: 17/500 \tTraining Loss: 0.010482\n",
      "Epoch: 18/500 \tTraining Loss: 0.010772\n",
      "Epoch: 19/500 \tTraining Loss: 0.010224\n",
      "Epoch: 20/500 \tTraining Loss: 0.009982\n",
      "Epoch: 21/500 \tTraining Loss: 0.009346\n",
      "Epoch: 22/500 \tTraining Loss: 0.009713\n",
      "Epoch: 23/500 \tTraining Loss: 0.009715\n",
      "Epoch: 24/500 \tTraining Loss: 0.009676\n",
      "Epoch: 25/500 \tTraining Loss: 0.009170\n",
      "Epoch: 26/500 \tTraining Loss: 0.008977\n",
      "Epoch: 27/500 \tTraining Loss: 0.009712\n",
      "Epoch: 28/500 \tTraining Loss: 0.010384\n",
      "Epoch: 29/500 \tTraining Loss: 0.009377\n",
      "Epoch: 30/500 \tTraining Loss: 0.008703\n",
      "Epoch: 31/500 \tTraining Loss: 0.009053\n",
      "Epoch: 32/500 \tTraining Loss: 0.008456\n",
      "Epoch: 33/500 \tTraining Loss: 0.008896\n",
      "Epoch: 34/500 \tTraining Loss: 0.009140\n",
      "Epoch: 35/500 \tTraining Loss: 0.009241\n",
      "Epoch: 36/500 \tTraining Loss: 0.008601\n",
      "Epoch: 37/500 \tTraining Loss: 0.008683\n",
      "Epoch: 38/500 \tTraining Loss: 0.008597\n",
      "Epoch: 39/500 \tTraining Loss: 0.008478\n",
      "Epoch: 40/500 \tTraining Loss: 0.009003\n",
      "Epoch: 41/500 \tTraining Loss: 0.008818\n",
      "Epoch: 42/500 \tTraining Loss: 0.008224\n",
      "Epoch: 43/500 \tTraining Loss: 0.008814\n",
      "Epoch: 44/500 \tTraining Loss: 0.008544\n",
      "Epoch: 45/500 \tTraining Loss: 0.008971\n",
      "Epoch: 46/500 \tTraining Loss: 0.009028\n",
      "Epoch: 47/500 \tTraining Loss: 0.008915\n",
      "Epoch: 48/500 \tTraining Loss: 0.008013\n",
      "Epoch: 49/500 \tTraining Loss: 0.008378\n",
      "Epoch: 50/500 \tTraining Loss: 0.008147\n",
      "Epoch: 51/500 \tTraining Loss: 0.008821\n",
      "Epoch: 52/500 \tTraining Loss: 0.007990\n",
      "Epoch: 53/500 \tTraining Loss: 0.008792\n",
      "Epoch: 54/500 \tTraining Loss: 0.009086\n",
      "Epoch: 55/500 \tTraining Loss: 0.007872\n",
      "Epoch: 56/500 \tTraining Loss: 0.008455\n",
      "Epoch: 57/500 \tTraining Loss: 0.008235\n",
      "Epoch: 58/500 \tTraining Loss: 0.008341\n",
      "Epoch: 59/500 \tTraining Loss: 0.008049\n",
      "Epoch: 60/500 \tTraining Loss: 0.008276\n",
      "Epoch: 61/500 \tTraining Loss: 0.008062\n",
      "Epoch: 62/500 \tTraining Loss: 0.008001\n",
      "Epoch: 63/500 \tTraining Loss: 0.008236\n",
      "Epoch: 64/500 \tTraining Loss: 0.008242\n",
      "Epoch: 65/500 \tTraining Loss: 0.008171\n",
      "Epoch: 66/500 \tTraining Loss: 0.008247\n",
      "Epoch: 67/500 \tTraining Loss: 0.007910\n",
      "Epoch: 68/500 \tTraining Loss: 0.007875\n",
      "Epoch: 69/500 \tTraining Loss: 0.007978\n",
      "Epoch: 70/500 \tTraining Loss: 0.007943\n",
      "Epoch: 71/500 \tTraining Loss: 0.007734\n",
      "Epoch: 72/500 \tTraining Loss: 0.007750\n",
      "Epoch: 73/500 \tTraining Loss: 0.007848\n",
      "Epoch: 74/500 \tTraining Loss: 0.008087\n",
      "Epoch: 75/500 \tTraining Loss: 0.007961\n",
      "Epoch: 76/500 \tTraining Loss: 0.008262\n",
      "Epoch: 77/500 \tTraining Loss: 0.008203\n",
      "Epoch: 78/500 \tTraining Loss: 0.008071\n",
      "Epoch: 79/500 \tTraining Loss: 0.008202\n",
      "Epoch: 80/500 \tTraining Loss: 0.007752\n",
      "Epoch: 81/500 \tTraining Loss: 0.007475\n",
      "Epoch: 82/500 \tTraining Loss: 0.007789\n",
      "Epoch: 83/500 \tTraining Loss: 0.007794\n",
      "Epoch: 84/500 \tTraining Loss: 0.007731\n",
      "Epoch: 85/500 \tTraining Loss: 0.007724\n",
      "Epoch: 86/500 \tTraining Loss: 0.008051\n",
      "Epoch: 87/500 \tTraining Loss: 0.008074\n",
      "Epoch: 88/500 \tTraining Loss: 0.008079\n",
      "Epoch: 89/500 \tTraining Loss: 0.007703\n",
      "Epoch: 90/500 \tTraining Loss: 0.008024\n",
      "Epoch: 91/500 \tTraining Loss: 0.007559\n",
      "Epoch: 92/500 \tTraining Loss: 0.008204\n",
      "Epoch: 93/500 \tTraining Loss: 0.007968\n",
      "Epoch: 94/500 \tTraining Loss: 0.007579\n",
      "Epoch: 95/500 \tTraining Loss: 0.007789\n",
      "Epoch: 96/500 \tTraining Loss: 0.007267\n",
      "Epoch: 97/500 \tTraining Loss: 0.007573\n",
      "Epoch: 98/500 \tTraining Loss: 0.008315\n",
      "Epoch: 99/500 \tTraining Loss: 0.007857\n",
      "Epoch: 100/500 \tTraining Loss: 0.007620\n",
      "Epoch: 101/500 \tTraining Loss: 0.007799\n",
      "Epoch: 102/500 \tTraining Loss: 0.007758\n",
      "Epoch: 103/500 \tTraining Loss: 0.007815\n",
      "Epoch: 104/500 \tTraining Loss: 0.007516\n",
      "Epoch: 105/500 \tTraining Loss: 0.007729\n",
      "Epoch: 106/500 \tTraining Loss: 0.008285\n",
      "Epoch: 107/500 \tTraining Loss: 0.008240\n",
      "Epoch: 108/500 \tTraining Loss: 0.007900\n",
      "Epoch: 109/500 \tTraining Loss: 0.007409\n",
      "Epoch: 110/500 \tTraining Loss: 0.008000\n",
      "Epoch: 111/500 \tTraining Loss: 0.008130\n",
      "Epoch: 112/500 \tTraining Loss: 0.007719\n",
      "Epoch: 113/500 \tTraining Loss: 0.007449\n",
      "Epoch: 114/500 \tTraining Loss: 0.008022\n",
      "Epoch: 115/500 \tTraining Loss: 0.007530\n",
      "Epoch: 116/500 \tTraining Loss: 0.007949\n",
      "Epoch: 117/500 \tTraining Loss: 0.007367\n",
      "Epoch: 118/500 \tTraining Loss: 0.007290\n",
      "Epoch: 119/500 \tTraining Loss: 0.007259\n",
      "Epoch: 120/500 \tTraining Loss: 0.007505\n",
      "Epoch: 121/500 \tTraining Loss: 0.007654\n",
      "Epoch: 122/500 \tTraining Loss: 0.007793\n",
      "Epoch: 123/500 \tTraining Loss: 0.007485\n",
      "Epoch: 124/500 \tTraining Loss: 0.007690\n",
      "Epoch: 125/500 \tTraining Loss: 0.007327\n",
      "Epoch: 126/500 \tTraining Loss: 0.007715\n",
      "Epoch: 127/500 \tTraining Loss: 0.007786\n",
      "Epoch: 128/500 \tTraining Loss: 0.007823\n",
      "Epoch: 129/500 \tTraining Loss: 0.007364\n",
      "Epoch: 130/500 \tTraining Loss: 0.007512\n",
      "Epoch: 131/500 \tTraining Loss: 0.007666\n",
      "Epoch: 132/500 \tTraining Loss: 0.007378\n",
      "Epoch: 133/500 \tTraining Loss: 0.007707\n",
      "Epoch: 134/500 \tTraining Loss: 0.007885\n",
      "Epoch: 135/500 \tTraining Loss: 0.007946\n",
      "Epoch: 136/500 \tTraining Loss: 0.007873\n",
      "Epoch: 137/500 \tTraining Loss: 0.007669\n",
      "Epoch: 138/500 \tTraining Loss: 0.007284\n",
      "Epoch: 139/500 \tTraining Loss: 0.007624\n",
      "Epoch: 140/500 \tTraining Loss: 0.007458\n",
      "Epoch: 141/500 \tTraining Loss: 0.007593\n",
      "Epoch: 142/500 \tTraining Loss: 0.007585\n",
      "Epoch: 143/500 \tTraining Loss: 0.007409\n",
      "Epoch: 144/500 \tTraining Loss: 0.007490\n",
      "Epoch: 145/500 \tTraining Loss: 0.007932\n",
      "Epoch: 146/500 \tTraining Loss: 0.007321\n",
      "Epoch: 147/500 \tTraining Loss: 0.007606\n",
      "Epoch: 148/500 \tTraining Loss: 0.007456\n",
      "Epoch: 149/500 \tTraining Loss: 0.007941\n",
      "Epoch: 150/500 \tTraining Loss: 0.007517\n",
      "Epoch: 151/500 \tTraining Loss: 0.008308\n",
      "Epoch: 152/500 \tTraining Loss: 0.007695\n",
      "Epoch: 153/500 \tTraining Loss: 0.007388\n",
      "Epoch: 154/500 \tTraining Loss: 0.007536\n",
      "Epoch: 155/500 \tTraining Loss: 0.007292\n",
      "Epoch: 156/500 \tTraining Loss: 0.007770\n",
      "Epoch: 157/500 \tTraining Loss: 0.007416\n",
      "Epoch: 158/500 \tTraining Loss: 0.008278\n",
      "Epoch: 159/500 \tTraining Loss: 0.007234\n",
      "Epoch: 160/500 \tTraining Loss: 0.007193\n",
      "Epoch: 161/500 \tTraining Loss: 0.007484\n",
      "Epoch: 162/500 \tTraining Loss: 0.007410\n",
      "Epoch: 163/500 \tTraining Loss: 0.007186\n",
      "Epoch: 164/500 \tTraining Loss: 0.007341\n",
      "Epoch: 165/500 \tTraining Loss: 0.008384\n",
      "Epoch: 166/500 \tTraining Loss: 0.007618\n",
      "Epoch: 167/500 \tTraining Loss: 0.007998\n",
      "Epoch: 168/500 \tTraining Loss: 0.007839\n",
      "Epoch: 169/500 \tTraining Loss: 0.007590\n",
      "Epoch: 170/500 \tTraining Loss: 0.007116\n",
      "Epoch: 171/500 \tTraining Loss: 0.007245\n",
      "Epoch: 172/500 \tTraining Loss: 0.007156\n",
      "Epoch: 173/500 \tTraining Loss: 0.007318\n",
      "Epoch: 174/500 \tTraining Loss: 0.007070\n",
      "Epoch: 175/500 \tTraining Loss: 0.007239\n",
      "Epoch: 176/500 \tTraining Loss: 0.008032\n",
      "Epoch: 177/500 \tTraining Loss: 0.007150\n",
      "Epoch: 178/500 \tTraining Loss: 0.007712\n",
      "Epoch: 179/500 \tTraining Loss: 0.007460\n",
      "Epoch: 180/500 \tTraining Loss: 0.008073\n",
      "Epoch: 181/500 \tTraining Loss: 0.007169\n",
      "Epoch: 182/500 \tTraining Loss: 0.008174\n",
      "Epoch: 183/500 \tTraining Loss: 0.008206\n",
      "Epoch: 184/500 \tTraining Loss: 0.007196\n",
      "Epoch: 185/500 \tTraining Loss: 0.007623\n",
      "Epoch: 186/500 \tTraining Loss: 0.007460\n",
      "Epoch: 187/500 \tTraining Loss: 0.007242\n",
      "Epoch: 188/500 \tTraining Loss: 0.007264\n",
      "Epoch: 189/500 \tTraining Loss: 0.007766\n",
      "Epoch: 190/500 \tTraining Loss: 0.007576\n",
      "Epoch: 191/500 \tTraining Loss: 0.007123\n",
      "Epoch: 192/500 \tTraining Loss: 0.007115\n",
      "Epoch: 193/500 \tTraining Loss: 0.007303\n",
      "Epoch: 194/500 \tTraining Loss: 0.007272\n",
      "Epoch: 195/500 \tTraining Loss: 0.007143\n",
      "Epoch: 196/500 \tTraining Loss: 0.007497\n",
      "Epoch: 197/500 \tTraining Loss: 0.007188\n",
      "Epoch: 198/500 \tTraining Loss: 0.007357\n",
      "Epoch: 199/500 \tTraining Loss: 0.007662\n",
      "Epoch: 200/500 \tTraining Loss: 0.007991\n",
      "Epoch: 201/500 \tTraining Loss: 0.008038\n",
      "Epoch: 202/500 \tTraining Loss: 0.007349\n",
      "Epoch: 203/500 \tTraining Loss: 0.007314\n",
      "Epoch: 204/500 \tTraining Loss: 0.007156\n",
      "Epoch: 205/500 \tTraining Loss: 0.007184\n",
      "Epoch: 206/500 \tTraining Loss: 0.007598\n",
      "Epoch: 207/500 \tTraining Loss: 0.007285\n",
      "Epoch: 208/500 \tTraining Loss: 0.007467\n",
      "Epoch: 209/500 \tTraining Loss: 0.007271\n",
      "Epoch: 210/500 \tTraining Loss: 0.007207\n",
      "Epoch: 211/500 \tTraining Loss: 0.007125\n",
      "Epoch: 212/500 \tTraining Loss: 0.007379\n",
      "Epoch: 213/500 \tTraining Loss: 0.007504\n",
      "Epoch: 214/500 \tTraining Loss: 0.007781\n",
      "Epoch: 215/500 \tTraining Loss: 0.007821\n",
      "Epoch: 216/500 \tTraining Loss: 0.007397\n",
      "Epoch: 217/500 \tTraining Loss: 0.007039\n",
      "Epoch: 218/500 \tTraining Loss: 0.007284\n",
      "Epoch: 219/500 \tTraining Loss: 0.007295\n",
      "Epoch: 220/500 \tTraining Loss: 0.008011\n",
      "Epoch: 221/500 \tTraining Loss: 0.007550\n",
      "Epoch: 222/500 \tTraining Loss: 0.007181\n",
      "Epoch: 223/500 \tTraining Loss: 0.007652\n",
      "Epoch: 224/500 \tTraining Loss: 0.007199\n",
      "Epoch: 225/500 \tTraining Loss: 0.007100\n",
      "Epoch: 226/500 \tTraining Loss: 0.007638\n",
      "Epoch: 227/500 \tTraining Loss: 0.007092\n",
      "Epoch: 228/500 \tTraining Loss: 0.007325\n",
      "Epoch: 229/500 \tTraining Loss: 0.007021\n",
      "Epoch: 230/500 \tTraining Loss: 0.007459\n",
      "Epoch: 231/500 \tTraining Loss: 0.007606\n",
      "Epoch: 232/500 \tTraining Loss: 0.007271\n",
      "Epoch: 233/500 \tTraining Loss: 0.007238\n",
      "Epoch: 234/500 \tTraining Loss: 0.007398\n",
      "Epoch: 235/500 \tTraining Loss: 0.007748\n",
      "Epoch: 236/500 \tTraining Loss: 0.007626\n",
      "Epoch: 237/500 \tTraining Loss: 0.007282\n",
      "Epoch: 238/500 \tTraining Loss: 0.007446\n",
      "Epoch: 239/500 \tTraining Loss: 0.007168\n",
      "Epoch: 240/500 \tTraining Loss: 0.007430\n",
      "Epoch: 241/500 \tTraining Loss: 0.007298\n",
      "Epoch: 242/500 \tTraining Loss: 0.007869\n",
      "Epoch: 243/500 \tTraining Loss: 0.007725\n",
      "Epoch: 244/500 \tTraining Loss: 0.007491\n",
      "Epoch: 245/500 \tTraining Loss: 0.007665\n",
      "Epoch: 246/500 \tTraining Loss: 0.007068\n",
      "Epoch: 247/500 \tTraining Loss: 0.006960\n",
      "Epoch: 248/500 \tTraining Loss: 0.007399\n",
      "Epoch: 249/500 \tTraining Loss: 0.007596\n",
      "Epoch: 250/500 \tTraining Loss: 0.007072\n",
      "Epoch: 251/500 \tTraining Loss: 0.007337\n",
      "Epoch: 252/500 \tTraining Loss: 0.007417\n",
      "Epoch: 253/500 \tTraining Loss: 0.007287\n",
      "Epoch: 254/500 \tTraining Loss: 0.007380\n",
      "Epoch: 255/500 \tTraining Loss: 0.007211\n",
      "Epoch: 256/500 \tTraining Loss: 0.007323\n",
      "Epoch: 257/500 \tTraining Loss: 0.007225\n",
      "Epoch: 258/500 \tTraining Loss: 0.006854\n",
      "Epoch: 259/500 \tTraining Loss: 0.006885\n",
      "Epoch: 260/500 \tTraining Loss: 0.007480\n",
      "Epoch: 261/500 \tTraining Loss: 0.007538\n",
      "Epoch: 262/500 \tTraining Loss: 0.007378\n",
      "Epoch: 263/500 \tTraining Loss: 0.007484\n",
      "Epoch: 264/500 \tTraining Loss: 0.007445\n",
      "Epoch: 265/500 \tTraining Loss: 0.007309\n",
      "Epoch: 266/500 \tTraining Loss: 0.007626\n",
      "Epoch: 267/500 \tTraining Loss: 0.007479\n",
      "Epoch: 268/500 \tTraining Loss: 0.007807\n",
      "Epoch: 269/500 \tTraining Loss: 0.007848\n",
      "Epoch: 270/500 \tTraining Loss: 0.007203\n",
      "Epoch: 271/500 \tTraining Loss: 0.007448\n",
      "Epoch: 272/500 \tTraining Loss: 0.007421\n",
      "Epoch: 273/500 \tTraining Loss: 0.006909\n",
      "Epoch: 274/500 \tTraining Loss: 0.007507\n",
      "Epoch: 275/500 \tTraining Loss: 0.007385\n",
      "Epoch: 276/500 \tTraining Loss: 0.007080\n",
      "Epoch: 277/500 \tTraining Loss: 0.007254\n",
      "Epoch: 278/500 \tTraining Loss: 0.006856\n",
      "Epoch: 279/500 \tTraining Loss: 0.007785\n",
      "Epoch: 280/500 \tTraining Loss: 0.007137\n",
      "Epoch: 281/500 \tTraining Loss: 0.007022\n",
      "Epoch: 282/500 \tTraining Loss: 0.007180\n",
      "Epoch: 283/500 \tTraining Loss: 0.007229\n",
      "Epoch: 284/500 \tTraining Loss: 0.006961\n",
      "Epoch: 285/500 \tTraining Loss: 0.007504\n",
      "Epoch: 286/500 \tTraining Loss: 0.007728\n",
      "Epoch: 287/500 \tTraining Loss: 0.006899\n",
      "Epoch: 288/500 \tTraining Loss: 0.006998\n",
      "Epoch: 289/500 \tTraining Loss: 0.007402\n",
      "Epoch: 290/500 \tTraining Loss: 0.007551\n",
      "Epoch: 291/500 \tTraining Loss: 0.007137\n",
      "Epoch: 292/500 \tTraining Loss: 0.007598\n",
      "Epoch: 293/500 \tTraining Loss: 0.006986\n",
      "Epoch: 294/500 \tTraining Loss: 0.007325\n",
      "Epoch: 295/500 \tTraining Loss: 0.007275\n",
      "Epoch: 296/500 \tTraining Loss: 0.007260\n",
      "Epoch: 297/500 \tTraining Loss: 0.007677\n",
      "Epoch: 298/500 \tTraining Loss: 0.007588\n",
      "Epoch: 299/500 \tTraining Loss: 0.007805\n",
      "Epoch: 300/500 \tTraining Loss: 0.007340\n",
      "Epoch: 301/500 \tTraining Loss: 0.006947\n",
      "Epoch: 302/500 \tTraining Loss: 0.007635\n",
      "Epoch: 303/500 \tTraining Loss: 0.007623\n",
      "Epoch: 304/500 \tTraining Loss: 0.006956\n",
      "Epoch: 305/500 \tTraining Loss: 0.007180\n",
      "Epoch: 306/500 \tTraining Loss: 0.007270\n",
      "Epoch: 307/500 \tTraining Loss: 0.007378\n",
      "Epoch: 308/500 \tTraining Loss: 0.007138\n",
      "Epoch: 309/500 \tTraining Loss: 0.007431\n",
      "Epoch: 310/500 \tTraining Loss: 0.007215\n",
      "Epoch: 311/500 \tTraining Loss: 0.007840\n",
      "Epoch: 312/500 \tTraining Loss: 0.007535\n",
      "Epoch: 313/500 \tTraining Loss: 0.007107\n",
      "Epoch: 314/500 \tTraining Loss: 0.007487\n",
      "Epoch: 315/500 \tTraining Loss: 0.007104\n",
      "Epoch: 316/500 \tTraining Loss: 0.007131\n",
      "Epoch: 317/500 \tTraining Loss: 0.007078\n",
      "Epoch: 318/500 \tTraining Loss: 0.007024\n",
      "Epoch: 319/500 \tTraining Loss: 0.007291\n",
      "Epoch: 320/500 \tTraining Loss: 0.006918\n",
      "Epoch: 321/500 \tTraining Loss: 0.007667\n",
      "Epoch: 322/500 \tTraining Loss: 0.007050\n",
      "Epoch: 323/500 \tTraining Loss: 0.007439\n",
      "Epoch: 324/500 \tTraining Loss: 0.007235\n",
      "Epoch: 325/500 \tTraining Loss: 0.007370\n",
      "Epoch: 326/500 \tTraining Loss: 0.006920\n",
      "Epoch: 327/500 \tTraining Loss: 0.007562\n",
      "Epoch: 328/500 \tTraining Loss: 0.007420\n",
      "Epoch: 329/500 \tTraining Loss: 0.006950\n",
      "Epoch: 330/500 \tTraining Loss: 0.007368\n",
      "Epoch: 331/500 \tTraining Loss: 0.007377\n",
      "Epoch: 332/500 \tTraining Loss: 0.007577\n",
      "Epoch: 333/500 \tTraining Loss: 0.006979\n",
      "Epoch: 334/500 \tTraining Loss: 0.007079\n",
      "Epoch: 335/500 \tTraining Loss: 0.007276\n",
      "Epoch: 336/500 \tTraining Loss: 0.007311\n",
      "Epoch: 337/500 \tTraining Loss: 0.007151\n",
      "Epoch: 338/500 \tTraining Loss: 0.007378\n",
      "Epoch: 339/500 \tTraining Loss: 0.007681\n",
      "Epoch: 340/500 \tTraining Loss: 0.007473\n",
      "Epoch: 341/500 \tTraining Loss: 0.007203\n",
      "Epoch: 342/500 \tTraining Loss: 0.007255\n",
      "Epoch: 343/500 \tTraining Loss: 0.007302\n",
      "Epoch: 344/500 \tTraining Loss: 0.007139\n",
      "Epoch: 345/500 \tTraining Loss: 0.007262\n",
      "Epoch: 346/500 \tTraining Loss: 0.007265\n",
      "Epoch: 347/500 \tTraining Loss: 0.007106\n",
      "Epoch: 348/500 \tTraining Loss: 0.006978\n",
      "Epoch: 349/500 \tTraining Loss: 0.007402\n",
      "Epoch: 350/500 \tTraining Loss: 0.007223\n",
      "Epoch: 351/500 \tTraining Loss: 0.006841\n",
      "Epoch: 352/500 \tTraining Loss: 0.007585\n",
      "Epoch: 353/500 \tTraining Loss: 0.006957\n",
      "Epoch: 354/500 \tTraining Loss: 0.007107\n",
      "Epoch: 355/500 \tTraining Loss: 0.006768\n",
      "Epoch: 356/500 \tTraining Loss: 0.007282\n",
      "Epoch: 357/500 \tTraining Loss: 0.008388\n",
      "Epoch: 358/500 \tTraining Loss: 0.007219\n",
      "Epoch: 359/500 \tTraining Loss: 0.006829\n",
      "Epoch: 360/500 \tTraining Loss: 0.006819\n",
      "Epoch: 361/500 \tTraining Loss: 0.007528\n",
      "Epoch: 362/500 \tTraining Loss: 0.007184\n",
      "Epoch: 363/500 \tTraining Loss: 0.007270\n",
      "Epoch: 364/500 \tTraining Loss: 0.007157\n",
      "Epoch: 365/500 \tTraining Loss: 0.007115\n",
      "Epoch: 366/500 \tTraining Loss: 0.007572\n",
      "Epoch: 367/500 \tTraining Loss: 0.007503\n",
      "Epoch: 368/500 \tTraining Loss: 0.007343\n",
      "Epoch: 369/500 \tTraining Loss: 0.007180\n",
      "Epoch: 370/500 \tTraining Loss: 0.007072\n",
      "Epoch: 371/500 \tTraining Loss: 0.007126\n",
      "Epoch: 372/500 \tTraining Loss: 0.008425\n",
      "Epoch: 373/500 \tTraining Loss: 0.007446\n",
      "Epoch: 374/500 \tTraining Loss: 0.007030\n",
      "Epoch: 375/500 \tTraining Loss: 0.007180\n",
      "Epoch: 376/500 \tTraining Loss: 0.007133\n",
      "Epoch: 377/500 \tTraining Loss: 0.007052\n",
      "Epoch: 378/500 \tTraining Loss: 0.006990\n",
      "Epoch: 379/500 \tTraining Loss: 0.007432\n",
      "Epoch: 380/500 \tTraining Loss: 0.007196\n",
      "Epoch: 381/500 \tTraining Loss: 0.007370\n",
      "Epoch: 382/500 \tTraining Loss: 0.007048\n",
      "Epoch: 383/500 \tTraining Loss: 0.007317\n",
      "Epoch: 384/500 \tTraining Loss: 0.007342\n",
      "Epoch: 385/500 \tTraining Loss: 0.007243\n",
      "Epoch: 386/500 \tTraining Loss: 0.007334\n",
      "Epoch: 387/500 \tTraining Loss: 0.006975\n",
      "Epoch: 388/500 \tTraining Loss: 0.007029\n",
      "Epoch: 389/500 \tTraining Loss: 0.007117\n",
      "Epoch: 390/500 \tTraining Loss: 0.007274\n",
      "Epoch: 391/500 \tTraining Loss: 0.007220\n",
      "Epoch: 392/500 \tTraining Loss: 0.007695\n",
      "Epoch: 393/500 \tTraining Loss: 0.006970\n",
      "Epoch: 394/500 \tTraining Loss: 0.007041\n",
      "Epoch: 395/500 \tTraining Loss: 0.007551\n",
      "Epoch: 396/500 \tTraining Loss: 0.007097\n",
      "Epoch: 397/500 \tTraining Loss: 0.007129\n",
      "Epoch: 398/500 \tTraining Loss: 0.007335\n",
      "Epoch: 399/500 \tTraining Loss: 0.007176\n",
      "Epoch: 400/500 \tTraining Loss: 0.006865\n",
      "Epoch: 401/500 \tTraining Loss: 0.007155\n",
      "Epoch: 402/500 \tTraining Loss: 0.007201\n",
      "Epoch: 403/500 \tTraining Loss: 0.007400\n",
      "Epoch: 404/500 \tTraining Loss: 0.007444\n",
      "Epoch: 405/500 \tTraining Loss: 0.007164\n",
      "Epoch: 406/500 \tTraining Loss: 0.007066\n",
      "Epoch: 407/500 \tTraining Loss: 0.007378\n",
      "Epoch: 408/500 \tTraining Loss: 0.006940\n",
      "Epoch: 409/500 \tTraining Loss: 0.007168\n",
      "Epoch: 410/500 \tTraining Loss: 0.006956\n",
      "Epoch: 411/500 \tTraining Loss: 0.007165\n",
      "Epoch: 412/500 \tTraining Loss: 0.006924\n",
      "Epoch: 413/500 \tTraining Loss: 0.007229\n",
      "Epoch: 414/500 \tTraining Loss: 0.006974\n",
      "Epoch: 415/500 \tTraining Loss: 0.006787\n",
      "Epoch: 416/500 \tTraining Loss: 0.007193\n",
      "Epoch: 417/500 \tTraining Loss: 0.007787\n",
      "Epoch: 418/500 \tTraining Loss: 0.007128\n",
      "Epoch: 419/500 \tTraining Loss: 0.007143\n",
      "Epoch: 420/500 \tTraining Loss: 0.007288\n",
      "Epoch: 421/500 \tTraining Loss: 0.007366\n",
      "Epoch: 422/500 \tTraining Loss: 0.007355\n",
      "Epoch: 423/500 \tTraining Loss: 0.007388\n",
      "Epoch: 424/500 \tTraining Loss: 0.007011\n",
      "Epoch: 425/500 \tTraining Loss: 0.006933\n",
      "Epoch: 426/500 \tTraining Loss: 0.006938\n",
      "Epoch: 427/500 \tTraining Loss: 0.007185\n",
      "Epoch: 428/500 \tTraining Loss: 0.007691\n",
      "Epoch: 429/500 \tTraining Loss: 0.006921\n",
      "Epoch: 430/500 \tTraining Loss: 0.007205\n",
      "Epoch: 431/500 \tTraining Loss: 0.007089\n",
      "Epoch: 432/500 \tTraining Loss: 0.006932\n",
      "Epoch: 433/500 \tTraining Loss: 0.007192\n",
      "Epoch: 434/500 \tTraining Loss: 0.007295\n",
      "Epoch: 435/500 \tTraining Loss: 0.007410\n",
      "Epoch: 436/500 \tTraining Loss: 0.006987\n",
      "Epoch: 437/500 \tTraining Loss: 0.007158\n",
      "Epoch: 438/500 \tTraining Loss: 0.007100\n",
      "Epoch: 439/500 \tTraining Loss: 0.007621\n",
      "Epoch: 440/500 \tTraining Loss: 0.007338\n",
      "Epoch: 441/500 \tTraining Loss: 0.006912\n",
      "Epoch: 442/500 \tTraining Loss: 0.007263\n",
      "Epoch: 443/500 \tTraining Loss: 0.007435\n",
      "Epoch: 444/500 \tTraining Loss: 0.007127\n",
      "Epoch: 445/500 \tTraining Loss: 0.007080\n",
      "Epoch: 446/500 \tTraining Loss: 0.007522\n",
      "Epoch: 447/500 \tTraining Loss: 0.006935\n",
      "Epoch: 448/500 \tTraining Loss: 0.007223\n",
      "Epoch: 449/500 \tTraining Loss: 0.006954\n",
      "Epoch: 450/500 \tTraining Loss: 0.007496\n",
      "Epoch: 451/500 \tTraining Loss: 0.007019\n",
      "Epoch: 452/500 \tTraining Loss: 0.008297\n",
      "Epoch: 453/500 \tTraining Loss: 0.007833\n",
      "Epoch: 454/500 \tTraining Loss: 0.007482\n",
      "Epoch: 455/500 \tTraining Loss: 0.007640\n",
      "Epoch: 456/500 \tTraining Loss: 0.007792\n",
      "Epoch: 457/500 \tTraining Loss: 0.007286\n",
      "Epoch: 458/500 \tTraining Loss: 0.007046\n",
      "Epoch: 459/500 \tTraining Loss: 0.006944\n",
      "Epoch: 460/500 \tTraining Loss: 0.006902\n",
      "Epoch: 461/500 \tTraining Loss: 0.007155\n",
      "Epoch: 462/500 \tTraining Loss: 0.007045\n",
      "Epoch: 463/500 \tTraining Loss: 0.006976\n",
      "Epoch: 464/500 \tTraining Loss: 0.007134\n",
      "Epoch: 465/500 \tTraining Loss: 0.006987\n",
      "Epoch: 466/500 \tTraining Loss: 0.007218\n",
      "Epoch: 467/500 \tTraining Loss: 0.007274\n",
      "Epoch: 468/500 \tTraining Loss: 0.006910\n",
      "Epoch: 469/500 \tTraining Loss: 0.007116\n",
      "Epoch: 470/500 \tTraining Loss: 0.007291\n",
      "Epoch: 471/500 \tTraining Loss: 0.006858\n",
      "Epoch: 472/500 \tTraining Loss: 0.007350\n",
      "Epoch: 473/500 \tTraining Loss: 0.007118\n",
      "Epoch: 474/500 \tTraining Loss: 0.007345\n",
      "Epoch: 475/500 \tTraining Loss: 0.007212\n",
      "Epoch: 476/500 \tTraining Loss: 0.007595\n",
      "Epoch: 477/500 \tTraining Loss: 0.007135\n",
      "Epoch: 478/500 \tTraining Loss: 0.007097\n",
      "Epoch: 479/500 \tTraining Loss: 0.006971\n",
      "Epoch: 480/500 \tTraining Loss: 0.007404\n",
      "Epoch: 481/500 \tTraining Loss: 0.007054\n",
      "Epoch: 482/500 \tTraining Loss: 0.007648\n",
      "Epoch: 483/500 \tTraining Loss: 0.007044\n",
      "Epoch: 484/500 \tTraining Loss: 0.006983\n",
      "Epoch: 485/500 \tTraining Loss: 0.007115\n",
      "Epoch: 486/500 \tTraining Loss: 0.007105\n",
      "Epoch: 487/500 \tTraining Loss: 0.007503\n",
      "Epoch: 488/500 \tTraining Loss: 0.008100\n",
      "Epoch: 489/500 \tTraining Loss: 0.007331\n",
      "Epoch: 490/500 \tTraining Loss: 0.007124\n",
      "Epoch: 491/500 \tTraining Loss: 0.007396\n",
      "Epoch: 492/500 \tTraining Loss: 0.007137\n",
      "Epoch: 493/500 \tTraining Loss: 0.007782\n",
      "Epoch: 494/500 \tTraining Loss: 0.007256\n",
      "Epoch: 495/500 \tTraining Loss: 0.006556\n",
      "Epoch: 496/500 \tTraining Loss: 0.006915\n",
      "Epoch: 497/500 \tTraining Loss: 0.007073\n",
      "Epoch: 498/500 \tTraining Loss: 0.007214\n",
      "Epoch: 499/500 \tTraining Loss: 0.007004\n",
      "Epoch: 500/500 \tTraining Loss: 0.006913\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500 \n",
    "\n",
    "model.to(device)    # bring the model to gpu\n",
    "model.train()       # prep model for training\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        #bring data and target to gpu\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    print('Epoch: {}/{} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1,\n",
    "        n_epochs, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "\n",
    "# Save the model  -- I already saved this and submitted\n",
    "torch.save(model.state_dict(), 'PyTorch_Model/NN_mish_Drop_L2_Huber_500Epoch_Quantile.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the saved state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4243952   0.34050685  1.5166202  ... -0.37391746  1.4139068\n",
      "  -0.6338669 ]\n",
      " [-1.0003517  -0.36588454 -1.0678427  ...  0.6701116  -1.0132602\n",
      "   0.94387686]\n",
      " [-0.12843658  0.69604397 -0.0389767  ...  0.42974162 -0.13418932\n",
      "   0.24576324]\n",
      " ...\n",
      " [ 0.55434453 -0.04057433  0.4893232  ... -0.37930888  0.55530435\n",
      "  -0.387529  ]\n",
      " [-1.2211802   1.1408283  -1.0243862  ...  1.6626871  -1.2770529\n",
      "   1.6462718 ]\n",
      " [-0.24878968 -1.9715118  -0.4748982  ... -1.2553831  -0.2162917\n",
      "  -0.9672264 ]]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PyTorch_Model/NN_mish_Drop_L2_Huber_500Epoch_Quantile.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()           # prep model for *evaluation*\n",
    "model.to(device)\n",
    "with torch.no_grad():  # turn off gradient to save memory\n",
    "    y_predNN_torch = model(X_test_torch.to(device))\n",
    "\n",
    "y_predNN_normal = y_predNN_torch.cpu().numpy()     # convert to numpy array\n",
    "y_test_normal = y_test_torch.cpu().numpy()\n",
    "print(y_predNN_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network scores in normal distribution: r2 = 0.9850587454131797, mape = 0.45778512954711914\n"
     ]
    }
   ],
   "source": [
    "mape = mean_absolute_percentage_error(y_test_normal, y_predNN_normal)\n",
    "r2 = r2_score(y_test_normal, y_predNN_normal)\n",
    "print(f\"Neural Network scores in normal distribution: r2 = {r2}, mape = {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network scores in actual distribution: r2 = 0.9659107838774554, mape = 0.048858523086829954\n"
     ]
    }
   ],
   "source": [
    "y_predNN = quantile.inverse_transform(y_predNN_normal)\n",
    "mape = mean_absolute_percentage_error(y_test, y_predNN.astype('float64'))\n",
    "r2 = r2_score(y_test, y_predNN)\n",
    "\n",
    "print(f\"Neural Network scores in actual distribution: r2 = {r2}, mape = {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36564744, 0.16922301, 0.06585995, 1.2958155 , 0.9326011 ,\n",
       "       0.07740711, 0.09295716, 0.10414718, 0.07343426, 0.01789803,\n",
       "       0.08224675, 0.28212318, 0.1394976 , 0.16284454, 0.87539864,\n",
       "       0.1371827 , 0.212249  , 0.07661303, 0.21062003, 0.10091688,\n",
       "       0.12237698, 0.18716325, 0.54712164, 0.18030716, 0.10481988,\n",
       "       0.07119735, 0.53709143, 0.03709681, 0.01620716, 0.09367422,\n",
       "       0.07000802, 0.44661102, 0.26816434, 0.21954523, 0.16532381,\n",
       "       0.8576233 , 0.7063503 , 0.10227039, 0.17975692, 1.260127  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predNN[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39918807, 0.18489788, 0.07076792, 1.603528  , 1.0806105 ,\n",
       "       0.06908134, 0.08818348, 0.10897039, 0.06826332, 0.01860524,\n",
       "       0.08134046, 0.28987974, 0.12975076, 0.17156978, 1.2115381 ,\n",
       "       0.15436813, 0.22705919, 0.06763056, 0.22765867, 0.10868868,\n",
       "       0.11713055, 0.19952142, 0.66621888, 0.15975925, 0.09596858,\n",
       "       0.06414993, 0.59720933, 0.03660972, 0.01633277, 0.08981635,\n",
       "       0.06839496, 0.49123496, 0.30692583, 0.2290165 , 0.18374079,\n",
       "       1.2546521 , 0.83790588, 0.08270955, 0.18826026, 1.7111793 ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.36676340e-02, 1.85481800e-02, 1.16362342e-01, ...,\n",
       "        3.87540700e+01, 9.85093640e-02, 8.02679810e-02],\n",
       "       [1.88341180e-02, 1.49853250e-02, 3.32267860e-02, ...,\n",
       "        1.10768800e+02, 2.12428740e-02, 3.99188070e-01],\n",
       "       [4.82065450e-02, 2.26087420e-02, 7.29268908e-02, ...,\n",
       "        9.14076770e+01, 5.16740420e-02, 1.84897880e-01],\n",
       "       ...,\n",
       "       [7.48656170e-02, 1.68266440e-02, 9.27517236e-02, ...,\n",
       "        3.51032030e+01, 7.93257060e-02, 9.26706940e-02],\n",
       "       [1.48923780e-02, 2.82288120e-02, 3.50396958e-02, ...,\n",
       "        3.61880250e+02, 1.65773410e-02, 1.17031230e+00],\n",
       "       [4.29282640e-02, 7.86490400e-03, 5.24556696e-02, ...,\n",
       "        1.25165350e+01, 4.73894250e-02, 4.77122370e-02]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3816304 ,  0.28268605,  1.4829322 , ..., -0.30745506,\n",
       "         1.3684012 , -0.5699484 ],\n",
       "       [-1.0927829 , -0.31069657, -1.1450505 , ...,  0.7362669 ,\n",
       "        -1.0969453 ,  1.0197092 ],\n",
       "       [-0.11125448,  0.76144725,  0.03355394, ...,  0.54064226,\n",
       "        -0.11799674,  0.33428347],\n",
       "       ...,\n",
       "       [ 0.60861874,  0.03356267,  0.5383628 , ..., -0.40858847,\n",
       "         0.60639787, -0.4104572 ],\n",
       "       [-1.3439791 ,  1.4611413 , -1.0646684 , ...,  1.977942  ,\n",
       "        -1.3777595 ,  1.9264768 ],\n",
       "       [-0.24901262, -2.1335402 , -0.47832558, ..., -1.3961294 ,\n",
       "        -0.22524492, -1.1426171 ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.4243952 ,  0.34050685,  1.5166202 , ..., -0.37391746,\n",
       "         1.4139068 , -0.6338669 ],\n",
       "       [-1.0003517 , -0.36588454, -1.0678427 , ...,  0.6701116 ,\n",
       "        -1.0132602 ,  0.94387686],\n",
       "       [-0.12843658,  0.69604397, -0.0389767 , ...,  0.42974162,\n",
       "        -0.13418932,  0.24576324],\n",
       "       ...,\n",
       "       [ 0.55434453, -0.04057433,  0.4893232 , ..., -0.37930888,\n",
       "         0.55530435, -0.387529  ],\n",
       "       [-1.2211802 ,  1.1408283 , -1.0243862 , ...,  1.6626871 ,\n",
       "        -1.2770529 ,  1.6462718 ],\n",
       "       [-0.24878968, -1.9715118 , -0.4748982 , ..., -1.2553831 ,\n",
       "        -0.2162917 , -0.9672264 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predNN_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Pressure\n",
      "[-0.05614389 -0.17634071 -0.10705814 ... -0.06102885 -0.25876847\n",
      " -0.04244541]\n",
      "\n",
      "Positive Impulse\n",
      "[ 36.247574 103.81985   82.0914   ...  36.054928 271.06186   14.490158]\n",
      "\n",
      "Positive Pressure\n",
      "[0.07607594 0.36564744 0.16922301 ... 0.09471353 0.85150105 0.05646219]\n"
     ]
    }
   ],
   "source": [
    "# Negative Pressure\n",
    "print(\"Negative Pressure\")\n",
    "print(y_predNN[:,3])\n",
    "print()\n",
    "\n",
    "\n",
    "# Positive Impulse\n",
    "print(\"Positive Impulse\")\n",
    "print(y_predNN[:,5])\n",
    "print()\n",
    "\n",
    "# Positive Pressure\n",
    "print(\"Positive Pressure\")\n",
    "print(y_predNN[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
