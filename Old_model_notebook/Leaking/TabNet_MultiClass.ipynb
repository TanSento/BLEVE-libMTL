{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import timeit\n",
    "#import shap\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read-in and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tank Failure Pressure (bar)</th>\n",
       "      <th>Liquid Ratio (%)</th>\n",
       "      <th>Tank Width (m)</th>\n",
       "      <th>Tank Length (m)</th>\n",
       "      <th>Tank Height (m)</th>\n",
       "      <th>Height of BLEVE (m)</th>\n",
       "      <th>Tank Height with Gas (m)</th>\n",
       "      <th>Vapour Temerature (K)</th>\n",
       "      <th>Liquid Temerature (K)</th>\n",
       "      <th>Status</th>\n",
       "      <th>Stand-off Distance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.010208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.012350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.014577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.016878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.019250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Tank Failure Pressure (bar)  Liquid Ratio (%)  Tank Width (m)  \\\n",
       "0  B1                         24.5          0.519805             2.2   \n",
       "1  B1                         24.5          0.519805             2.2   \n",
       "2  B1                         24.5          0.519805             2.2   \n",
       "3  B1                         24.5          0.519805             2.2   \n",
       "4  B1                         24.5          0.519805             2.2   \n",
       "\n",
       "   Tank Length (m)  Tank Height (m)  Height of BLEVE (m)  \\\n",
       "0              6.0              1.0                  1.6   \n",
       "1              6.0              1.0                  1.6   \n",
       "2              6.0              1.0                  1.6   \n",
       "3              6.0              1.0                  1.6   \n",
       "4              6.0              1.0                  1.6   \n",
       "\n",
       "   Tank Height with Gas (m)   Vapour Temerature (K)   Liquid Temerature (K)  \\\n",
       "0                       0.4                   307.8                   339.0   \n",
       "1                       0.4                   307.8                   339.0   \n",
       "2                       0.4                   307.8                   339.0   \n",
       "3                       0.4                   307.8                   339.0   \n",
       "4                       0.4                   307.8                   339.0   \n",
       "\n",
       "      Status  Stand-off Distance    Target  \n",
       "0  Subcooled                 5.0  0.010208  \n",
       "1  Subcooled                 6.0  0.012350  \n",
       "2  Subcooled                 7.0  0.014577  \n",
       "3  Subcooled                 8.0  0.016878  \n",
       "4  Subcooled                 9.0  0.019250  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"data/arrival_time_COMPLETE.csv\")\n",
    "df2 = pd.read_csv(\"data/negative_duration_COMPLETE.csv\")\n",
    "df3 = pd.read_csv(\"data/negative_peak_time_COMPLETE.csv\")\n",
    "df4 = pd.read_csv(\"data/negative_pressure_COMPLETE.csv\")\n",
    "df5 = pd.read_csv(\"data/positive_duration_COMPLETE.csv\")\n",
    "df6 = pd.read_csv(\"data/positive_impulse_COMPLETE.csv\")\n",
    "df7 = pd.read_csv(\"data/positive_peak_time_COMPLETE.csv\")\n",
    "df8 = pd.read_csv(\"data/positive_pressure_COMPLETE.csv\")\n",
    "\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tank Failure Pressure (bar)</th>\n",
       "      <th>Liquid Ratio (%)</th>\n",
       "      <th>Tank Width (m)</th>\n",
       "      <th>Tank Length (m)</th>\n",
       "      <th>Tank Height (m)</th>\n",
       "      <th>Height of BLEVE (m)</th>\n",
       "      <th>Tank Height with Gas (m)</th>\n",
       "      <th>Vapour Temerature (K)</th>\n",
       "      <th>Liquid Temerature (K)</th>\n",
       "      <th>Status</th>\n",
       "      <th>Stand-off Distance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.006817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.007816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.008326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.008817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35995</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35996</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35997</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35998</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Tank Failure Pressure (bar)  Liquid Ratio (%)  Tank Width (m)  \\\n",
       "0        B1                     24.50000          0.519805             2.2   \n",
       "1        B1                     24.50000          0.519805             2.2   \n",
       "2        B1                     24.50000          0.519805             2.2   \n",
       "3        B1                     24.50000          0.519805             2.2   \n",
       "4        B1                     24.50000          0.519805             2.2   \n",
       "...     ...                          ...               ...             ...   \n",
       "35995  P500                     11.40239          0.442321             1.4   \n",
       "35996  P500                     11.40239          0.442321             1.4   \n",
       "35997  P500                     11.40239          0.442321             1.4   \n",
       "35998  P500                     11.40239          0.442321             1.4   \n",
       "35999  P500                     11.40239          0.442321             1.4   \n",
       "\n",
       "       Tank Length (m)  Tank Height (m)  Height of BLEVE (m)  \\\n",
       "0                  6.0              1.0                  1.6   \n",
       "1                  6.0              1.0                  1.6   \n",
       "2                  6.0              1.0                  1.6   \n",
       "3                  6.0              1.0                  1.6   \n",
       "4                  6.0              1.0                  1.6   \n",
       "...                ...              ...                  ...   \n",
       "35995              2.8              1.2                  1.4   \n",
       "35996              2.8              1.2                  1.4   \n",
       "35997              2.8              1.2                  1.4   \n",
       "35998              2.8              1.2                  1.4   \n",
       "35999              2.8              1.2                  1.4   \n",
       "\n",
       "       Tank Height with Gas (m)   Vapour Temerature (K)  \\\n",
       "0                           0.4                   307.8   \n",
       "1                           0.4                   307.8   \n",
       "2                           0.4                   307.8   \n",
       "3                           0.4                   307.8   \n",
       "4                           0.4                   307.8   \n",
       "...                         ...                     ...   \n",
       "35995                       0.8                   388.2   \n",
       "35996                       0.8                   388.2   \n",
       "35997                       0.8                   388.2   \n",
       "35998                       0.8                   388.2   \n",
       "35999                       0.8                   388.2   \n",
       "\n",
       "        Liquid Temerature (K)  Status  Stand-off Distance    Target  \n",
       "0                       339.0       0                 5.0  0.006817  \n",
       "1                       339.0       0                 6.0  0.007302  \n",
       "2                       339.0       0                 7.0  0.007816  \n",
       "3                       339.0       0                 8.0  0.008326  \n",
       "4                       339.0       0                 9.0  0.008817  \n",
       "...                       ...     ...                 ...       ...  \n",
       "35995                   366.7       1                36.0       NaN  \n",
       "35996                   366.7       1                37.0       NaN  \n",
       "35997                   366.7       1                38.0       NaN  \n",
       "35998                   366.7       1                39.0       NaN  \n",
       "35999                   366.7       1                40.0       NaN  \n",
       "\n",
       "[36000 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding 'Status' feature into 0 and 1 \n",
    "# 0 for Subcooled and 1 for Superheated\n",
    "# Doing Similarly for ID (Do we need dummy encoding ??)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()\n",
    "\n",
    "df5['Status'] = LE.fit_transform(df5['Status'])\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tank Failure Pressure (bar)</th>\n",
       "      <th>Liquid Ratio (%)</th>\n",
       "      <th>Tank Width (m)</th>\n",
       "      <th>Tank Length (m)</th>\n",
       "      <th>Tank Height (m)</th>\n",
       "      <th>Height of BLEVE (m)</th>\n",
       "      <th>Tank Height with Gas (m)</th>\n",
       "      <th>Vapour Temerature (K)</th>\n",
       "      <th>Liquid Temerature (K)</th>\n",
       "      <th>Status</th>\n",
       "      <th>Stand-off Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28795</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28796</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28797</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28798</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28799</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28800 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tank Failure Pressure (bar)  Liquid Ratio (%)  Tank Width (m)  \\\n",
       "0                         24.50000          0.519805             2.2   \n",
       "1                         24.50000          0.519805             2.2   \n",
       "2                         24.50000          0.519805             2.2   \n",
       "3                         24.50000          0.519805             2.2   \n",
       "4                         24.50000          0.519805             2.2   \n",
       "...                            ...               ...             ...   \n",
       "28795                     33.17377          0.372041             1.0   \n",
       "28796                     33.17377          0.372041             1.0   \n",
       "28797                     33.17377          0.372041             1.0   \n",
       "28798                     33.17377          0.372041             1.0   \n",
       "28799                     33.17377          0.372041             1.0   \n",
       "\n",
       "       Tank Length (m)  Tank Height (m)  Height of BLEVE (m)  \\\n",
       "0                  6.0              1.0                  1.6   \n",
       "1                  6.0              1.0                  1.6   \n",
       "2                  6.0              1.0                  1.6   \n",
       "3                  6.0              1.0                  1.6   \n",
       "4                  6.0              1.0                  1.6   \n",
       "...                ...              ...                  ...   \n",
       "28795              2.2              0.6                  0.2   \n",
       "28796              2.2              0.6                  0.2   \n",
       "28797              2.2              0.6                  0.2   \n",
       "28798              2.2              0.6                  0.2   \n",
       "28799              2.2              0.6                  0.2   \n",
       "\n",
       "       Tank Height with Gas (m)   Vapour Temerature (K)  \\\n",
       "0                           0.4                   307.8   \n",
       "1                           0.4                   307.8   \n",
       "2                           0.4                   307.8   \n",
       "3                           0.4                   307.8   \n",
       "4                           0.4                   307.8   \n",
       "...                         ...                     ...   \n",
       "28795                       0.4                   312.7   \n",
       "28796                       0.4                   312.7   \n",
       "28797                       0.4                   312.7   \n",
       "28798                       0.4                   312.7   \n",
       "28799                       0.4                   312.7   \n",
       "\n",
       "        Liquid Temerature (K)  Status  Stand-off Distance  \n",
       "0                       339.0       0                 5.0  \n",
       "1                       339.0       0                 6.0  \n",
       "2                       339.0       0                 7.0  \n",
       "3                       339.0       0                 8.0  \n",
       "4                       339.0       0                 9.0  \n",
       "...                       ...     ...                 ...  \n",
       "28795                   318.2       0                36.0  \n",
       "28796                   318.2       0                37.0  \n",
       "28797                   318.2       0                38.0  \n",
       "28798                   318.2       0                39.0  \n",
       "28799                   318.2       0                40.0  \n",
       "\n",
       "[28800 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df5.drop(['ID','Target'], axis=1)[:28800]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.006817\n",
       "1        0.007302\n",
       "2        0.007816\n",
       "3        0.008326\n",
       "4        0.008817\n",
       "           ...   \n",
       "28795    0.012178\n",
       "28796    0.012275\n",
       "28797    0.012374\n",
       "28798    0.012477\n",
       "28799    0.012573\n",
       "Name: Target, Length: 28800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y5 = df5['Target'][:28800]\n",
    "y5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df1['Target'][:28800]\n",
    "y2 = df2['Target'][:28800]\n",
    "y3 = df3['Target'][:28800]\n",
    "y4 = df4['Target'][:28800]\n",
    "y6 = df6['Target'][:28800]\n",
    "y7 = df7['Target'][:28800]\n",
    "y8 = df8['Target'][:28800]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 11)\n",
      "(7200, 11)\n"
     ]
    }
   ],
   "source": [
    "X_traindf, X_testdf, y1_train, y1_test = train_test_split(X, y1, test_size=0.25, random_state=42)\n",
    "print(X_traindf.shape)\n",
    "print(X_testdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_train, y2_test = train_test_split(y2, test_size=0.25, random_state=42)\n",
    "y3_train, y3_test = train_test_split(y3, test_size=0.25, random_state=42)\n",
    "y4_train, y4_test = train_test_split(y4, test_size=0.25, random_state=42)\n",
    "y5_train, y5_test = train_test_split(y5, test_size=0.25, random_state=42)\n",
    "y6_train, y6_test = train_test_split(y6, test_size=0.25, random_state=42)\n",
    "y7_train, y7_test = train_test_split(y7, test_size=0.25, random_state=42)\n",
    "y8_train, y8_test = train_test_split(y8, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01284534, 0.01444022, 0.10035855, ..., 0.10251021, 0.07990494,\n",
       "       0.01143898])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y7_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1268162 , 0.62633157, 0.04595783, ..., 0.02130297, 0.13602383,\n",
       "       2.3361142 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y8_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((y1_train.values.reshape(-1,1), y2_train.values.reshape(-1,1), y3_train.values.reshape(-1,1), \n",
    "                          y4_train.values.reshape(-1,1), y5_train.values.reshape(-1,1), y6_train.values.reshape(-1,1),\n",
    "                          y7_train.values.reshape(-1,1), y8_train.values.reshape(-1,1)), axis=1)\n",
    "\n",
    "y_test = np.concatenate((y1_test.values.reshape(-1,1), y2_test.values.reshape(-1,1), y3_test.values.reshape(-1,1), \n",
    "                          y4_test.values.reshape(-1,1), y5_test.values.reshape(-1,1), y6_test.values.reshape(-1,1),\n",
    "                          y7_test.values.reshape(-1,1), y8_test.values.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 8)\n",
      "(7200, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.11694350e-02, 1.56393350e-02, 2.44314220e-02, ...,\n",
       "        2.86203890e+02, 1.28453400e-02, 1.12681620e+00],\n",
       "       [1.24119570e-02, 1.88029450e-02, 2.82499930e-02, ...,\n",
       "        1.80425900e+02, 1.44402250e-02, 6.26331570e-01],\n",
       "       [9.50448220e-02, 1.51018300e-02, 1.09446822e-01, ...,\n",
       "        1.49908640e+01, 1.00358550e-01, 4.59578340e-02],\n",
       "       ...,\n",
       "       [9.67233260e-02, 9.91223000e-03, 1.08929812e-01, ...,\n",
       "        7.44885020e+00, 1.02510210e-01, 2.13029660e-02],\n",
       "       [7.58939240e-02, 2.09730370e-02, 9.80206378e-02, ...,\n",
       "        6.19014550e+01, 7.99049360e-02, 1.36023830e-01],\n",
       "       [1.00712810e-02, 3.38855160e-02, 3.30022184e-02, ...,\n",
       "        7.96478820e+02, 1.14389770e-02, 2.33611420e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.36676340e-02, 1.85481800e-02, 1.16362342e-01, ...,\n",
       "        3.87540700e+01, 9.85093640e-02, 8.02679810e-02],\n",
       "       [1.88341180e-02, 1.49853250e-02, 3.32267860e-02, ...,\n",
       "        1.10768800e+02, 2.12428740e-02, 3.99188070e-01],\n",
       "       [4.82065450e-02, 2.26087420e-02, 7.29268908e-02, ...,\n",
       "        9.14076770e+01, 5.16740420e-02, 1.84897880e-01],\n",
       "       ...,\n",
       "       [7.48656170e-02, 1.68266440e-02, 9.27517236e-02, ...,\n",
       "        3.51032030e+01, 7.93257060e-02, 9.26706940e-02],\n",
       "       [1.48923780e-02, 2.82288120e-02, 3.50396958e-02, ...,\n",
       "        3.61880250e+02, 1.65773410e-02, 1.17031230e+00],\n",
       "       [4.29282640e-02, 7.86490400e-03, 5.24556696e-02, ...,\n",
       "        1.25165350e+01, 4.73894250e-02, 4.77122370e-02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization and Power Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing both X_train and X_test using standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_traindf)\n",
    "X_test = scaler.transform(X_testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check if it succeeded\n",
    "# df_stdscal = pd.DataFrame(X_train)\n",
    "# df_stdscal.hist(figsize = (20,20), bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "y_train_normal = quantile.fit_transform(y_train)\n",
    "y_test_normal = quantile.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check if it succeeded\n",
    "# df_stdscal = pd.DataFrame(y_train)\n",
    "# df_stdscal.hist(figsize = (20,20), bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "\n",
    "torch.backends.cudnn.version() , torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16238    0\n",
       "26642    1\n",
       "5504     0\n",
       "27453    0\n",
       "766      1\n",
       "        ..\n",
       "21575    1\n",
       "5390     0\n",
       "860      1\n",
       "15795    0\n",
       "23654    1\n",
       "Name: Status, Length: 21600, dtype: int32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking categorical feature -- Status\n",
    "X_traindf.iloc[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7424062 ,  1.34697151, -0.7424062 , ...,  1.34697151,\n",
       "       -0.7424062 ,  1.34697151])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\buita\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "clf = TabNetRegressor(cat_dims=[2], cat_emb_dim=[2], cat_idxs=[9])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuberLoss - 500 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.37551 | train_mae: 0.56002 | train_rmse: 0.74619 | train_mse: 0.55681 | valid_mae: 0.55575 | valid_rmse: 0.74032 | valid_mse: 0.54808 |  0:00:06s\n",
      "epoch 1  | loss: 0.17681 | train_mae: 0.3689  | train_rmse: 0.50566 | train_mse: 0.25569 | valid_mae: 0.36952 | valid_rmse: 0.50633 | valid_mse: 0.25637 |  0:00:10s\n",
      "epoch 2  | loss: 0.0877  | train_mae: 0.25647 | train_rmse: 0.36249 | train_mse: 0.1314  | valid_mae: 0.25662 | valid_rmse: 0.36313 | valid_mse: 0.13187 |  0:00:15s\n",
      "epoch 3  | loss: 0.05746 | train_mae: 0.23024 | train_rmse: 0.32283 | train_mse: 0.10422 | valid_mae: 0.22989 | valid_rmse: 0.32367 | valid_mse: 0.10476 |  0:00:19s\n",
      "epoch 4  | loss: 0.0504  | train_mae: 0.21852 | train_rmse: 0.29888 | train_mse: 0.08933 | valid_mae: 0.21972 | valid_rmse: 0.30151 | valid_mse: 0.09091 |  0:00:23s\n",
      "epoch 5  | loss: 0.04483 | train_mae: 0.20157 | train_rmse: 0.28513 | train_mse: 0.0813  | valid_mae: 0.20213 | valid_rmse: 0.28633 | valid_mse: 0.08198 |  0:00:28s\n",
      "epoch 6  | loss: 0.03851 | train_mae: 0.16685 | train_rmse: 0.24181 | train_mse: 0.05847 | valid_mae: 0.16738 | valid_rmse: 0.24383 | valid_mse: 0.05945 |  0:00:32s\n",
      "epoch 7  | loss: 0.0351  | train_mae: 0.17291 | train_rmse: 0.24938 | train_mse: 0.06219 | valid_mae: 0.17309 | valid_rmse: 0.25051 | valid_mse: 0.06275 |  0:00:36s\n",
      "epoch 8  | loss: 0.03289 | train_mae: 0.14732 | train_rmse: 0.22124 | train_mse: 0.04895 | valid_mae: 0.14793 | valid_rmse: 0.22318 | valid_mse: 0.04981 |  0:00:41s\n",
      "epoch 9  | loss: 0.03473 | train_mae: 0.15976 | train_rmse: 0.23084 | train_mse: 0.05329 | valid_mae: 0.15978 | valid_rmse: 0.23082 | valid_mse: 0.05328 |  0:00:45s\n",
      "epoch 10 | loss: 0.02978 | train_mae: 0.14147 | train_rmse: 0.21149 | train_mse: 0.04473 | valid_mae: 0.14053 | valid_rmse: 0.20985 | valid_mse: 0.04404 |  0:00:50s\n",
      "epoch 11 | loss: 0.02809 | train_mae: 0.15184 | train_rmse: 0.22072 | train_mse: 0.04872 | valid_mae: 0.15242 | valid_rmse: 0.22165 | valid_mse: 0.04913 |  0:00:54s\n",
      "epoch 12 | loss: 0.02735 | train_mae: 0.13607 | train_rmse: 0.20412 | train_mse: 0.04166 | valid_mae: 0.13605 | valid_rmse: 0.205   | valid_mse: 0.04202 |  0:00:59s\n",
      "epoch 13 | loss: 0.03093 | train_mae: 0.13821 | train_rmse: 0.20657 | train_mse: 0.04267 | valid_mae: 0.13751 | valid_rmse: 0.2065  | valid_mse: 0.04264 |  0:01:03s\n",
      "epoch 14 | loss: 0.02619 | train_mae: 0.142   | train_rmse: 0.2066  | train_mse: 0.04268 | valid_mae: 0.14108 | valid_rmse: 0.20582 | valid_mse: 0.04236 |  0:01:07s\n",
      "epoch 15 | loss: 0.02428 | train_mae: 0.13061 | train_rmse: 0.19756 | train_mse: 0.03903 | valid_mae: 0.13109 | valid_rmse: 0.19838 | valid_mse: 0.03935 |  0:01:11s\n",
      "epoch 16 | loss: 0.0232  | train_mae: 0.12381 | train_rmse: 0.18577 | train_mse: 0.03451 | valid_mae: 0.12375 | valid_rmse: 0.1866  | valid_mse: 0.03482 |  0:01:15s\n",
      "epoch 17 | loss: 0.02305 | train_mae: 0.12024 | train_rmse: 0.18459 | train_mse: 0.03407 | valid_mae: 0.11996 | valid_rmse: 0.18492 | valid_mse: 0.03419 |  0:01:19s\n",
      "epoch 18 | loss: 0.02272 | train_mae: 0.11721 | train_rmse: 0.17805 | train_mse: 0.0317  | valid_mae: 0.11684 | valid_rmse: 0.17779 | valid_mse: 0.03161 |  0:01:24s\n",
      "epoch 19 | loss: 0.02215 | train_mae: 0.12443 | train_rmse: 0.18875 | train_mse: 0.03563 | valid_mae: 0.12411 | valid_rmse: 0.18861 | valid_mse: 0.03558 |  0:01:28s\n",
      "epoch 20 | loss: 0.02204 | train_mae: 0.12463 | train_rmse: 0.18707 | train_mse: 0.03499 | valid_mae: 0.12548 | valid_rmse: 0.18845 | valid_mse: 0.03551 |  0:01:32s\n",
      "epoch 21 | loss: 0.02193 | train_mae: 0.11787 | train_rmse: 0.17971 | train_mse: 0.0323  | valid_mae: 0.11769 | valid_rmse: 0.1801  | valid_mse: 0.03244 |  0:01:37s\n",
      "epoch 22 | loss: 0.02236 | train_mae: 0.1348  | train_rmse: 0.19683 | train_mse: 0.03874 | valid_mae: 0.13515 | valid_rmse: 0.19695 | valid_mse: 0.03879 |  0:01:41s\n",
      "epoch 23 | loss: 0.02083 | train_mae: 0.11887 | train_rmse: 0.17746 | train_mse: 0.03149 | valid_mae: 0.11922 | valid_rmse: 0.17881 | valid_mse: 0.03197 |  0:01:45s\n",
      "epoch 24 | loss: 0.02142 | train_mae: 0.1139  | train_rmse: 0.17675 | train_mse: 0.03124 | valid_mae: 0.11395 | valid_rmse: 0.17699 | valid_mse: 0.03133 |  0:01:50s\n",
      "epoch 25 | loss: 0.01959 | train_mae: 0.13241 | train_rmse: 0.18874 | train_mse: 0.03562 | valid_mae: 0.13318 | valid_rmse: 0.19041 | valid_mse: 0.03625 |  0:01:54s\n",
      "epoch 26 | loss: 0.02133 | train_mae: 0.11035 | train_rmse: 0.16821 | train_mse: 0.0283  | valid_mae: 0.11057 | valid_rmse: 0.16863 | valid_mse: 0.02844 |  0:01:58s\n",
      "epoch 27 | loss: 0.02004 | train_mae: 0.10647 | train_rmse: 0.1649  | train_mse: 0.02719 | valid_mae: 0.10663 | valid_rmse: 0.16655 | valid_mse: 0.02774 |  0:02:02s\n",
      "epoch 28 | loss: 0.01879 | train_mae: 0.12949 | train_rmse: 0.18423 | train_mse: 0.03394 | valid_mae: 0.12962 | valid_rmse: 0.18534 | valid_mse: 0.03435 |  0:02:06s\n",
      "epoch 29 | loss: 0.0201  | train_mae: 0.12369 | train_rmse: 0.1861  | train_mse: 0.03463 | valid_mae: 0.12365 | valid_rmse: 0.18615 | valid_mse: 0.03465 |  0:02:10s\n",
      "epoch 30 | loss: 0.01926 | train_mae: 0.10982 | train_rmse: 0.16768 | train_mse: 0.02812 | valid_mae: 0.11113 | valid_rmse: 0.17004 | valid_mse: 0.02891 |  0:02:14s\n",
      "epoch 31 | loss: 0.01941 | train_mae: 0.10705 | train_rmse: 0.16353 | train_mse: 0.02674 | valid_mae: 0.10815 | valid_rmse: 0.16599 | valid_mse: 0.02755 |  0:02:18s\n",
      "epoch 32 | loss: 0.01878 | train_mae: 0.12442 | train_rmse: 0.18323 | train_mse: 0.03357 | valid_mae: 0.12421 | valid_rmse: 0.18375 | valid_mse: 0.03377 |  0:02:22s\n",
      "epoch 33 | loss: 0.0188  | train_mae: 0.1174  | train_rmse: 0.17367 | train_mse: 0.03016 | valid_mae: 0.11764 | valid_rmse: 0.1749  | valid_mse: 0.03059 |  0:02:26s\n",
      "epoch 34 | loss: 0.01848 | train_mae: 0.12002 | train_rmse: 0.17438 | train_mse: 0.03041 | valid_mae: 0.11991 | valid_rmse: 0.17588 | valid_mse: 0.03093 |  0:02:30s\n",
      "epoch 35 | loss: 0.01779 | train_mae: 0.10411 | train_rmse: 0.16119 | train_mse: 0.02598 | valid_mae: 0.10451 | valid_rmse: 0.16256 | valid_mse: 0.02643 |  0:02:34s\n",
      "epoch 36 | loss: 0.0174  | train_mae: 0.10504 | train_rmse: 0.16021 | train_mse: 0.02567 | valid_mae: 0.10584 | valid_rmse: 0.16213 | valid_mse: 0.02629 |  0:02:38s\n",
      "epoch 37 | loss: 0.01843 | train_mae: 0.13037 | train_rmse: 0.18259 | train_mse: 0.03334 | valid_mae: 0.13058 | valid_rmse: 0.18357 | valid_mse: 0.0337  |  0:02:42s\n",
      "epoch 38 | loss: 0.01739 | train_mae: 0.10499 | train_rmse: 0.16422 | train_mse: 0.02697 | valid_mae: 0.1057  | valid_rmse: 0.16541 | valid_mse: 0.02736 |  0:02:46s\n",
      "epoch 39 | loss: 0.01763 | train_mae: 0.10283 | train_rmse: 0.15685 | train_mse: 0.0246  | valid_mae: 0.10365 | valid_rmse: 0.15923 | valid_mse: 0.02535 |  0:02:50s\n",
      "epoch 40 | loss: 0.01811 | train_mae: 0.12117 | train_rmse: 0.18058 | train_mse: 0.03261 | valid_mae: 0.12205 | valid_rmse: 0.18288 | valid_mse: 0.03344 |  0:02:54s\n",
      "epoch 41 | loss: 0.01765 | train_mae: 0.10948 | train_rmse: 0.17132 | train_mse: 0.02935 | valid_mae: 0.10991 | valid_rmse: 0.17171 | valid_mse: 0.02948 |  0:02:58s\n",
      "epoch 42 | loss: 0.01756 | train_mae: 0.11091 | train_rmse: 0.16223 | train_mse: 0.02632 | valid_mae: 0.11182 | valid_rmse: 0.1644  | valid_mse: 0.02703 |  0:03:03s\n",
      "epoch 43 | loss: 0.0188  | train_mae: 0.12033 | train_rmse: 0.17796 | train_mse: 0.03167 | valid_mae: 0.12061 | valid_rmse: 0.17802 | valid_mse: 0.03169 |  0:03:07s\n",
      "epoch 44 | loss: 0.01728 | train_mae: 0.11162 | train_rmse: 0.16472 | train_mse: 0.02713 | valid_mae: 0.11221 | valid_rmse: 0.1659  | valid_mse: 0.02752 |  0:03:11s\n",
      "epoch 45 | loss: 0.01676 | train_mae: 0.10195 | train_rmse: 0.15598 | train_mse: 0.02433 | valid_mae: 0.10264 | valid_rmse: 0.15732 | valid_mse: 0.02475 |  0:03:14s\n",
      "epoch 46 | loss: 0.01651 | train_mae: 0.12052 | train_rmse: 0.17408 | train_mse: 0.0303  | valid_mae: 0.12062 | valid_rmse: 0.1754  | valid_mse: 0.03077 |  0:03:18s\n",
      "epoch 47 | loss: 0.01765 | train_mae: 0.10994 | train_rmse: 0.16273 | train_mse: 0.02648 | valid_mae: 0.10953 | valid_rmse: 0.16296 | valid_mse: 0.02655 |  0:03:23s\n",
      "epoch 48 | loss: 0.01783 | train_mae: 0.11009 | train_rmse: 0.16504 | train_mse: 0.02724 | valid_mae: 0.1108  | valid_rmse: 0.16681 | valid_mse: 0.02783 |  0:03:27s\n",
      "epoch 49 | loss: 0.0166  | train_mae: 0.10372 | train_rmse: 0.15907 | train_mse: 0.0253  | valid_mae: 0.10376 | valid_rmse: 0.1592  | valid_mse: 0.02534 |  0:03:32s\n",
      "epoch 50 | loss: 0.01607 | train_mae: 0.10036 | train_rmse: 0.15661 | train_mse: 0.02453 | valid_mae: 0.10075 | valid_rmse: 0.1571  | valid_mse: 0.02468 |  0:03:36s\n",
      "epoch 51 | loss: 0.0153  | train_mae: 0.12028 | train_rmse: 0.17521 | train_mse: 0.0307  | valid_mae: 0.12071 | valid_rmse: 0.17552 | valid_mse: 0.03081 |  0:03:41s\n",
      "epoch 52 | loss: 0.01657 | train_mae: 0.10443 | train_rmse: 0.15758 | train_mse: 0.02483 | valid_mae: 0.10536 | valid_rmse: 0.1598  | valid_mse: 0.02554 |  0:03:45s\n",
      "epoch 53 | loss: 0.01582 | train_mae: 0.10018 | train_rmse: 0.15246 | train_mse: 0.02324 | valid_mae: 0.10056 | valid_rmse: 0.15297 | valid_mse: 0.0234  |  0:03:49s\n",
      "epoch 54 | loss: 0.016   | train_mae: 0.09685 | train_rmse: 0.15003 | train_mse: 0.02251 | valid_mae: 0.09746 | valid_rmse: 0.15152 | valid_mse: 0.02296 |  0:03:53s\n",
      "epoch 55 | loss: 0.01567 | train_mae: 0.12    | train_rmse: 0.17421 | train_mse: 0.03035 | valid_mae: 0.12085 | valid_rmse: 0.17538 | valid_mse: 0.03076 |  0:03:57s\n",
      "epoch 56 | loss: 0.01509 | train_mae: 0.11172 | train_rmse: 0.16405 | train_mse: 0.02691 | valid_mae: 0.11204 | valid_rmse: 0.16476 | valid_mse: 0.02715 |  0:04:01s\n",
      "epoch 57 | loss: 0.01475 | train_mae: 0.10568 | train_rmse: 0.15298 | train_mse: 0.0234  | valid_mae: 0.10674 | valid_rmse: 0.15467 | valid_mse: 0.02392 |  0:04:05s\n",
      "epoch 58 | loss: 0.01539 | train_mae: 0.09328 | train_rmse: 0.14332 | train_mse: 0.02054 | valid_mae: 0.09372 | valid_rmse: 0.14393 | valid_mse: 0.02072 |  0:04:09s\n",
      "epoch 59 | loss: 0.01527 | train_mae: 0.1134  | train_rmse: 0.16526 | train_mse: 0.02731 | valid_mae: 0.11347 | valid_rmse: 0.16512 | valid_mse: 0.02726 |  0:04:13s\n",
      "epoch 60 | loss: 0.01544 | train_mae: 0.11575 | train_rmse: 0.1676  | train_mse: 0.02809 | valid_mae: 0.11569 | valid_rmse: 0.16718 | valid_mse: 0.02795 |  0:04:17s\n",
      "epoch 61 | loss: 0.01543 | train_mae: 0.10295 | train_rmse: 0.15287 | train_mse: 0.02337 | valid_mae: 0.10324 | valid_rmse: 0.15299 | valid_mse: 0.02341 |  0:04:21s\n",
      "epoch 62 | loss: 0.01497 | train_mae: 0.11686 | train_rmse: 0.1648  | train_mse: 0.02716 | valid_mae: 0.11689 | valid_rmse: 0.16615 | valid_mse: 0.02761 |  0:04:25s\n",
      "epoch 63 | loss: 0.01554 | train_mae: 0.12385 | train_rmse: 0.17691 | train_mse: 0.0313  | valid_mae: 0.12326 | valid_rmse: 0.1765  | valid_mse: 0.03115 |  0:04:29s\n",
      "epoch 64 | loss: 0.01578 | train_mae: 0.10019 | train_rmse: 0.15022 | train_mse: 0.02257 | valid_mae: 0.10065 | valid_rmse: 0.15119 | valid_mse: 0.02286 |  0:04:33s\n",
      "epoch 65 | loss: 0.01475 | train_mae: 0.1244  | train_rmse: 0.17479 | train_mse: 0.03055 | valid_mae: 0.12586 | valid_rmse: 0.17756 | valid_mse: 0.03153 |  0:04:37s\n",
      "epoch 66 | loss: 0.01417 | train_mae: 0.09418 | train_rmse: 0.13947 | train_mse: 0.01945 | valid_mae: 0.09501 | valid_rmse: 0.1411  | valid_mse: 0.01991 |  0:04:41s\n",
      "epoch 67 | loss: 0.01412 | train_mae: 0.09064 | train_rmse: 0.13852 | train_mse: 0.01919 | valid_mae: 0.0918  | valid_rmse: 0.1401  | valid_mse: 0.01963 |  0:04:45s\n",
      "epoch 68 | loss: 0.01516 | train_mae: 0.09967 | train_rmse: 0.14287 | train_mse: 0.02041 | valid_mae: 0.10047 | valid_rmse: 0.14464 | valid_mse: 0.02092 |  0:04:49s\n",
      "epoch 69 | loss: 0.01426 | train_mae: 0.09812 | train_rmse: 0.1475  | train_mse: 0.02175 | valid_mae: 0.09889 | valid_rmse: 0.14917 | valid_mse: 0.02225 |  0:04:53s\n",
      "epoch 70 | loss: 0.0143  | train_mae: 0.11622 | train_rmse: 0.16738 | train_mse: 0.02802 | valid_mae: 0.11741 | valid_rmse: 0.17057 | valid_mse: 0.0291  |  0:04:57s\n",
      "epoch 71 | loss: 0.01393 | train_mae: 0.10717 | train_rmse: 0.15492 | train_mse: 0.024   | valid_mae: 0.1068  | valid_rmse: 0.15489 | valid_mse: 0.02399 |  0:05:01s\n",
      "epoch 72 | loss: 0.01449 | train_mae: 0.0952  | train_rmse: 0.14466 | train_mse: 0.02093 | valid_mae: 0.09545 | valid_rmse: 0.14547 | valid_mse: 0.02116 |  0:05:05s\n",
      "epoch 73 | loss: 0.01338 | train_mae: 0.10033 | train_rmse: 0.14821 | train_mse: 0.02197 | valid_mae: 0.1014  | valid_rmse: 0.15046 | valid_mse: 0.02264 |  0:05:09s\n",
      "epoch 74 | loss: 0.01321 | train_mae: 0.12237 | train_rmse: 0.16278 | train_mse: 0.0265  | valid_mae: 0.12199 | valid_rmse: 0.16204 | valid_mse: 0.02626 |  0:05:13s\n",
      "epoch 75 | loss: 0.01372 | train_mae: 0.10611 | train_rmse: 0.15307 | train_mse: 0.02343 | valid_mae: 0.10737 | valid_rmse: 0.15471 | valid_mse: 0.02393 |  0:05:17s\n",
      "epoch 76 | loss: 0.01408 | train_mae: 0.09749 | train_rmse: 0.14434 | train_mse: 0.02083 | valid_mae: 0.09834 | valid_rmse: 0.14635 | valid_mse: 0.02142 |  0:05:21s\n",
      "epoch 77 | loss: 0.01395 | train_mae: 0.12565 | train_rmse: 0.17913 | train_mse: 0.03209 | valid_mae: 0.12501 | valid_rmse: 0.1794  | valid_mse: 0.03219 |  0:05:25s\n",
      "epoch 78 | loss: 0.01352 | train_mae: 0.09264 | train_rmse: 0.1377  | train_mse: 0.01896 | valid_mae: 0.0934  | valid_rmse: 0.13961 | valid_mse: 0.01949 |  0:05:28s\n",
      "epoch 79 | loss: 0.01274 | train_mae: 0.09319 | train_rmse: 0.14125 | train_mse: 0.01995 | valid_mae: 0.09427 | valid_rmse: 0.14322 | valid_mse: 0.02051 |  0:05:33s\n",
      "epoch 80 | loss: 0.01413 | train_mae: 0.10288 | train_rmse: 0.14733 | train_mse: 0.02171 | valid_mae: 0.10443 | valid_rmse: 0.14916 | valid_mse: 0.02225 |  0:05:36s\n",
      "epoch 81 | loss: 0.01304 | train_mae: 0.09514 | train_rmse: 0.1429  | train_mse: 0.02042 | valid_mae: 0.09614 | valid_rmse: 0.14447 | valid_mse: 0.02087 |  0:05:41s\n",
      "epoch 82 | loss: 0.01363 | train_mae: 0.11146 | train_rmse: 0.1653  | train_mse: 0.02732 | valid_mae: 0.11322 | valid_rmse: 0.16756 | valid_mse: 0.02808 |  0:05:45s\n",
      "epoch 83 | loss: 0.01344 | train_mae: 0.09788 | train_rmse: 0.14349 | train_mse: 0.02059 | valid_mae: 0.09883 | valid_rmse: 0.14538 | valid_mse: 0.02114 |  0:05:49s\n",
      "epoch 84 | loss: 0.01301 | train_mae: 0.11078 | train_rmse: 0.16216 | train_mse: 0.0263  | valid_mae: 0.11205 | valid_rmse: 0.16431 | valid_mse: 0.027   |  0:05:53s\n",
      "epoch 85 | loss: 0.01329 | train_mae: 0.09936 | train_rmse: 0.14733 | train_mse: 0.02171 | valid_mae: 0.10091 | valid_rmse: 0.15008 | valid_mse: 0.02252 |  0:05:57s\n",
      "epoch 86 | loss: 0.0134  | train_mae: 0.09954 | train_rmse: 0.15014 | train_mse: 0.02254 | valid_mae: 0.10046 | valid_rmse: 0.15115 | valid_mse: 0.02285 |  0:06:01s\n",
      "epoch 87 | loss: 0.01273 | train_mae: 0.09021 | train_rmse: 0.13339 | train_mse: 0.01779 | valid_mae: 0.09094 | valid_rmse: 0.13544 | valid_mse: 0.01835 |  0:06:05s\n",
      "epoch 88 | loss: 0.01219 | train_mae: 0.09594 | train_rmse: 0.14012 | train_mse: 0.01963 | valid_mae: 0.09662 | valid_rmse: 0.142   | valid_mse: 0.02016 |  0:06:09s\n",
      "epoch 89 | loss: 0.01279 | train_mae: 0.11015 | train_rmse: 0.15625 | train_mse: 0.02441 | valid_mae: 0.11046 | valid_rmse: 0.15666 | valid_mse: 0.02454 |  0:06:13s\n",
      "epoch 90 | loss: 0.01285 | train_mae: 0.11243 | train_rmse: 0.15453 | train_mse: 0.02388 | valid_mae: 0.11213 | valid_rmse: 0.15438 | valid_mse: 0.02383 |  0:06:17s\n",
      "epoch 91 | loss: 0.01311 | train_mae: 0.09882 | train_rmse: 0.14184 | train_mse: 0.02012 | valid_mae: 0.10003 | valid_rmse: 0.14423 | valid_mse: 0.0208  |  0:06:21s\n",
      "epoch 92 | loss: 0.01252 | train_mae: 0.0969  | train_rmse: 0.14401 | train_mse: 0.02074 | valid_mae: 0.09818 | valid_rmse: 0.14612 | valid_mse: 0.02135 |  0:06:25s\n",
      "epoch 93 | loss: 0.01216 | train_mae: 0.09519 | train_rmse: 0.13912 | train_mse: 0.01936 | valid_mae: 0.09634 | valid_rmse: 0.14175 | valid_mse: 0.02009 |  0:06:28s\n",
      "epoch 94 | loss: 0.01201 | train_mae: 0.10504 | train_rmse: 0.15094 | train_mse: 0.02278 | valid_mae: 0.10496 | valid_rmse: 0.15133 | valid_mse: 0.0229  |  0:06:32s\n",
      "epoch 95 | loss: 0.01251 | train_mae: 0.10399 | train_rmse: 0.14807 | train_mse: 0.02192 | valid_mae: 0.10537 | valid_rmse: 0.1503  | valid_mse: 0.02259 |  0:06:36s\n",
      "epoch 96 | loss: 0.0129  | train_mae: 0.09199 | train_rmse: 0.14172 | train_mse: 0.02008 | valid_mae: 0.09206 | valid_rmse: 0.14163 | valid_mse: 0.02006 |  0:06:40s\n",
      "epoch 97 | loss: 0.01233 | train_mae: 0.10019 | train_rmse: 0.13939 | train_mse: 0.01943 | valid_mae: 0.10095 | valid_rmse: 0.14101 | valid_mse: 0.01988 |  0:06:44s\n",
      "epoch 98 | loss: 0.0134  | train_mae: 0.09349 | train_rmse: 0.13904 | train_mse: 0.01933 | valid_mae: 0.094   | valid_rmse: 0.14156 | valid_mse: 0.02004 |  0:06:48s\n",
      "epoch 99 | loss: 0.01246 | train_mae: 0.09877 | train_rmse: 0.14377 | train_mse: 0.02067 | valid_mae: 0.09932 | valid_rmse: 0.14468 | valid_mse: 0.02093 |  0:06:53s\n",
      "epoch 100| loss: 0.01202 | train_mae: 0.12358 | train_rmse: 0.178   | train_mse: 0.03168 | valid_mae: 0.12458 | valid_rmse: 0.1789  | valid_mse: 0.03201 |  0:06:57s\n",
      "epoch 101| loss: 0.01255 | train_mae: 0.0932  | train_rmse: 0.13505 | train_mse: 0.01824 | valid_mae: 0.09416 | valid_rmse: 0.13763 | valid_mse: 0.01894 |  0:07:01s\n",
      "epoch 102| loss: 0.012   | train_mae: 0.11224 | train_rmse: 0.15985 | train_mse: 0.02555 | valid_mae: 0.11225 | valid_rmse: 0.15955 | valid_mse: 0.02546 |  0:07:05s\n",
      "epoch 103| loss: 0.01243 | train_mae: 0.12823 | train_rmse: 0.19647 | train_mse: 0.0386  | valid_mae: 0.12782 | valid_rmse: 0.19542 | valid_mse: 0.03819 |  0:07:09s\n",
      "epoch 104| loss: 0.01169 | train_mae: 0.0884  | train_rmse: 0.13252 | train_mse: 0.01756 | valid_mae: 0.08919 | valid_rmse: 0.1341  | valid_mse: 0.01798 |  0:07:13s\n",
      "epoch 105| loss: 0.01204 | train_mae: 0.08967 | train_rmse: 0.13307 | train_mse: 0.01771 | valid_mae: 0.09011 | valid_rmse: 0.135   | valid_mse: 0.01823 |  0:07:17s\n",
      "epoch 106| loss: 0.01203 | train_mae: 0.08938 | train_rmse: 0.13053 | train_mse: 0.01704 | valid_mae: 0.08961 | valid_rmse: 0.13194 | valid_mse: 0.01741 |  0:07:21s\n",
      "epoch 107| loss: 0.0115  | train_mae: 0.10775 | train_rmse: 0.1563  | train_mse: 0.02443 | valid_mae: 0.10822 | valid_rmse: 0.15852 | valid_mse: 0.02513 |  0:07:25s\n",
      "epoch 108| loss: 0.01119 | train_mae: 0.09205 | train_rmse: 0.13355 | train_mse: 0.01783 | valid_mae: 0.0931  | valid_rmse: 0.13557 | valid_mse: 0.01838 |  0:07:29s\n",
      "epoch 109| loss: 0.01167 | train_mae: 0.0872  | train_rmse: 0.12856 | train_mse: 0.01653 | valid_mae: 0.08851 | valid_rmse: 0.13197 | valid_mse: 0.01742 |  0:07:33s\n",
      "epoch 110| loss: 0.01129 | train_mae: 0.08844 | train_rmse: 0.13338 | train_mse: 0.01779 | valid_mae: 0.08939 | valid_rmse: 0.13617 | valid_mse: 0.01854 |  0:07:37s\n",
      "epoch 111| loss: 0.01111 | train_mae: 0.09813 | train_rmse: 0.14137 | train_mse: 0.01999 | valid_mae: 0.09929 | valid_rmse: 0.14348 | valid_mse: 0.02059 |  0:07:41s\n",
      "epoch 112| loss: 0.01182 | train_mae: 0.09286 | train_rmse: 0.14392 | train_mse: 0.02071 | valid_mae: 0.09365 | valid_rmse: 0.14503 | valid_mse: 0.02103 |  0:07:45s\n",
      "epoch 113| loss: 0.01155 | train_mae: 0.10931 | train_rmse: 0.15076 | train_mse: 0.02273 | valid_mae: 0.10986 | valid_rmse: 0.15182 | valid_mse: 0.02305 |  0:07:49s\n",
      "epoch 114| loss: 0.0119  | train_mae: 0.11635 | train_rmse: 0.15903 | train_mse: 0.02529 | valid_mae: 0.11658 | valid_rmse: 0.15989 | valid_mse: 0.02556 |  0:07:53s\n",
      "epoch 115| loss: 0.01032 | train_mae: 0.11072 | train_rmse: 0.15642 | train_mse: 0.02447 | valid_mae: 0.1116  | valid_rmse: 0.15876 | valid_mse: 0.02521 |  0:07:57s\n",
      "epoch 116| loss: 0.01102 | train_mae: 0.08909 | train_rmse: 0.13482 | train_mse: 0.01818 | valid_mae: 0.09033 | valid_rmse: 0.13693 | valid_mse: 0.01875 |  0:08:01s\n",
      "epoch 117| loss: 0.01156 | train_mae: 0.11423 | train_rmse: 0.16582 | train_mse: 0.0275  | valid_mae: 0.11459 | valid_rmse: 0.1662  | valid_mse: 0.02762 |  0:08:05s\n",
      "epoch 118| loss: 0.01117 | train_mae: 0.08059 | train_rmse: 0.12449 | train_mse: 0.0155  | valid_mae: 0.08154 | valid_rmse: 0.12733 | valid_mse: 0.01621 |  0:08:09s\n",
      "epoch 119| loss: 0.01097 | train_mae: 0.08643 | train_rmse: 0.12782 | train_mse: 0.01634 | valid_mae: 0.08723 | valid_rmse: 0.13041 | valid_mse: 0.01701 |  0:08:13s\n",
      "epoch 120| loss: 0.01144 | train_mae: 0.08297 | train_rmse: 0.12629 | train_mse: 0.01595 | valid_mae: 0.08444 | valid_rmse: 0.12938 | valid_mse: 0.01674 |  0:08:17s\n",
      "epoch 121| loss: 0.01138 | train_mae: 0.10003 | train_rmse: 0.14311 | train_mse: 0.02048 | valid_mae: 0.10182 | valid_rmse: 0.14652 | valid_mse: 0.02147 |  0:08:21s\n",
      "epoch 122| loss: 0.01178 | train_mae: 0.09062 | train_rmse: 0.13841 | train_mse: 0.01916 | valid_mae: 0.09196 | valid_rmse: 0.14079 | valid_mse: 0.01982 |  0:08:25s\n",
      "epoch 123| loss: 0.01162 | train_mae: 0.0995  | train_rmse: 0.14746 | train_mse: 0.02174 | valid_mae: 0.09999 | valid_rmse: 0.14834 | valid_mse: 0.022   |  0:08:29s\n",
      "epoch 124| loss: 0.01137 | train_mae: 0.0864  | train_rmse: 0.12949 | train_mse: 0.01677 | valid_mae: 0.08763 | valid_rmse: 0.13238 | valid_mse: 0.01752 |  0:08:33s\n",
      "epoch 125| loss: 0.01101 | train_mae: 0.12563 | train_rmse: 0.17698 | train_mse: 0.03132 | valid_mae: 0.12635 | valid_rmse: 0.1773  | valid_mse: 0.03143 |  0:08:37s\n",
      "epoch 126| loss: 0.01115 | train_mae: 0.09801 | train_rmse: 0.13951 | train_mse: 0.01946 | valid_mae: 0.0992  | valid_rmse: 0.14237 | valid_mse: 0.02027 |  0:08:41s\n",
      "epoch 127| loss: 0.01135 | train_mae: 0.10938 | train_rmse: 0.15788 | train_mse: 0.02493 | valid_mae: 0.10955 | valid_rmse: 0.15782 | valid_mse: 0.02491 |  0:08:45s\n",
      "epoch 128| loss: 0.0106  | train_mae: 0.09392 | train_rmse: 0.13243 | train_mse: 0.01754 | valid_mae: 0.09472 | valid_rmse: 0.13453 | valid_mse: 0.0181  |  0:08:49s\n",
      "epoch 129| loss: 0.01043 | train_mae: 0.07899 | train_rmse: 0.11823 | train_mse: 0.01398 | valid_mae: 0.08022 | valid_rmse: 0.12093 | valid_mse: 0.01463 |  0:08:53s\n",
      "epoch 130| loss: 0.01065 | train_mae: 0.10022 | train_rmse: 0.14451 | train_mse: 0.02088 | valid_mae: 0.10196 | valid_rmse: 0.14841 | valid_mse: 0.02203 |  0:08:57s\n",
      "epoch 131| loss: 0.0103  | train_mae: 0.09693 | train_rmse: 0.15137 | train_mse: 0.02291 | valid_mae: 0.09823 | valid_rmse: 0.15291 | valid_mse: 0.02338 |  0:09:01s\n",
      "epoch 132| loss: 0.01087 | train_mae: 0.08906 | train_rmse: 0.13409 | train_mse: 0.01798 | valid_mae: 0.09006 | valid_rmse: 0.13631 | valid_mse: 0.01858 |  0:09:05s\n",
      "epoch 133| loss: 0.01354 | train_mae: 0.21093 | train_rmse: 0.60245 | train_mse: 0.36294 | valid_mae: 0.20444 | valid_rmse: 0.54198 | valid_mse: 0.29374 |  0:09:09s\n",
      "epoch 134| loss: 0.0115  | train_mae: 0.09845 | train_rmse: 0.14679 | train_mse: 0.02155 | valid_mae: 0.09886 | valid_rmse: 0.14728 | valid_mse: 0.02169 |  0:09:13s\n",
      "epoch 135| loss: 0.01068 | train_mae: 0.08007 | train_rmse: 0.11748 | train_mse: 0.0138  | valid_mae: 0.08137 | valid_rmse: 0.12033 | valid_mse: 0.01448 |  0:09:17s\n",
      "epoch 136| loss: 0.01067 | train_mae: 0.09224 | train_rmse: 0.13019 | train_mse: 0.01695 | valid_mae: 0.09279 | valid_rmse: 0.13119 | valid_mse: 0.01721 |  0:09:21s\n",
      "epoch 137| loss: 0.01207 | train_mae: 0.10901 | train_rmse: 0.45346 | train_mse: 0.20562 | valid_mae: 0.10035 | valid_rmse: 0.2867  | valid_mse: 0.0822  |  0:09:25s\n",
      "epoch 138| loss: 0.01137 | train_mae: 0.11813 | train_rmse: 0.16954 | train_mse: 0.02874 | valid_mae: 0.11818 | valid_rmse: 0.17003 | valid_mse: 0.02891 |  0:09:29s\n",
      "epoch 139| loss: 0.01052 | train_mae: 0.11984 | train_rmse: 0.16932 | train_mse: 0.02867 | valid_mae: 0.11985 | valid_rmse: 0.1691  | valid_mse: 0.02859 |  0:09:33s\n",
      "epoch 140| loss: 0.01105 | train_mae: 0.10107 | train_rmse: 0.1451  | train_mse: 0.02106 | valid_mae: 0.1009  | valid_rmse: 0.14539 | valid_mse: 0.02114 |  0:09:37s\n",
      "epoch 141| loss: 0.01074 | train_mae: 0.10481 | train_rmse: 0.14734 | train_mse: 0.02171 | valid_mae: 0.1067  | valid_rmse: 0.15103 | valid_mse: 0.02281 |  0:09:41s\n",
      "epoch 142| loss: 0.01042 | train_mae: 0.08724 | train_rmse: 0.1304  | train_mse: 0.017   | valid_mae: 0.08861 | valid_rmse: 0.13315 | valid_mse: 0.01773 |  0:09:45s\n",
      "epoch 143| loss: 0.01    | train_mae: 0.08442 | train_rmse: 0.12678 | train_mse: 0.01607 | valid_mae: 0.08528 | valid_rmse: 0.12872 | valid_mse: 0.01657 |  0:09:49s\n",
      "epoch 144| loss: 0.01036 | train_mae: 0.08672 | train_rmse: 0.12862 | train_mse: 0.01654 | valid_mae: 0.08792 | valid_rmse: 0.13111 | valid_mse: 0.01719 |  0:09:53s\n",
      "epoch 145| loss: 0.01029 | train_mae: 0.09985 | train_rmse: 0.14636 | train_mse: 0.02142 | valid_mae: 0.10109 | valid_rmse: 0.14832 | valid_mse: 0.022   |  0:09:57s\n",
      "epoch 146| loss: 0.01028 | train_mae: 0.08252 | train_rmse: 0.1234  | train_mse: 0.01523 | valid_mae: 0.08289 | valid_rmse: 0.12467 | valid_mse: 0.01554 |  0:10:01s\n",
      "epoch 147| loss: 0.01059 | train_mae: 0.09779 | train_rmse: 0.15443 | train_mse: 0.02385 | valid_mae: 0.0989  | valid_rmse: 0.15583 | valid_mse: 0.02428 |  0:10:05s\n",
      "epoch 148| loss: 0.00956 | train_mae: 0.08826 | train_rmse: 0.13465 | train_mse: 0.01813 | valid_mae: 0.0884  | valid_rmse: 0.13396 | valid_mse: 0.01794 |  0:10:09s\n",
      "epoch 149| loss: 0.00978 | train_mae: 0.08243 | train_rmse: 0.1248  | train_mse: 0.01558 | valid_mae: 0.08327 | valid_rmse: 0.12669 | valid_mse: 0.01605 |  0:10:13s\n",
      "epoch 150| loss: 0.00969 | train_mae: 0.09339 | train_rmse: 0.13821 | train_mse: 0.0191  | valid_mae: 0.094   | valid_rmse: 0.13938 | valid_mse: 0.01943 |  0:10:17s\n",
      "epoch 151| loss: 0.01007 | train_mae: 0.0959  | train_rmse: 0.14067 | train_mse: 0.01979 | valid_mae: 0.0967  | valid_rmse: 0.14147 | valid_mse: 0.02001 |  0:10:21s\n",
      "epoch 152| loss: 0.00967 | train_mae: 0.08493 | train_rmse: 0.12595 | train_mse: 0.01586 | valid_mae: 0.08533 | valid_rmse: 0.12788 | valid_mse: 0.01635 |  0:10:25s\n",
      "epoch 153| loss: 0.01007 | train_mae: 0.08895 | train_rmse: 0.13845 | train_mse: 0.01917 | valid_mae: 0.08973 | valid_rmse: 0.13934 | valid_mse: 0.01941 |  0:10:29s\n",
      "epoch 154| loss: 0.0106  | train_mae: 0.1232  | train_rmse: 0.17142 | train_mse: 0.02938 | valid_mae: 0.12261 | valid_rmse: 0.17053 | valid_mse: 0.02908 |  0:10:33s\n",
      "epoch 155| loss: 0.01068 | train_mae: 0.09968 | train_rmse: 0.15085 | train_mse: 0.02276 | valid_mae: 0.10042 | valid_rmse: 0.15133 | valid_mse: 0.0229  |  0:10:37s\n",
      "epoch 156| loss: 0.00985 | train_mae: 0.08645 | train_rmse: 0.13574 | train_mse: 0.01843 | valid_mae: 0.0871  | valid_rmse: 0.13816 | valid_mse: 0.01909 |  0:10:41s\n",
      "epoch 157| loss: 0.00997 | train_mae: 0.10324 | train_rmse: 0.15557 | train_mse: 0.0242  | valid_mae: 0.10348 | valid_rmse: 0.1568  | valid_mse: 0.02459 |  0:10:45s\n",
      "epoch 158| loss: 0.01074 | train_mae: 0.0804  | train_rmse: 0.12181 | train_mse: 0.01484 | valid_mae: 0.08192 | valid_rmse: 0.12454 | valid_mse: 0.01551 |  0:10:49s\n",
      "epoch 159| loss: 0.00982 | train_mae: 0.08244 | train_rmse: 0.12369 | train_mse: 0.0153  | valid_mae: 0.08326 | valid_rmse: 0.12584 | valid_mse: 0.01584 |  0:10:53s\n",
      "epoch 160| loss: 0.0094  | train_mae: 0.08359 | train_rmse: 0.12291 | train_mse: 0.01511 | valid_mae: 0.08457 | valid_rmse: 0.12543 | valid_mse: 0.01573 |  0:10:57s\n",
      "epoch 161| loss: 0.00994 | train_mae: 0.09676 | train_rmse: 0.1376  | train_mse: 0.01893 | valid_mae: 0.09676 | valid_rmse: 0.13822 | valid_mse: 0.0191  |  0:11:01s\n",
      "epoch 162| loss: 0.00941 | train_mae: 0.08763 | train_rmse: 0.132   | train_mse: 0.01743 | valid_mae: 0.08841 | valid_rmse: 0.1332  | valid_mse: 0.01774 |  0:11:05s\n",
      "epoch 163| loss: 0.00976 | train_mae: 0.09918 | train_rmse: 0.1516  | train_mse: 0.02298 | valid_mae: 0.09983 | valid_rmse: 0.15267 | valid_mse: 0.02331 |  0:11:09s\n",
      "epoch 164| loss: 0.00994 | train_mae: 0.07774 | train_rmse: 0.11336 | train_mse: 0.01285 | valid_mae: 0.07883 | valid_rmse: 0.11583 | valid_mse: 0.01342 |  0:11:13s\n",
      "epoch 165| loss: 0.00973 | train_mae: 0.0812  | train_rmse: 0.1221  | train_mse: 0.01491 | valid_mae: 0.08235 | valid_rmse: 0.12428 | valid_mse: 0.01545 |  0:11:17s\n",
      "epoch 166| loss: 0.00924 | train_mae: 0.08335 | train_rmse: 0.12397 | train_mse: 0.01537 | valid_mae: 0.08358 | valid_rmse: 0.12488 | valid_mse: 0.0156  |  0:11:21s\n",
      "epoch 167| loss: 0.00895 | train_mae: 0.0861  | train_rmse: 0.12601 | train_mse: 0.01588 | valid_mae: 0.08681 | valid_rmse: 0.12731 | valid_mse: 0.01621 |  0:11:25s\n",
      "epoch 168| loss: 0.0092  | train_mae: 0.10831 | train_rmse: 0.15929 | train_mse: 0.02537 | valid_mae: 0.10825 | valid_rmse: 0.15949 | valid_mse: 0.02544 |  0:11:29s\n",
      "epoch 169| loss: 0.00944 | train_mae: 0.10139 | train_rmse: 0.14333 | train_mse: 0.02054 | valid_mae: 0.10312 | valid_rmse: 0.14619 | valid_mse: 0.02137 |  0:11:33s\n",
      "epoch 170| loss: 0.00905 | train_mae: 0.09214 | train_rmse: 0.13361 | train_mse: 0.01785 | valid_mae: 0.09284 | valid_rmse: 0.13418 | valid_mse: 0.018   |  0:11:37s\n",
      "epoch 171| loss: 0.01009 | train_mae: 0.07358 | train_rmse: 0.11563 | train_mse: 0.01337 | valid_mae: 0.07495 | valid_rmse: 0.11794 | valid_mse: 0.01391 |  0:11:41s\n",
      "epoch 172| loss: 0.00946 | train_mae: 0.08831 | train_rmse: 0.13259 | train_mse: 0.01758 | valid_mae: 0.08891 | valid_rmse: 0.13242 | valid_mse: 0.01753 |  0:11:45s\n",
      "epoch 173| loss: 0.01021 | train_mae: 0.08299 | train_rmse: 0.12957 | train_mse: 0.01679 | valid_mae: 0.08395 | valid_rmse: 0.13154 | valid_mse: 0.0173  |  0:11:49s\n",
      "epoch 174| loss: 0.00909 | train_mae: 0.08317 | train_rmse: 0.12132 | train_mse: 0.01472 | valid_mae: 0.08357 | valid_rmse: 0.12284 | valid_mse: 0.01509 |  0:11:53s\n",
      "epoch 175| loss: 0.0092  | train_mae: 0.08872 | train_rmse: 0.12467 | train_mse: 0.01554 | valid_mae: 0.08903 | valid_rmse: 0.12555 | valid_mse: 0.01576 |  0:11:57s\n",
      "epoch 176| loss: 0.00882 | train_mae: 0.08872 | train_rmse: 0.13274 | train_mse: 0.01762 | valid_mae: 0.08953 | valid_rmse: 0.13439 | valid_mse: 0.01806 |  0:12:01s\n",
      "epoch 177| loss: 0.00921 | train_mae: 0.10483 | train_rmse: 0.14452 | train_mse: 0.02089 | valid_mae: 0.10607 | valid_rmse: 0.1466  | valid_mse: 0.02149 |  0:12:05s\n",
      "epoch 178| loss: 0.01136 | train_mae: 0.12511 | train_rmse: 0.22141 | train_mse: 0.04902 | valid_mae: 0.12486 | valid_rmse: 0.19865 | valid_mse: 0.03946 |  0:12:09s\n",
      "epoch 179| loss: 0.00939 | train_mae: 0.09077 | train_rmse: 0.14083 | train_mse: 0.01983 | valid_mae: 0.09199 | valid_rmse: 0.14007 | valid_mse: 0.01962 |  0:12:13s\n",
      "epoch 180| loss: 0.00922 | train_mae: 0.10667 | train_rmse: 0.15885 | train_mse: 0.02523 | valid_mae: 0.10583 | valid_rmse: 0.15346 | valid_mse: 0.02355 |  0:12:17s\n",
      "epoch 181| loss: 0.00968 | train_mae: 0.09253 | train_rmse: 0.16667 | train_mse: 0.02778 | valid_mae: 0.09267 | valid_rmse: 0.15171 | valid_mse: 0.02301 |  0:12:21s\n",
      "epoch 182| loss: 0.00944 | train_mae: 0.08625 | train_rmse: 0.14487 | train_mse: 0.02099 | valid_mae: 0.08693 | valid_rmse: 0.14152 | valid_mse: 0.02003 |  0:12:25s\n",
      "epoch 183| loss: 0.00904 | train_mae: 0.09015 | train_rmse: 0.1299  | train_mse: 0.01687 | valid_mae: 0.09122 | valid_rmse: 0.13278 | valid_mse: 0.01763 |  0:12:29s\n",
      "epoch 184| loss: 0.01019 | train_mae: 0.10055 | train_rmse: 0.13927 | train_mse: 0.0194  | valid_mae: 0.10096 | valid_rmse: 0.13998 | valid_mse: 0.01959 |  0:12:33s\n",
      "epoch 185| loss: 0.0096  | train_mae: 0.07733 | train_rmse: 0.12448 | train_mse: 0.01549 | valid_mae: 0.07814 | valid_rmse: 0.12615 | valid_mse: 0.01591 |  0:12:37s\n",
      "epoch 186| loss: 0.00896 | train_mae: 0.08512 | train_rmse: 0.16977 | train_mse: 0.02882 | valid_mae: 0.08454 | valid_rmse: 0.14052 | valid_mse: 0.01975 |  0:12:41s\n",
      "epoch 187| loss: 0.00904 | train_mae: 0.09755 | train_rmse: 0.14781 | train_mse: 0.02185 | valid_mae: 0.09913 | valid_rmse: 0.14801 | valid_mse: 0.02191 |  0:12:45s\n",
      "epoch 188| loss: 0.00915 | train_mae: 0.08169 | train_rmse: 0.13717 | train_mse: 0.01882 | valid_mae: 0.08204 | valid_rmse: 0.13148 | valid_mse: 0.01729 |  0:12:49s\n",
      "epoch 189| loss: 0.00897 | train_mae: 0.10628 | train_rmse: 0.17813 | train_mse: 0.03173 | valid_mae: 0.10757 | valid_rmse: 0.18677 | valid_mse: 0.03488 |  0:12:53s\n",
      "epoch 190| loss: 0.01018 | train_mae: 0.07903 | train_rmse: 0.12085 | train_mse: 0.0146  | valid_mae: 0.08045 | valid_rmse: 0.12495 | valid_mse: 0.01561 |  0:12:57s\n",
      "epoch 191| loss: 0.0091  | train_mae: 0.08678 | train_rmse: 0.13559 | train_mse: 0.01838 | valid_mae: 0.08687 | valid_rmse: 0.13262 | valid_mse: 0.01759 |  0:13:01s\n",
      "epoch 192| loss: 0.00916 | train_mae: 0.087   | train_rmse: 0.16464 | train_mse: 0.02711 | valid_mae: 0.08703 | valid_rmse: 0.13916 | valid_mse: 0.01936 |  0:13:05s\n",
      "epoch 193| loss: 0.00949 | train_mae: 0.08502 | train_rmse: 0.13866 | train_mse: 0.01923 | valid_mae: 0.08585 | valid_rmse: 0.13566 | valid_mse: 0.0184  |  0:13:09s\n",
      "epoch 194| loss: 0.00928 | train_mae: 0.09646 | train_rmse: 0.14063 | train_mse: 0.01978 | valid_mae: 0.09682 | valid_rmse: 0.14032 | valid_mse: 0.01969 |  0:13:13s\n",
      "epoch 195| loss: 0.0094  | train_mae: 0.13215 | train_rmse: 0.19057 | train_mse: 0.03632 | valid_mae: 0.13258 | valid_rmse: 0.18962 | valid_mse: 0.03596 |  0:13:17s\n",
      "epoch 196| loss: 0.00924 | train_mae: 0.11361 | train_rmse: 0.15937 | train_mse: 0.0254  | valid_mae: 0.11405 | valid_rmse: 0.16026 | valid_mse: 0.02568 |  0:13:21s\n",
      "epoch 197| loss: 0.00878 | train_mae: 0.10108 | train_rmse: 0.15256 | train_mse: 0.02327 | valid_mae: 0.10137 | valid_rmse: 0.14931 | valid_mse: 0.02229 |  0:13:25s\n",
      "epoch 198| loss: 0.00915 | train_mae: 0.10025 | train_rmse: 0.145   | train_mse: 0.02103 | valid_mae: 0.10008 | valid_rmse: 0.1472  | valid_mse: 0.02167 |  0:13:29s\n",
      "epoch 199| loss: 0.00917 | train_mae: 0.09663 | train_rmse: 0.13732 | train_mse: 0.01886 | valid_mae: 0.09717 | valid_rmse: 0.13986 | valid_mse: 0.01956 |  0:13:33s\n",
      "epoch 200| loss: 0.00973 | train_mae: 0.36025 | train_rmse: 0.7623  | train_mse: 0.58111 | valid_mae: 0.36295 | valid_rmse: 0.76004 | valid_mse: 0.57766 |  0:13:37s\n",
      "epoch 201| loss: 0.02479 | train_mae: 0.15399 | train_rmse: 0.22313 | train_mse: 0.04979 | valid_mae: 0.1546  | valid_rmse: 0.22298 | valid_mse: 0.04972 |  0:13:42s\n",
      "epoch 202| loss: 0.015   | train_mae: 0.11399 | train_rmse: 0.16352 | train_mse: 0.02674 | valid_mae: 0.11261 | valid_rmse: 0.16238 | valid_mse: 0.02637 |  0:13:46s\n",
      "epoch 203| loss: 0.01247 | train_mae: 0.10726 | train_rmse: 0.15194 | train_mse: 0.02308 | valid_mae: 0.10671 | valid_rmse: 0.15205 | valid_mse: 0.02312 |  0:13:50s\n",
      "epoch 204| loss: 0.01164 | train_mae: 0.09911 | train_rmse: 0.14702 | train_mse: 0.02162 | valid_mae: 0.10032 | valid_rmse: 0.14942 | valid_mse: 0.02233 |  0:13:54s\n",
      "epoch 205| loss: 0.01071 | train_mae: 0.08816 | train_rmse: 0.13458 | train_mse: 0.01811 | valid_mae: 0.08908 | valid_rmse: 0.13623 | valid_mse: 0.01856 |  0:13:58s\n",
      "epoch 206| loss: 0.01157 | train_mae: 0.17855 | train_rmse: 0.2473  | train_mse: 0.06116 | valid_mae: 0.17894 | valid_rmse: 0.24729 | valid_mse: 0.06115 |  0:14:02s\n",
      "epoch 207| loss: 0.01109 | train_mae: 0.09655 | train_rmse: 0.14323 | train_mse: 0.02052 | valid_mae: 0.09692 | valid_rmse: 0.14321 | valid_mse: 0.02051 |  0:14:06s\n",
      "epoch 208| loss: 0.01057 | train_mae: 0.09101 | train_rmse: 0.13993 | train_mse: 0.01958 | valid_mae: 0.09199 | valid_rmse: 0.14193 | valid_mse: 0.02014 |  0:14:10s\n",
      "epoch 209| loss: 0.01041 | train_mae: 0.08419 | train_rmse: 0.126   | train_mse: 0.01587 | valid_mae: 0.08464 | valid_rmse: 0.12708 | valid_mse: 0.01615 |  0:14:14s\n",
      "epoch 210| loss: 0.01064 | train_mae: 0.16862 | train_rmse: 0.24339 | train_mse: 0.05924 | valid_mae: 0.16914 | valid_rmse: 0.24395 | valid_mse: 0.05951 |  0:14:18s\n",
      "epoch 211| loss: 0.01133 | train_mae: 0.09645 | train_rmse: 0.14592 | train_mse: 0.02129 | valid_mae: 0.09727 | valid_rmse: 0.14745 | valid_mse: 0.02174 |  0:14:22s\n",
      "epoch 212| loss: 0.0101  | train_mae: 0.09402 | train_rmse: 0.1353  | train_mse: 0.01831 | valid_mae: 0.09517 | valid_rmse: 0.13756 | valid_mse: 0.01892 |  0:14:26s\n",
      "epoch 213| loss: 0.01019 | train_mae: 0.08433 | train_rmse: 0.12548 | train_mse: 0.01575 | valid_mae: 0.08553 | valid_rmse: 0.12969 | valid_mse: 0.01682 |  0:14:30s\n",
      "epoch 214| loss: 0.01056 | train_mae: 0.08767 | train_rmse: 0.1296  | train_mse: 0.0168  | valid_mae: 0.08792 | valid_rmse: 0.13002 | valid_mse: 0.01691 |  0:14:34s\n",
      "epoch 215| loss: 0.00988 | train_mae: 0.08696 | train_rmse: 0.13074 | train_mse: 0.01709 | valid_mae: 0.08778 | valid_rmse: 0.13172 | valid_mse: 0.01735 |  0:14:38s\n",
      "epoch 216| loss: 0.0095  | train_mae: 0.08525 | train_rmse: 0.12824 | train_mse: 0.01645 | valid_mae: 0.08652 | valid_rmse: 0.13145 | valid_mse: 0.01728 |  0:14:42s\n",
      "epoch 217| loss: 0.00951 | train_mae: 0.10475 | train_rmse: 0.15503 | train_mse: 0.02403 | valid_mae: 0.10625 | valid_rmse: 0.15796 | valid_mse: 0.02495 |  0:14:46s\n",
      "epoch 218| loss: 0.00964 | train_mae: 0.12709 | train_rmse: 0.1796  | train_mse: 0.03226 | valid_mae: 0.1274  | valid_rmse: 0.17963 | valid_mse: 0.03227 |  0:14:50s\n",
      "epoch 219| loss: 0.0101  | train_mae: 0.10603 | train_rmse: 0.15114 | train_mse: 0.02284 | valid_mae: 0.10591 | valid_rmse: 0.15116 | valid_mse: 0.02285 |  0:14:54s\n",
      "epoch 220| loss: 0.00971 | train_mae: 0.09365 | train_rmse: 0.13545 | train_mse: 0.01835 | valid_mae: 0.09417 | valid_rmse: 0.13665 | valid_mse: 0.01867 |  0:14:58s\n",
      "epoch 221| loss: 0.0103  | train_mae: 0.10773 | train_rmse: 0.16206 | train_mse: 0.02626 | valid_mae: 0.10922 | valid_rmse: 0.16526 | valid_mse: 0.02731 |  0:15:02s\n",
      "epoch 222| loss: 0.00999 | train_mae: 0.11095 | train_rmse: 0.15874 | train_mse: 0.0252  | valid_mae: 0.11234 | valid_rmse: 0.16098 | valid_mse: 0.02591 |  0:15:06s\n",
      "epoch 223| loss: 0.00959 | train_mae: 0.08493 | train_rmse: 0.12839 | train_mse: 0.01648 | valid_mae: 0.08656 | valid_rmse: 0.13155 | valid_mse: 0.01731 |  0:15:10s\n",
      "epoch 224| loss: 0.01022 | train_mae: 0.07739 | train_rmse: 0.11512 | train_mse: 0.01325 | valid_mae: 0.07878 | valid_rmse: 0.11818 | valid_mse: 0.01397 |  0:15:14s\n",
      "epoch 225| loss: 0.00957 | train_mae: 0.08888 | train_rmse: 0.12989 | train_mse: 0.01687 | valid_mae: 0.08975 | valid_rmse: 0.13148 | valid_mse: 0.01729 |  0:15:18s\n",
      "epoch 226| loss: 0.00875 | train_mae: 0.09752 | train_rmse: 0.13345 | train_mse: 0.01781 | valid_mae: 0.09743 | valid_rmse: 0.13493 | valid_mse: 0.01821 |  0:15:22s\n",
      "epoch 227| loss: 0.00907 | train_mae: 0.0893  | train_rmse: 0.13286 | train_mse: 0.01765 | valid_mae: 0.08928 | valid_rmse: 0.13304 | valid_mse: 0.0177  |  0:15:26s\n",
      "epoch 228| loss: 0.00917 | train_mae: 0.07708 | train_rmse: 0.11302 | train_mse: 0.01277 | valid_mae: 0.07822 | valid_rmse: 0.11571 | valid_mse: 0.01339 |  0:15:30s\n",
      "epoch 229| loss: 0.00957 | train_mae: 0.07318 | train_rmse: 0.11024 | train_mse: 0.01215 | valid_mae: 0.07448 | valid_rmse: 0.11305 | valid_mse: 0.01278 |  0:15:34s\n",
      "epoch 230| loss: 0.00951 | train_mae: 0.10213 | train_rmse: 0.15432 | train_mse: 0.02381 | valid_mae: 0.10358 | valid_rmse: 0.15771 | valid_mse: 0.02487 |  0:15:38s\n",
      "epoch 231| loss: 0.00868 | train_mae: 0.0796  | train_rmse: 0.11831 | train_mse: 0.014   | valid_mae: 0.08097 | valid_rmse: 0.12054 | valid_mse: 0.01453 |  0:15:42s\n",
      "epoch 232| loss: 0.00943 | train_mae: 0.11946 | train_rmse: 0.21682 | train_mse: 0.04701 | valid_mae: 0.12131 | valid_rmse: 0.22348 | valid_mse: 0.04994 |  0:15:46s\n",
      "epoch 233| loss: 0.009   | train_mae: 0.09622 | train_rmse: 0.14372 | train_mse: 0.02066 | valid_mae: 0.09741 | valid_rmse: 0.14708 | valid_mse: 0.02163 |  0:15:50s\n",
      "epoch 234| loss: 0.00925 | train_mae: 0.11373 | train_rmse: 0.17573 | train_mse: 0.03088 | valid_mae: 0.11492 | valid_rmse: 0.17868 | valid_mse: 0.03193 |  0:15:54s\n",
      "epoch 235| loss: 0.01141 | train_mae: 0.09824 | train_rmse: 0.1468  | train_mse: 0.02155 | valid_mae: 0.0993  | valid_rmse: 0.14871 | valid_mse: 0.02211 |  0:15:58s\n",
      "epoch 236| loss: 0.01027 | train_mae: 0.095   | train_rmse: 0.14034 | train_mse: 0.0197  | valid_mae: 0.09501 | valid_rmse: 0.14056 | valid_mse: 0.01976 |  0:16:02s\n",
      "epoch 237| loss: 0.0091  | train_mae: 0.10506 | train_rmse: 0.15267 | train_mse: 0.02331 | valid_mae: 0.10555 | valid_rmse: 0.15334 | valid_mse: 0.02351 |  0:16:06s\n",
      "epoch 238| loss: 0.00887 | train_mae: 0.0893  | train_rmse: 0.12843 | train_mse: 0.01649 | valid_mae: 0.09005 | valid_rmse: 0.13053 | valid_mse: 0.01704 |  0:16:10s\n",
      "epoch 239| loss: 0.00933 | train_mae: 0.09114 | train_rmse: 0.13317 | train_mse: 0.01773 | valid_mae: 0.09155 | valid_rmse: 0.13477 | valid_mse: 0.01816 |  0:16:14s\n",
      "epoch 240| loss: 0.00906 | train_mae: 0.08052 | train_rmse: 0.1186  | train_mse: 0.01407 | valid_mae: 0.08175 | valid_rmse: 0.12161 | valid_mse: 0.01479 |  0:16:18s\n",
      "epoch 241| loss: 0.00935 | train_mae: 0.09506 | train_rmse: 0.14071 | train_mse: 0.0198  | valid_mae: 0.09625 | valid_rmse: 0.14287 | valid_mse: 0.02041 |  0:16:22s\n",
      "epoch 242| loss: 0.00939 | train_mae: 0.08865 | train_rmse: 0.13099 | train_mse: 0.01716 | valid_mae: 0.09031 | valid_rmse: 0.13421 | valid_mse: 0.01801 |  0:16:26s\n",
      "epoch 243| loss: 0.00961 | train_mae: 0.12053 | train_rmse: 0.17594 | train_mse: 0.03096 | valid_mae: 0.12162 | valid_rmse: 0.1789  | valid_mse: 0.032   |  0:16:30s\n",
      "epoch 244| loss: 0.00952 | train_mae: 0.10217 | train_rmse: 0.1506  | train_mse: 0.02268 | valid_mae: 0.10267 | valid_rmse: 0.15242 | valid_mse: 0.02323 |  0:16:34s\n",
      "epoch 245| loss: 0.00948 | train_mae: 0.14415 | train_rmse: 0.21025 | train_mse: 0.0442  | valid_mae: 0.14499 | valid_rmse: 0.20965 | valid_mse: 0.04395 |  0:16:38s\n",
      "epoch 246| loss: 0.02106 | train_mae: 0.14725 | train_rmse: 0.21098 | train_mse: 0.04451 | valid_mae: 0.14713 | valid_rmse: 0.21417 | valid_mse: 0.04587 |  0:16:42s\n",
      "epoch 247| loss: 0.01514 | train_mae: 0.11469 | train_rmse: 0.16609 | train_mse: 0.02759 | valid_mae: 0.11404 | valid_rmse: 0.16585 | valid_mse: 0.02751 |  0:16:46s\n",
      "epoch 248| loss: 0.01355 | train_mae: 0.11781 | train_rmse: 0.16803 | train_mse: 0.02823 | valid_mae: 0.1175  | valid_rmse: 0.16784 | valid_mse: 0.02817 |  0:16:50s\n",
      "epoch 249| loss: 0.01186 | train_mae: 0.0935  | train_rmse: 0.13868 | train_mse: 0.01923 | valid_mae: 0.09389 | valid_rmse: 0.13969 | valid_mse: 0.01951 |  0:16:54s\n",
      "epoch 250| loss: 0.01152 | train_mae: 0.10292 | train_rmse: 0.14724 | train_mse: 0.02168 | valid_mae: 0.10335 | valid_rmse: 0.14898 | valid_mse: 0.02219 |  0:16:58s\n",
      "epoch 251| loss: 0.01146 | train_mae: 0.09297 | train_rmse: 0.13752 | train_mse: 0.01891 | valid_mae: 0.09268 | valid_rmse: 0.13718 | valid_mse: 0.01882 |  0:17:02s\n",
      "epoch 252| loss: 0.01188 | train_mae: 0.1015  | train_rmse: 0.1432  | train_mse: 0.02051 | valid_mae: 0.10191 | valid_rmse: 0.14417 | valid_mse: 0.02078 |  0:17:06s\n",
      "epoch 253| loss: 0.01127 | train_mae: 0.09705 | train_rmse: 0.13951 | train_mse: 0.01946 | valid_mae: 0.09719 | valid_rmse: 0.14022 | valid_mse: 0.01966 |  0:17:10s\n",
      "epoch 254| loss: 0.01093 | train_mae: 0.13796 | train_rmse: 0.19453 | train_mse: 0.03784 | valid_mae: 0.13719 | valid_rmse: 0.19412 | valid_mse: 0.03768 |  0:17:14s\n",
      "epoch 255| loss: 0.01151 | train_mae: 0.09608 | train_rmse: 0.14017 | train_mse: 0.01965 | valid_mae: 0.096   | valid_rmse: 0.1405  | valid_mse: 0.01974 |  0:17:18s\n",
      "epoch 256| loss: 0.01063 | train_mae: 0.12189 | train_rmse: 0.17712 | train_mse: 0.03137 | valid_mae: 0.12213 | valid_rmse: 0.17768 | valid_mse: 0.03157 |  0:17:22s\n",
      "epoch 257| loss: 0.01024 | train_mae: 0.08895 | train_rmse: 0.13321 | train_mse: 0.01774 | valid_mae: 0.09019 | valid_rmse: 0.13536 | valid_mse: 0.01832 |  0:17:26s\n",
      "epoch 258| loss: 0.01037 | train_mae: 0.09361 | train_rmse: 0.14034 | train_mse: 0.01969 | valid_mae: 0.09356 | valid_rmse: 0.14007 | valid_mse: 0.01962 |  0:17:30s\n",
      "epoch 259| loss: 0.00997 | train_mae: 0.10857 | train_rmse: 0.16373 | train_mse: 0.02681 | valid_mae: 0.10945 | valid_rmse: 0.16576 | valid_mse: 0.02748 |  0:17:34s\n",
      "epoch 260| loss: 0.01    | train_mae: 0.08799 | train_rmse: 0.13172 | train_mse: 0.01735 | valid_mae: 0.08878 | valid_rmse: 0.13279 | valid_mse: 0.01763 |  0:17:38s\n",
      "epoch 261| loss: 0.0097  | train_mae: 0.09469 | train_rmse: 0.13824 | train_mse: 0.01911 | valid_mae: 0.09584 | valid_rmse: 0.14147 | valid_mse: 0.02001 |  0:17:42s\n",
      "epoch 262| loss: 0.00964 | train_mae: 0.09655 | train_rmse: 0.14526 | train_mse: 0.0211  | valid_mae: 0.09762 | valid_rmse: 0.14839 | valid_mse: 0.02202 |  0:17:46s\n",
      "epoch 263| loss: 0.00917 | train_mae: 0.08642 | train_rmse: 0.12904 | train_mse: 0.01665 | valid_mae: 0.08724 | valid_rmse: 0.13044 | valid_mse: 0.01701 |  0:17:50s\n",
      "epoch 264| loss: 0.00926 | train_mae: 0.0868  | train_rmse: 0.12834 | train_mse: 0.01647 | valid_mae: 0.0877  | valid_rmse: 0.13068 | valid_mse: 0.01708 |  0:17:54s\n",
      "epoch 265| loss: 0.00947 | train_mae: 0.08321 | train_rmse: 0.12321 | train_mse: 0.01518 | valid_mae: 0.08357 | valid_rmse: 0.12483 | valid_mse: 0.01558 |  0:17:58s\n",
      "epoch 266| loss: 0.0114  | train_mae: 0.11479 | train_rmse: 0.17517 | train_mse: 0.03068 | valid_mae: 0.11571 | valid_rmse: 0.17826 | valid_mse: 0.03178 |  0:18:02s\n",
      "epoch 267| loss: 0.00995 | train_mae: 0.12966 | train_rmse: 0.18194 | train_mse: 0.0331  | valid_mae: 0.12872 | valid_rmse: 0.18036 | valid_mse: 0.03253 |  0:18:06s\n",
      "epoch 268| loss: 0.00913 | train_mae: 0.09383 | train_rmse: 0.13353 | train_mse: 0.01783 | valid_mae: 0.09376 | valid_rmse: 0.13387 | valid_mse: 0.01792 |  0:18:10s\n",
      "epoch 269| loss: 0.00931 | train_mae: 0.08795 | train_rmse: 0.13411 | train_mse: 0.01799 | valid_mae: 0.08906 | valid_rmse: 0.13677 | valid_mse: 0.01871 |  0:18:14s\n",
      "epoch 270| loss: 0.00913 | train_mae: 0.08257 | train_rmse: 0.12376 | train_mse: 0.01532 | valid_mae: 0.08293 | valid_rmse: 0.12541 | valid_mse: 0.01573 |  0:18:18s\n",
      "epoch 271| loss: 0.00931 | train_mae: 0.10432 | train_rmse: 0.15883 | train_mse: 0.02523 | valid_mae: 0.10466 | valid_rmse: 0.15899 | valid_mse: 0.02528 |  0:18:22s\n",
      "epoch 272| loss: 0.00991 | train_mae: 0.10155 | train_rmse: 0.14957 | train_mse: 0.02237 | valid_mae: 0.10279 | valid_rmse: 0.15245 | valid_mse: 0.02324 |  0:18:26s\n",
      "epoch 273| loss: 0.00901 | train_mae: 0.07866 | train_rmse: 0.11801 | train_mse: 0.01393 | valid_mae: 0.07953 | valid_rmse: 0.11952 | valid_mse: 0.01429 |  0:18:30s\n",
      "epoch 274| loss: 0.00853 | train_mae: 0.11078 | train_rmse: 0.18695 | train_mse: 0.03495 | valid_mae: 0.11148 | valid_rmse: 0.18845 | valid_mse: 0.03551 |  0:18:34s\n",
      "epoch 275| loss: 0.00908 | train_mae: 0.09261 | train_rmse: 0.13375 | train_mse: 0.01789 | valid_mae: 0.09342 | valid_rmse: 0.13598 | valid_mse: 0.01849 |  0:18:38s\n",
      "epoch 276| loss: 0.00926 | train_mae: 0.08401 | train_rmse: 0.12428 | train_mse: 0.01545 | valid_mae: 0.08469 | valid_rmse: 0.12593 | valid_mse: 0.01586 |  0:18:42s\n",
      "epoch 277| loss: 0.00895 | train_mae: 0.08455 | train_rmse: 0.12214 | train_mse: 0.01492 | valid_mae: 0.08469 | valid_rmse: 0.12293 | valid_mse: 0.01511 |  0:18:46s\n",
      "epoch 278| loss: 0.00889 | train_mae: 0.21345 | train_rmse: 0.52265 | train_mse: 0.27316 | valid_mae: 0.22118 | valid_rmse: 0.54235 | valid_mse: 0.29415 |  0:18:50s\n",
      "epoch 279| loss: 0.00905 | train_mae: 0.08364 | train_rmse: 0.12426 | train_mse: 0.01544 | valid_mae: 0.08495 | valid_rmse: 0.12679 | valid_mse: 0.01607 |  0:18:54s\n",
      "epoch 280| loss: 0.00867 | train_mae: 0.08987 | train_rmse: 0.13091 | train_mse: 0.01714 | valid_mae: 0.08979 | valid_rmse: 0.13133 | valid_mse: 0.01725 |  0:18:58s\n",
      "epoch 281| loss: 0.00983 | train_mae: 0.08435 | train_rmse: 0.12581 | train_mse: 0.01583 | valid_mae: 0.08485 | valid_rmse: 0.12773 | valid_mse: 0.01631 |  0:19:02s\n",
      "epoch 282| loss: 0.00895 | train_mae: 0.08811 | train_rmse: 0.13124 | train_mse: 0.01722 | valid_mae: 0.08915 | valid_rmse: 0.13355 | valid_mse: 0.01784 |  0:19:06s\n",
      "epoch 283| loss: 0.00855 | train_mae: 0.08747 | train_rmse: 0.12583 | train_mse: 0.01583 | valid_mae: 0.08751 | valid_rmse: 0.12659 | valid_mse: 0.01602 |  0:19:10s\n",
      "epoch 284| loss: 0.00848 | train_mae: 0.07243 | train_rmse: 0.10945 | train_mse: 0.01198 | valid_mae: 0.07375 | valid_rmse: 0.1123  | valid_mse: 0.01261 |  0:19:14s\n",
      "epoch 285| loss: 0.01354 | train_mae: 0.13191 | train_rmse: 0.19146 | train_mse: 0.03666 | valid_mae: 0.13299 | valid_rmse: 0.19366 | valid_mse: 0.03751 |  0:19:18s\n",
      "epoch 286| loss: 0.0096  | train_mae: 0.09109 | train_rmse: 0.13333 | train_mse: 0.01778 | valid_mae: 0.09086 | valid_rmse: 0.13271 | valid_mse: 0.01761 |  0:19:22s\n",
      "epoch 287| loss: 0.00864 | train_mae: 0.08095 | train_rmse: 0.12251 | train_mse: 0.01501 | valid_mae: 0.08208 | valid_rmse: 0.12556 | valid_mse: 0.01576 |  0:19:26s\n",
      "epoch 288| loss: 0.00989 | train_mae: 0.13461 | train_rmse: 0.18904 | train_mse: 0.03574 | valid_mae: 0.13466 | valid_rmse: 0.19064 | valid_mse: 0.03634 |  0:19:30s\n",
      "epoch 289| loss: 0.00844 | train_mae: 0.07466 | train_rmse: 0.11089 | train_mse: 0.0123  | valid_mae: 0.07617 | valid_rmse: 0.11471 | valid_mse: 0.01316 |  0:19:34s\n",
      "epoch 290| loss: 0.00897 | train_mae: 0.09971 | train_rmse: 0.15641 | train_mse: 0.02446 | valid_mae: 0.10191 | valid_rmse: 0.16131 | valid_mse: 0.02602 |  0:19:38s\n",
      "epoch 291| loss: 0.00893 | train_mae: 0.08815 | train_rmse: 0.13317 | train_mse: 0.01773 | valid_mae: 0.08917 | valid_rmse: 0.13568 | valid_mse: 0.01841 |  0:19:42s\n",
      "epoch 292| loss: 0.01599 | train_mae: 0.18482 | train_rmse: 0.25373 | train_mse: 0.06438 | valid_mae: 0.18349 | valid_rmse: 0.25208 | valid_mse: 0.06354 |  0:19:46s\n",
      "epoch 293| loss: 0.01059 | train_mae: 0.09297 | train_rmse: 0.13609 | train_mse: 0.01852 | valid_mae: 0.09273 | valid_rmse: 0.13613 | valid_mse: 0.01853 |  0:19:50s\n",
      "epoch 294| loss: 0.00961 | train_mae: 0.09555 | train_rmse: 0.14257 | train_mse: 0.02033 | valid_mae: 0.09627 | valid_rmse: 0.14427 | valid_mse: 0.02081 |  0:19:54s\n",
      "epoch 295| loss: 0.00915 | train_mae: 0.08976 | train_rmse: 0.1348  | train_mse: 0.01817 | valid_mae: 0.09026 | valid_rmse: 0.13616 | valid_mse: 0.01854 |  0:19:58s\n",
      "epoch 296| loss: 0.00908 | train_mae: 0.09618 | train_rmse: 0.14659 | train_mse: 0.02149 | valid_mae: 0.09809 | valid_rmse: 0.15045 | valid_mse: 0.02264 |  0:20:02s\n",
      "epoch 297| loss: 0.00894 | train_mae: 0.10736 | train_rmse: 0.1689  | train_mse: 0.02853 | valid_mae: 0.10973 | valid_rmse: 0.17375 | valid_mse: 0.03019 |  0:20:06s\n",
      "epoch 298| loss: 0.0093  | train_mae: 0.10807 | train_rmse: 0.15023 | train_mse: 0.02257 | valid_mae: 0.10913 | valid_rmse: 0.15265 | valid_mse: 0.0233  |  0:20:10s\n",
      "epoch 299| loss: 0.00871 | train_mae: 0.09994 | train_rmse: 0.15567 | train_mse: 0.02423 | valid_mae: 0.10136 | valid_rmse: 0.15893 | valid_mse: 0.02526 |  0:20:14s\n",
      "epoch 300| loss: 0.00932 | train_mae: 0.09164 | train_rmse: 0.13866 | train_mse: 0.01923 | valid_mae: 0.09254 | valid_rmse: 0.14119 | valid_mse: 0.01993 |  0:20:18s\n",
      "epoch 301| loss: 0.00871 | train_mae: 0.07937 | train_rmse: 0.11793 | train_mse: 0.01391 | valid_mae: 0.07949 | valid_rmse: 0.11915 | valid_mse: 0.0142  |  0:20:22s\n",
      "epoch 302| loss: 0.00859 | train_mae: 0.09601 | train_rmse: 0.15188 | train_mse: 0.02307 | valid_mae: 0.09796 | valid_rmse: 0.15567 | valid_mse: 0.02423 |  0:20:26s\n",
      "epoch 303| loss: 0.0088  | train_mae: 0.08221 | train_rmse: 0.12236 | train_mse: 0.01497 | valid_mae: 0.08288 | valid_rmse: 0.12459 | valid_mse: 0.01552 |  0:20:30s\n",
      "epoch 304| loss: 0.0086  | train_mae: 0.10857 | train_rmse: 0.15665 | train_mse: 0.02454 | valid_mae: 0.1099  | valid_rmse: 0.15987 | valid_mse: 0.02556 |  0:20:34s\n",
      "epoch 305| loss: 0.00884 | train_mae: 0.10246 | train_rmse: 0.14896 | train_mse: 0.02219 | valid_mae: 0.10436 | valid_rmse: 0.15318 | valid_mse: 0.02346 |  0:20:38s\n",
      "epoch 306| loss: 0.00919 | train_mae: 0.10341 | train_rmse: 0.15393 | train_mse: 0.0237  | valid_mae: 0.10525 | valid_rmse: 0.15783 | valid_mse: 0.02491 |  0:20:42s\n",
      "epoch 307| loss: 0.00835 | train_mae: 0.08662 | train_rmse: 0.13109 | train_mse: 0.01719 | valid_mae: 0.08801 | valid_rmse: 0.13476 | valid_mse: 0.01816 |  0:20:46s\n",
      "epoch 308| loss: 0.00886 | train_mae: 0.077   | train_rmse: 0.11289 | train_mse: 0.01274 | valid_mae: 0.07811 | valid_rmse: 0.11542 | valid_mse: 0.01332 |  0:20:50s\n",
      "epoch 309| loss: 0.00864 | train_mae: 0.09652 | train_rmse: 0.15292 | train_mse: 0.02338 | valid_mae: 0.09755 | valid_rmse: 0.15499 | valid_mse: 0.02402 |  0:20:54s\n",
      "epoch 310| loss: 0.00883 | train_mae: 0.09396 | train_rmse: 0.14354 | train_mse: 0.0206  | valid_mae: 0.09609 | valid_rmse: 0.14788 | valid_mse: 0.02187 |  0:20:58s\n",
      "epoch 311| loss: 0.00892 | train_mae: 0.07992 | train_rmse: 0.11971 | train_mse: 0.01433 | valid_mae: 0.08133 | valid_rmse: 0.12295 | valid_mse: 0.01512 |  0:21:02s\n",
      "epoch 312| loss: 0.00829 | train_mae: 0.08314 | train_rmse: 0.12133 | train_mse: 0.01472 | valid_mae: 0.0834  | valid_rmse: 0.12277 | valid_mse: 0.01507 |  0:21:06s\n",
      "epoch 313| loss: 0.00845 | train_mae: 0.08731 | train_rmse: 0.12883 | train_mse: 0.0166  | valid_mae: 0.08768 | valid_rmse: 0.13009 | valid_mse: 0.01692 |  0:21:10s\n",
      "epoch 314| loss: 0.00866 | train_mae: 0.08437 | train_rmse: 0.12704 | train_mse: 0.01614 | valid_mae: 0.08521 | valid_rmse: 0.13031 | valid_mse: 0.01698 |  0:21:14s\n",
      "epoch 315| loss: 0.00837 | train_mae: 0.43608 | train_rmse: 2.51406 | train_mse: 6.3205  | valid_mae: 0.46663 | valid_rmse: 2.65743 | valid_mse: 7.06191 |  0:21:18s\n",
      "epoch 316| loss: 0.00933 | train_mae: 0.10597 | train_rmse: 0.20221 | train_mse: 0.04089 | valid_mae: 0.108   | valid_rmse: 0.209   | valid_mse: 0.04368 |  0:21:22s\n",
      "epoch 317| loss: 0.00871 | train_mae: 0.11516 | train_rmse: 0.16223 | train_mse: 0.02632 | valid_mae: 0.11452 | valid_rmse: 0.16107 | valid_mse: 0.02594 |  0:21:26s\n",
      "epoch 318| loss: 0.00812 | train_mae: 0.08216 | train_rmse: 0.12411 | train_mse: 0.0154  | valid_mae: 0.08269 | valid_rmse: 0.12546 | valid_mse: 0.01574 |  0:21:30s\n",
      "epoch 319| loss: 0.00837 | train_mae: 0.07362 | train_rmse: 0.10789 | train_mse: 0.01164 | valid_mae: 0.07482 | valid_rmse: 0.11103 | valid_mse: 0.01233 |  0:21:34s\n",
      "epoch 320| loss: 0.00876 | train_mae: 0.09693 | train_rmse: 0.14441 | train_mse: 0.02085 | valid_mae: 0.09904 | valid_rmse: 0.14892 | valid_mse: 0.02218 |  0:21:38s\n",
      "epoch 321| loss: 0.00918 | train_mae: 0.09005 | train_rmse: 0.13417 | train_mse: 0.018   | valid_mae: 0.09152 | valid_rmse: 0.13712 | valid_mse: 0.0188  |  0:21:42s\n",
      "epoch 322| loss: 0.00832 | train_mae: 0.09113 | train_rmse: 0.12961 | train_mse: 0.0168  | valid_mae: 0.09155 | valid_rmse: 0.13062 | valid_mse: 0.01706 |  0:21:46s\n",
      "epoch 323| loss: 0.00986 | train_mae: 0.10435 | train_rmse: 0.15462 | train_mse: 0.02391 | valid_mae: 0.10639 | valid_rmse: 0.15951 | valid_mse: 0.02544 |  0:21:50s\n",
      "epoch 324| loss: 0.0088  | train_mae: 0.07382 | train_rmse: 0.10997 | train_mse: 0.01209 | valid_mae: 0.07533 | valid_rmse: 0.11357 | valid_mse: 0.0129  |  0:21:54s\n",
      "epoch 325| loss: 0.00817 | train_mae: 0.10168 | train_rmse: 0.24431 | train_mse: 0.05969 | valid_mae: 0.10435 | valid_rmse: 0.25447 | valid_mse: 0.06476 |  0:21:58s\n",
      "epoch 326| loss: 0.00844 | train_mae: 0.59116 | train_rmse: 3.26869 | train_mse: 10.68431| valid_mae: 0.63046 | valid_rmse: 3.43445 | valid_mse: 11.79543|  0:22:02s\n",
      "epoch 327| loss: 0.00814 | train_mae: 0.1034  | train_rmse: 0.16009 | train_mse: 0.02563 | valid_mae: 0.10522 | valid_rmse: 0.16455 | valid_mse: 0.02708 |  0:22:06s\n",
      "epoch 328| loss: 0.00858 | train_mae: 0.08206 | train_rmse: 0.12662 | train_mse: 0.01603 | valid_mae: 0.08347 | valid_rmse: 0.13045 | valid_mse: 0.01702 |  0:22:10s\n",
      "epoch 329| loss: 0.00834 | train_mae: 0.08759 | train_rmse: 0.1312  | train_mse: 0.01721 | valid_mae: 0.08961 | valid_rmse: 0.13508 | valid_mse: 0.01825 |  0:22:14s\n",
      "epoch 330| loss: 0.00825 | train_mae: 0.62743 | train_rmse: 3.67843 | train_mse: 13.53086| valid_mae: 0.66188 | valid_rmse: 3.81281 | valid_mse: 14.5375 |  0:22:18s\n",
      "epoch 331| loss: 0.0083  | train_mae: 0.08779 | train_rmse: 0.13321 | train_mse: 0.01775 | valid_mae: 0.08921 | valid_rmse: 0.13699 | valid_mse: 0.01877 |  0:22:22s\n",
      "epoch 332| loss: 0.00849 | train_mae: 0.09917 | train_rmse: 0.15463 | train_mse: 0.02391 | valid_mae: 0.10113 | valid_rmse: 0.159   | valid_mse: 0.02528 |  0:22:26s\n",
      "epoch 333| loss: 0.00844 | train_mae: 0.11397 | train_rmse: 0.1841  | train_mse: 0.03389 | valid_mae: 0.11643 | valid_rmse: 0.18767 | valid_mse: 0.03522 |  0:22:30s\n",
      "epoch 334| loss: 0.00845 | train_mae: 0.08675 | train_rmse: 0.12917 | train_mse: 0.01669 | valid_mae: 0.08839 | valid_rmse: 0.13281 | valid_mse: 0.01764 |  0:22:34s\n",
      "epoch 335| loss: 0.0082  | train_mae: 0.10264 | train_rmse: 0.16115 | train_mse: 0.02597 | valid_mae: 0.10464 | valid_rmse: 0.16535 | valid_mse: 0.02734 |  0:22:38s\n",
      "epoch 336| loss: 0.00841 | train_mae: 0.08595 | train_rmse: 0.13031 | train_mse: 0.01698 | valid_mae: 0.08757 | valid_rmse: 0.13359 | valid_mse: 0.01785 |  0:22:42s\n",
      "epoch 337| loss: 0.00862 | train_mae: 0.11098 | train_rmse: 0.23484 | train_mse: 0.05515 | valid_mae: 0.11456 | valid_rmse: 0.24463 | valid_mse: 0.05984 |  0:22:46s\n",
      "epoch 338| loss: 0.00818 | train_mae: 0.10196 | train_rmse: 0.16303 | train_mse: 0.02658 | valid_mae: 0.10416 | valid_rmse: 0.16768 | valid_mse: 0.02812 |  0:22:50s\n",
      "epoch 339| loss: 0.00803 | train_mae: 0.1059  | train_rmse: 0.1561  | train_mse: 0.02437 | valid_mae: 0.10545 | valid_rmse: 0.15494 | valid_mse: 0.02401 |  0:22:54s\n",
      "epoch 340| loss: 0.00858 | train_mae: 0.12707 | train_rmse: 0.39643 | train_mse: 0.15716 | valid_mae: 0.13039 | valid_rmse: 0.41139 | valid_mse: 0.16924 |  0:22:58s\n",
      "epoch 341| loss: 0.00833 | train_mae: 0.07602 | train_rmse: 0.11365 | train_mse: 0.01292 | valid_mae: 0.07727 | valid_rmse: 0.1167  | valid_mse: 0.01362 |  0:23:02s\n",
      "epoch 342| loss: 0.00849 | train_mae: 0.10618 | train_rmse: 0.20949 | train_mse: 0.04389 | valid_mae: 0.10927 | valid_rmse: 0.21664 | valid_mse: 0.04693 |  0:23:06s\n",
      "epoch 343| loss: 0.00817 | train_mae: 0.10342 | train_rmse: 0.15659 | train_mse: 0.02452 | valid_mae: 0.10465 | valid_rmse: 0.15987 | valid_mse: 0.02556 |  0:23:10s\n",
      "epoch 344| loss: 0.00852 | train_mae: 0.09273 | train_rmse: 0.13967 | train_mse: 0.01951 | valid_mae: 0.0942  | valid_rmse: 0.14253 | valid_mse: 0.02031 |  0:23:14s\n",
      "epoch 345| loss: 0.00808 | train_mae: 0.07806 | train_rmse: 0.1209  | train_mse: 0.01462 | valid_mae: 0.07952 | valid_rmse: 0.12406 | valid_mse: 0.01539 |  0:23:18s\n",
      "epoch 346| loss: 0.00779 | train_mae: 0.12053 | train_rmse: 0.19198 | train_mse: 0.03686 | valid_mae: 0.12282 | valid_rmse: 0.19687 | valid_mse: 0.03876 |  0:23:22s\n",
      "epoch 347| loss: 0.00819 | train_mae: 0.44041 | train_rmse: 2.55743 | train_mse: 6.54047 | valid_mae: 0.47152 | valid_rmse: 2.71994 | valid_mse: 7.39807 |  0:23:26s\n",
      "epoch 348| loss: 0.00854 | train_mae: 0.08072 | train_rmse: 0.12109 | train_mse: 0.01466 | valid_mae: 0.08169 | valid_rmse: 0.12417 | valid_mse: 0.01542 |  0:23:30s\n",
      "epoch 349| loss: 0.00793 | train_mae: 0.1982  | train_rmse: 0.80068 | train_mse: 0.64109 | valid_mae: 0.20578 | valid_rmse: 0.83961 | valid_mse: 0.70494 |  0:23:34s\n",
      "epoch 350| loss: 0.0083  | train_mae: 0.09289 | train_rmse: 0.14486 | train_mse: 0.02098 | valid_mae: 0.09441 | valid_rmse: 0.14746 | valid_mse: 0.02174 |  0:23:38s\n",
      "epoch 351| loss: 0.00811 | train_mae: 0.1017  | train_rmse: 0.14807 | train_mse: 0.02192 | valid_mae: 0.10263 | valid_rmse: 0.15058 | valid_mse: 0.02268 |  0:23:43s\n",
      "epoch 352| loss: 0.00817 | train_mae: 0.10155 | train_rmse: 0.13628 | train_mse: 0.01857 | valid_mae: 0.10274 | valid_rmse: 0.13865 | valid_mse: 0.01922 |  0:23:47s\n",
      "epoch 353| loss: 0.00905 | train_mae: 0.09346 | train_rmse: 0.13729 | train_mse: 0.01885 | valid_mae: 0.09411 | valid_rmse: 0.13923 | valid_mse: 0.01939 |  0:23:51s\n",
      "epoch 354| loss: 0.00839 | train_mae: 0.12865 | train_rmse: 0.18947 | train_mse: 0.0359  | valid_mae: 0.12968 | valid_rmse: 0.19229 | valid_mse: 0.03698 |  0:23:55s\n",
      "epoch 355| loss: 0.00813 | train_mae: 0.08864 | train_rmse: 0.1347  | train_mse: 0.01814 | valid_mae: 0.08999 | valid_rmse: 0.1381  | valid_mse: 0.01907 |  0:23:59s\n",
      "epoch 356| loss: 0.00824 | train_mae: 0.07993 | train_rmse: 0.11465 | train_mse: 0.01314 | valid_mae: 0.08153 | valid_rmse: 0.11796 | valid_mse: 0.01391 |  0:24:03s\n",
      "epoch 357| loss: 0.00816 | train_mae: 0.08605 | train_rmse: 0.12645 | train_mse: 0.01599 | valid_mae: 0.08677 | valid_rmse: 0.12883 | valid_mse: 0.0166  |  0:24:07s\n",
      "epoch 358| loss: 0.00794 | train_mae: 0.10194 | train_rmse: 0.16134 | train_mse: 0.02603 | valid_mae: 0.10311 | valid_rmse: 0.16354 | valid_mse: 0.02674 |  0:24:10s\n",
      "epoch 359| loss: 0.00846 | train_mae: 0.06958 | train_rmse: 0.10489 | train_mse: 0.011   | valid_mae: 0.07099 | valid_rmse: 0.10859 | valid_mse: 0.01179 |  0:24:15s\n",
      "epoch 360| loss: 0.00802 | train_mae: 0.08275 | train_rmse: 0.12536 | train_mse: 0.01571 | valid_mae: 0.08427 | valid_rmse: 0.12882 | valid_mse: 0.01659 |  0:24:19s\n",
      "epoch 361| loss: 0.00864 | train_mae: 0.08897 | train_rmse: 0.13425 | train_mse: 0.01802 | valid_mae: 0.08968 | valid_rmse: 0.13544 | valid_mse: 0.01834 |  0:24:22s\n",
      "epoch 362| loss: 0.01304 | train_mae: 0.20701 | train_rmse: 0.33668 | train_mse: 0.11335 | valid_mae: 0.20703 | valid_rmse: 0.33707 | valid_mse: 0.11361 |  0:24:27s\n",
      "epoch 363| loss: 0.01093 | train_mae: 0.75906 | train_rmse: 4.92334 | train_mse: 24.23931| valid_mae: 0.79191 | valid_rmse: 5.0545  | valid_mse: 25.54798|  0:24:31s\n",
      "epoch 364| loss: 0.0093  | train_mae: 0.08844 | train_rmse: 0.12742 | train_mse: 0.01624 | valid_mae: 0.08939 | valid_rmse: 0.1299  | valid_mse: 0.01687 |  0:24:35s\n",
      "epoch 365| loss: 0.00866 | train_mae: 0.07386 | train_rmse: 0.11001 | train_mse: 0.0121  | valid_mae: 0.07499 | valid_rmse: 0.11304 | valid_mse: 0.01278 |  0:24:39s\n",
      "epoch 366| loss: 0.00835 | train_mae: 0.08153 | train_rmse: 0.13315 | train_mse: 0.01773 | valid_mae: 0.08303 | valid_rmse: 0.13666 | valid_mse: 0.01867 |  0:24:43s\n",
      "epoch 367| loss: 0.00871 | train_mae: 0.06802 | train_rmse: 0.10328 | train_mse: 0.01067 | valid_mae: 0.06922 | valid_rmse: 0.10592 | valid_mse: 0.01122 |  0:24:47s\n",
      "epoch 368| loss: 0.00835 | train_mae: 0.0705  | train_rmse: 0.1054  | train_mse: 0.01111 | valid_mae: 0.07139 | valid_rmse: 0.1075  | valid_mse: 0.01156 |  0:24:51s\n",
      "epoch 369| loss: 0.00811 | train_mae: 0.08214 | train_rmse: 0.12168 | train_mse: 0.01481 | valid_mae: 0.08306 | valid_rmse: 0.12363 | valid_mse: 0.01528 |  0:24:55s\n",
      "epoch 370| loss: 0.00799 | train_mae: 0.16222 | train_rmse: 0.7487  | train_mse: 0.56056 | valid_mae: 0.1694  | valid_rmse: 0.79984 | valid_mse: 0.63975 |  0:24:59s\n",
      "epoch 371| loss: 0.00851 | train_mae: 0.22011 | train_rmse: 0.57875 | train_mse: 0.33495 | valid_mae: 0.22667 | valid_rmse: 0.59606 | valid_mse: 0.35529 |  0:25:03s\n",
      "epoch 372| loss: 0.008   | train_mae: 0.17597 | train_rmse: 0.38385 | train_mse: 0.14734 | valid_mae: 0.18001 | valid_rmse: 0.3977  | valid_mse: 0.15817 |  0:25:08s\n",
      "epoch 373| loss: 0.0087  | train_mae: 0.12187 | train_rmse: 0.16846 | train_mse: 0.02838 | valid_mae: 0.12203 | valid_rmse: 0.16924 | valid_mse: 0.02864 |  0:25:12s\n",
      "epoch 374| loss: 0.00824 | train_mae: 0.07346 | train_rmse: 0.11552 | train_mse: 0.01334 | valid_mae: 0.07372 | valid_rmse: 0.11487 | valid_mse: 0.0132  |  0:25:16s\n",
      "epoch 375| loss: 0.01245 | train_mae: 8.9868  | train_rmse: 26.32586| train_mse: 693.05098| valid_mae: 9.43815 | valid_rmse: 27.14888| valid_mse: 737.06161|  0:25:20s\n",
      "epoch 376| loss: 0.01773 | train_mae: 0.24577 | train_rmse: 1.00092 | train_mse: 1.00184 | valid_mae: 0.23046 | valid_rmse: 0.72753 | valid_mse: 0.5293  |  0:25:25s\n",
      "epoch 377| loss: 0.01156 | train_mae: 0.19667 | train_rmse: 0.69402 | train_mse: 0.48166 | valid_mae: 0.19029 | valid_rmse: 0.48375 | valid_mse: 0.23402 |  0:25:29s\n",
      "epoch 378| loss: 0.00938 | train_mae: 0.33355 | train_rmse: 5.31695 | train_mse: 28.26997| valid_mae: 0.26466 | valid_rmse: 4.34748 | valid_mse: 18.9006 |  0:25:33s\n",
      "epoch 379| loss: 0.00942 | train_mae: 0.23025 | train_rmse: 1.95598 | train_mse: 3.82586 | valid_mae: 0.21981 | valid_rmse: 1.64671 | valid_mse: 2.71166 |  0:25:37s\n",
      "epoch 380| loss: 0.00949 | train_mae: 0.62071 | train_rmse: 3.21612 | train_mse: 10.34346| valid_mae: 0.62795 | valid_rmse: 2.8271  | valid_mse: 7.99247 |  0:25:41s\n",
      "epoch 381| loss: 0.00914 | train_mae: 0.10169 | train_rmse: 0.14455 | train_mse: 0.0209  | valid_mae: 0.10209 | valid_rmse: 0.1455  | valid_mse: 0.02117 |  0:25:45s\n",
      "epoch 382| loss: 0.00895 | train_mae: 1.65504 | train_rmse: 7.88442 | train_mse: 62.16404| valid_mae: 1.83267 | valid_rmse: 8.46284 | valid_mse: 71.61969|  0:25:49s\n",
      "epoch 383| loss: 0.00928 | train_mae: 0.08563 | train_rmse: 0.12719 | train_mse: 0.01618 | valid_mae: 0.0871  | valid_rmse: 0.13042 | valid_mse: 0.01701 |  0:25:53s\n",
      "epoch 384| loss: 0.00841 | train_mae: 0.07817 | train_rmse: 0.11721 | train_mse: 0.01374 | valid_mae: 0.07865 | valid_rmse: 0.11838 | valid_mse: 0.01401 |  0:25:57s\n",
      "epoch 385| loss: 0.00858 | train_mae: 0.16665 | train_rmse: 0.56944 | train_mse: 0.32426 | valid_mae: 0.16197 | valid_rmse: 0.38795 | valid_mse: 0.15051 |  0:26:01s\n",
      "epoch 386| loss: 0.00819 | train_mae: 0.07959 | train_rmse: 0.1171  | train_mse: 0.01371 | valid_mae: 0.08027 | valid_rmse: 0.11951 | valid_mse: 0.01428 |  0:26:06s\n",
      "epoch 387| loss: 0.00831 | train_mae: 0.178   | train_rmse: 2.81867 | train_mse: 7.94488 | valid_mae: 0.13335 | valid_rmse: 1.81922 | valid_mse: 3.30955 |  0:26:10s\n",
      "epoch 388| loss: 0.00878 | train_mae: 0.16415 | train_rmse: 1.99753 | train_mse: 3.99011 | valid_mae: 0.13451 | valid_rmse: 1.26281 | valid_mse: 1.59469 |  0:26:14s\n",
      "epoch 389| loss: 0.00851 | train_mae: 0.26376 | train_rmse: 1.87654 | train_mse: 3.52142 | valid_mae: 0.24593 | valid_rmse: 1.22316 | valid_mse: 1.49612 |  0:26:19s\n",
      "epoch 390| loss: 0.00802 | train_mae: 0.12225 | train_rmse: 1.46664 | train_mse: 2.15105 | valid_mae: 0.10036 | valid_rmse: 0.81889 | valid_mse: 0.67058 |  0:26:23s\n",
      "epoch 391| loss: 0.00835 | train_mae: 0.18438 | train_rmse: 3.22862 | train_mse: 10.424  | valid_mae: 0.12803 | valid_rmse: 2.10077 | valid_mse: 4.41325 |  0:26:28s\n",
      "epoch 392| loss: 0.00826 | train_mae: 0.38278 | train_rmse: 6.16947 | train_mse: 38.06235| valid_mae: 0.25111 | valid_rmse: 3.88912 | valid_mse: 15.12527|  0:26:32s\n",
      "epoch 393| loss: 0.0081  | train_mae: 0.26871 | train_rmse: 3.38423 | train_mse: 11.45299| valid_mae: 0.20789 | valid_rmse: 2.20418 | valid_mse: 4.85841 |  0:26:36s\n",
      "epoch 394| loss: 0.00796 | train_mae: 0.35723 | train_rmse: 3.9726  | train_mse: 15.78155| valid_mae: 0.29121 | valid_rmse: 2.52819 | valid_mse: 6.39174 |  0:26:40s\n",
      "epoch 395| loss: 0.00851 | train_mae: 0.43271 | train_rmse: 8.46675 | train_mse: 71.68591| valid_mae: 0.2374  | valid_rmse: 4.84675 | valid_mse: 23.491  |  0:26:45s\n",
      "epoch 396| loss: 0.00853 | train_mae: 0.39165 | train_rmse: 7.08827 | train_mse: 50.24361| valid_mae: 0.25009 | valid_rmse: 5.07348 | valid_mse: 25.74021|  0:26:49s\n",
      "epoch 397| loss: 0.00826 | train_mae: 0.73451 | train_rmse: 14.05171| train_mse: 197.4506| valid_mae: 0.44251 | valid_rmse: 10.18347| valid_mse: 103.70301|  0:26:54s\n",
      "epoch 398| loss: 0.00804 | train_mae: 0.48061 | train_rmse: 9.22684 | train_mse: 85.13466| valid_mae: 0.27082 | valid_rmse: 6.13936 | valid_mse: 37.69179|  0:26:58s\n",
      "epoch 399| loss: 0.00829 | train_mae: 0.43983 | train_rmse: 7.78607 | train_mse: 60.6229 | valid_mae: 0.26548 | valid_rmse: 5.45614 | valid_mse: 29.76949|  0:27:03s\n",
      "epoch 400| loss: 0.00844 | train_mae: 0.3355  | train_rmse: 5.75616 | train_mse: 33.13338| valid_mae: 0.24229 | valid_rmse: 4.26243 | valid_mse: 18.16828|  0:27:07s\n",
      "epoch 401| loss: 0.00822 | train_mae: 0.26713 | train_rmse: 5.33262 | train_mse: 28.43686| valid_mae: 0.1811  | valid_rmse: 3.8928  | valid_mse: 15.15386|  0:27:12s\n",
      "epoch 402| loss: 0.00781 | train_mae: 0.48272 | train_rmse: 9.56201 | train_mse: 91.43207| valid_mae: 0.27993 | valid_rmse: 6.5649  | valid_mse: 43.09788|  0:27:16s\n",
      "epoch 403| loss: 0.00809 | train_mae: 0.36813 | train_rmse: 7.43189 | train_mse: 55.23305| valid_mae: 0.22867 | valid_rmse: 5.30548 | valid_mse: 28.14808|  0:27:20s\n",
      "epoch 404| loss: 0.0082  | train_mae: 0.44127 | train_rmse: 6.16683 | train_mse: 38.02976| valid_mae: 0.32829 | valid_rmse: 4.34565 | valid_mse: 18.88469|  0:27:24s\n",
      "epoch 405| loss: 0.00814 | train_mae: 0.35832 | train_rmse: 6.5661  | train_mse: 43.1137 | valid_mae: 0.23228 | valid_rmse: 4.50588 | valid_mse: 20.30299|  0:27:28s\n",
      "epoch 406| loss: 0.00791 | train_mae: 0.48683 | train_rmse: 6.42702 | train_mse: 41.30663| valid_mae: 0.37389 | valid_rmse: 4.62889 | valid_mse: 21.42659|  0:27:32s\n",
      "epoch 407| loss: 0.0082  | train_mae: 0.47978 | train_rmse: 6.68569 | train_mse: 44.69846| valid_mae: 0.34407 | valid_rmse: 4.81183 | valid_mse: 23.15372|  0:27:36s\n",
      "epoch 408| loss: 0.00783 | train_mae: 0.61008 | train_rmse: 4.28603 | train_mse: 18.37007| valid_mae: 0.56572 | valid_rmse: 3.26075 | valid_mse: 10.6325 |  0:27:40s\n",
      "epoch 409| loss: 0.01412 | train_mae: 0.36642 | train_rmse: 2.59911 | train_mse: 6.75537 | valid_mae: 0.30605 | valid_rmse: 1.76567 | valid_mse: 3.1176  |  0:27:44s\n",
      "epoch 410| loss: 0.014   | train_mae: 0.09316 | train_rmse: 0.13271 | train_mse: 0.01761 | valid_mae: 0.09364 | valid_rmse: 0.13565 | valid_mse: 0.0184  |  0:27:49s\n",
      "epoch 411| loss: 0.01739 | train_mae: 0.11338 | train_rmse: 0.16672 | train_mse: 0.0278  | valid_mae: 0.11241 | valid_rmse: 0.16643 | valid_mse: 0.0277  |  0:27:53s\n",
      "epoch 412| loss: 0.01154 | train_mae: 0.08821 | train_rmse: 0.1323  | train_mse: 0.0175  | valid_mae: 0.08863 | valid_rmse: 0.13451 | valid_mse: 0.01809 |  0:27:57s\n",
      "epoch 413| loss: 0.01034 | train_mae: 0.09028 | train_rmse: 0.13438 | train_mse: 0.01806 | valid_mae: 0.09073 | valid_rmse: 0.13598 | valid_mse: 0.01849 |  0:28:01s\n",
      "epoch 414| loss: 0.01041 | train_mae: 0.08901 | train_rmse: 0.13385 | train_mse: 0.01791 | valid_mae: 0.08978 | valid_rmse: 0.13659 | valid_mse: 0.01866 |  0:28:06s\n",
      "epoch 415| loss: 0.00965 | train_mae: 0.08237 | train_rmse: 0.12296 | train_mse: 0.01512 | valid_mae: 0.08377 | valid_rmse: 0.12644 | valid_mse: 0.01599 |  0:28:10s\n",
      "epoch 416| loss: 0.00953 | train_mae: 0.07495 | train_rmse: 0.11516 | train_mse: 0.01326 | valid_mae: 0.07572 | valid_rmse: 0.11815 | valid_mse: 0.01396 |  0:28:14s\n",
      "epoch 417| loss: 0.00973 | train_mae: 0.08359 | train_rmse: 0.12874 | train_mse: 0.01657 | valid_mae: 0.0844  | valid_rmse: 0.13057 | valid_mse: 0.01705 |  0:28:18s\n",
      "epoch 418| loss: 0.00938 | train_mae: 0.07274 | train_rmse: 0.11022 | train_mse: 0.01215 | valid_mae: 0.07427 | valid_rmse: 0.11412 | valid_mse: 0.01302 |  0:28:22s\n",
      "epoch 419| loss: 0.00941 | train_mae: 0.08861 | train_rmse: 0.1299  | train_mse: 0.01687 | valid_mae: 0.08927 | valid_rmse: 0.13173 | valid_mse: 0.01735 |  0:28:26s\n",
      "epoch 420| loss: 0.00921 | train_mae: 0.08147 | train_rmse: 0.1232  | train_mse: 0.01518 | valid_mae: 0.08225 | valid_rmse: 0.12579 | valid_mse: 0.01582 |  0:28:30s\n",
      "epoch 421| loss: 0.0096  | train_mae: 0.08329 | train_rmse: 0.12265 | train_mse: 0.01504 | valid_mae: 0.08438 | valid_rmse: 0.12626 | valid_mse: 0.01594 |  0:28:34s\n",
      "epoch 422| loss: 0.00954 | train_mae: 0.07361 | train_rmse: 0.11055 | train_mse: 0.01222 | valid_mae: 0.07541 | valid_rmse: 0.11516 | valid_mse: 0.01326 |  0:28:38s\n",
      "epoch 423| loss: 0.01001 | train_mae: 0.08381 | train_rmse: 0.12544 | train_mse: 0.01573 | valid_mae: 0.08498 | valid_rmse: 0.12868 | valid_mse: 0.01656 |  0:28:43s\n",
      "epoch 424| loss: 0.00903 | train_mae: 0.08044 | train_rmse: 0.11725 | train_mse: 0.01375 | valid_mae: 0.08126 | valid_rmse: 0.11986 | valid_mse: 0.01437 |  0:28:46s\n",
      "epoch 425| loss: 0.00913 | train_mae: 0.07892 | train_rmse: 0.11933 | train_mse: 0.01424 | valid_mae: 0.08055 | valid_rmse: 0.123   | valid_mse: 0.01513 |  0:28:50s\n",
      "epoch 426| loss: 0.00954 | train_mae: 0.07763 | train_rmse: 0.11543 | train_mse: 0.01333 | valid_mae: 0.07885 | valid_rmse: 0.11948 | valid_mse: 0.01427 |  0:28:54s\n",
      "epoch 427| loss: 0.00919 | train_mae: 0.09325 | train_rmse: 0.13744 | train_mse: 0.01889 | valid_mae: 0.09287 | valid_rmse: 0.13752 | valid_mse: 0.01891 |  0:28:58s\n",
      "epoch 428| loss: 0.00873 | train_mae: 0.08866 | train_rmse: 0.13013 | train_mse: 0.01693 | valid_mae: 0.08914 | valid_rmse: 0.13168 | valid_mse: 0.01734 |  0:29:03s\n",
      "epoch 429| loss: 0.00924 | train_mae: 0.07775 | train_rmse: 0.11703 | train_mse: 0.0137  | valid_mae: 0.07877 | valid_rmse: 0.11948 | valid_mse: 0.01428 |  0:29:07s\n",
      "epoch 430| loss: 0.00963 | train_mae: 0.07927 | train_rmse: 0.12147 | train_mse: 0.01476 | valid_mae: 0.08002 | valid_rmse: 0.12299 | valid_mse: 0.01513 |  0:29:11s\n",
      "epoch 431| loss: 0.00897 | train_mae: 0.07334 | train_rmse: 0.11331 | train_mse: 0.01284 | valid_mae: 0.07381 | valid_rmse: 0.11507 | valid_mse: 0.01324 |  0:29:15s\n",
      "epoch 432| loss: 0.00941 | train_mae: 0.07604 | train_rmse: 0.1131  | train_mse: 0.01279 | valid_mae: 0.07724 | valid_rmse: 0.11685 | valid_mse: 0.01365 |  0:29:19s\n",
      "epoch 433| loss: 0.00882 | train_mae: 0.07926 | train_rmse: 0.11946 | train_mse: 0.01427 | valid_mae: 0.07969 | valid_rmse: 0.12079 | valid_mse: 0.01459 |  0:29:23s\n",
      "epoch 434| loss: 0.00989 | train_mae: 0.08038 | train_rmse: 0.12136 | train_mse: 0.01473 | valid_mae: 0.08196 | valid_rmse: 0.12524 | valid_mse: 0.01569 |  0:29:27s\n",
      "epoch 435| loss: 0.00918 | train_mae: 0.10176 | train_rmse: 0.14132 | train_mse: 0.01997 | valid_mae: 0.10235 | valid_rmse: 0.1427  | valid_mse: 0.02036 |  0:29:31s\n",
      "epoch 436| loss: 0.00871 | train_mae: 0.08395 | train_rmse: 0.12724 | train_mse: 0.01619 | valid_mae: 0.08405 | valid_rmse: 0.12896 | valid_mse: 0.01663 |  0:29:35s\n",
      "epoch 437| loss: 0.00882 | train_mae: 0.07888 | train_rmse: 0.11819 | train_mse: 0.01397 | valid_mae: 0.07987 | valid_rmse: 0.12169 | valid_mse: 0.01481 |  0:29:40s\n",
      "epoch 438| loss: 0.00891 | train_mae: 0.08163 | train_rmse: 0.11644 | train_mse: 0.01356 | valid_mae: 0.08216 | valid_rmse: 0.11896 | valid_mse: 0.01415 |  0:29:45s\n",
      "epoch 439| loss: 0.01062 | train_mae: 0.10047 | train_rmse: 0.13846 | train_mse: 0.01917 | valid_mae: 0.10188 | valid_rmse: 0.14208 | valid_mse: 0.02019 |  0:29:50s\n",
      "epoch 440| loss: 0.00964 | train_mae: 0.08261 | train_rmse: 0.11915 | train_mse: 0.0142  | valid_mae: 0.0834  | valid_rmse: 0.12218 | valid_mse: 0.01493 |  0:29:55s\n",
      "epoch 441| loss: 0.00919 | train_mae: 0.0762  | train_rmse: 0.11428 | train_mse: 0.01306 | valid_mae: 0.07719 | valid_rmse: 0.11755 | valid_mse: 0.01382 |  0:29:59s\n",
      "epoch 442| loss: 0.00876 | train_mae: 0.07611 | train_rmse: 0.11709 | train_mse: 0.01371 | valid_mae: 0.07693 | valid_rmse: 0.11978 | valid_mse: 0.01435 |  0:30:03s\n",
      "epoch 443| loss: 0.0087  | train_mae: 0.07347 | train_rmse: 0.11031 | train_mse: 0.01217 | valid_mae: 0.07469 | valid_rmse: 0.11392 | valid_mse: 0.01298 |  0:30:07s\n",
      "epoch 444| loss: 0.00902 | train_mae: 0.08085 | train_rmse: 0.12443 | train_mse: 0.01548 | valid_mae: 0.08122 | valid_rmse: 0.12587 | valid_mse: 0.01584 |  0:30:12s\n",
      "epoch 445| loss: 0.00897 | train_mae: 0.09007 | train_rmse: 0.12898 | train_mse: 0.01664 | valid_mae: 0.0905  | valid_rmse: 0.13098 | valid_mse: 0.01716 |  0:30:16s\n",
      "epoch 446| loss: 0.00859 | train_mae: 0.09533 | train_rmse: 0.13448 | train_mse: 0.01808 | valid_mae: 0.09483 | valid_rmse: 0.13538 | valid_mse: 0.01833 |  0:30:20s\n",
      "epoch 447| loss: 0.00913 | train_mae: 0.10275 | train_rmse: 0.14681 | train_mse: 0.02155 | valid_mae: 0.10241 | valid_rmse: 0.14715 | valid_mse: 0.02165 |  0:30:24s\n",
      "epoch 448| loss: 0.00934 | train_mae: 0.08058 | train_rmse: 0.11893 | train_mse: 0.01414 | valid_mae: 0.08158 | valid_rmse: 0.1227  | valid_mse: 0.01506 |  0:30:28s\n",
      "epoch 449| loss: 0.00874 | train_mae: 0.0795  | train_rmse: 0.1175  | train_mse: 0.01381 | valid_mae: 0.07997 | valid_rmse: 0.11901 | valid_mse: 0.01416 |  0:30:32s\n",
      "epoch 450| loss: 0.00863 | train_mae: 0.07821 | train_rmse: 0.11739 | train_mse: 0.01378 | valid_mae: 0.07888 | valid_rmse: 0.11928 | valid_mse: 0.01423 |  0:30:36s\n",
      "epoch 451| loss: 0.00879 | train_mae: 0.082   | train_rmse: 0.12648 | train_mse: 0.016   | valid_mae: 0.08259 | valid_rmse: 0.12843 | valid_mse: 0.01649 |  0:30:40s\n",
      "epoch 452| loss: 0.00866 | train_mae: 0.07391 | train_rmse: 0.11641 | train_mse: 0.01355 | valid_mae: 0.07446 | valid_rmse: 0.1178  | valid_mse: 0.01388 |  0:30:44s\n",
      "epoch 453| loss: 0.00912 | train_mae: 0.07628 | train_rmse: 0.11423 | train_mse: 0.01305 | valid_mae: 0.07746 | valid_rmse: 0.1177  | valid_mse: 0.01385 |  0:30:48s\n",
      "epoch 454| loss: 0.0094  | train_mae: 0.08808 | train_rmse: 0.13118 | train_mse: 0.01721 | valid_mae: 0.0887  | valid_rmse: 0.13309 | valid_mse: 0.01771 |  0:30:53s\n",
      "epoch 455| loss: 0.0093  | train_mae: 0.07403 | train_rmse: 0.11254 | train_mse: 0.01266 | valid_mae: 0.07505 | valid_rmse: 0.11594 | valid_mse: 0.01344 |  0:30:57s\n",
      "epoch 456| loss: 0.0089  | train_mae: 0.07947 | train_rmse: 0.12948 | train_mse: 0.01677 | valid_mae: 0.08077 | valid_rmse: 0.13305 | valid_mse: 0.0177  |  0:31:01s\n",
      "epoch 457| loss: 0.00841 | train_mae: 0.07055 | train_rmse: 0.1063  | train_mse: 0.0113  | valid_mae: 0.07164 | valid_rmse: 0.10971 | valid_mse: 0.01204 |  0:31:05s\n",
      "epoch 458| loss: 0.00959 | train_mae: 0.19518 | train_rmse: 3.6426  | train_mse: 13.26852| valid_mae: 0.19589 | valid_rmse: 3.63964 | valid_mse: 13.24698|  0:31:09s\n",
      "epoch 459| loss: 0.00869 | train_mae: 0.11846 | train_rmse: 2.08098 | train_mse: 4.33048 | valid_mae: 0.12324 | valid_rmse: 2.10697 | valid_mse: 4.4393  |  0:31:13s\n",
      "epoch 460| loss: 0.00869 | train_mae: 0.1059  | train_rmse: 1.53614 | train_mse: 2.35972 | valid_mae: 0.10344 | valid_rmse: 1.39819 | valid_mse: 1.95493 |  0:31:17s\n",
      "epoch 461| loss: 0.00832 | train_mae: 0.23211 | train_rmse: 5.70273 | train_mse: 32.52113| valid_mae: 0.28181 | valid_rmse: 6.4404  | valid_mse: 41.47871|  0:31:22s\n",
      "epoch 462| loss: 0.0089  | train_mae: 0.20811 | train_rmse: 4.45013 | train_mse: 19.80363| valid_mae: 0.23287 | valid_rmse: 4.77013 | valid_mse: 22.75411|  0:31:25s\n",
      "epoch 463| loss: 0.00862 | train_mae: 0.14255 | train_rmse: 2.53546 | train_mse: 6.42856 | valid_mae: 0.14484 | valid_rmse: 2.55361 | valid_mse: 6.52094 |  0:31:29s\n",
      "epoch 464| loss: 0.00885 | train_mae: 0.11149 | train_rmse: 1.73354 | train_mse: 3.00515 | valid_mae: 0.1186  | valid_rmse: 1.75976 | valid_mse: 3.09676 |  0:31:33s\n",
      "epoch 465| loss: 0.00884 | train_mae: 0.10803 | train_rmse: 1.41395 | train_mse: 1.99924 | valid_mae: 0.10386 | valid_rmse: 1.2236  | valid_mse: 1.49721 |  0:31:37s\n",
      "epoch 466| loss: 0.00875 | train_mae: 0.10745 | train_rmse: 1.03678 | train_mse: 1.07491 | valid_mae: 0.10481 | valid_rmse: 0.93014 | valid_mse: 0.86516 |  0:31:42s\n",
      "epoch 467| loss: 0.00912 | train_mae: 0.2068  | train_rmse: 5.38595 | train_mse: 29.00848| valid_mae: 0.25386 | valid_rmse: 6.10937 | valid_mse: 37.32441|  0:31:46s\n",
      "epoch 468| loss: 0.00854 | train_mae: 0.13394 | train_rmse: 2.84922 | train_mse: 8.11806 | valid_mae: 0.15565 | valid_rmse: 3.22102 | valid_mse: 10.37496|  0:31:50s\n",
      "epoch 469| loss: 0.00852 | train_mae: 0.11468 | train_rmse: 1.60016 | train_mse: 2.56051 | valid_mae: 0.13587 | valid_rmse: 1.97005 | valid_mse: 3.8811  |  0:31:54s\n",
      "epoch 470| loss: 0.00854 | train_mae: 0.08842 | train_rmse: 0.85377 | train_mse: 0.72893 | valid_mae: 0.08937 | valid_rmse: 0.84956 | valid_mse: 0.72176 |  0:31:58s\n",
      "epoch 471| loss: 0.00845 | train_mae: 0.14593 | train_rmse: 2.35532 | train_mse: 5.54755 | valid_mae: 0.18246 | valid_rmse: 3.15654 | valid_mse: 9.96377 |  0:32:02s\n",
      "epoch 472| loss: 0.00882 | train_mae: 0.21903 | train_rmse: 4.26465 | train_mse: 18.18722| valid_mae: 0.27731 | valid_rmse: 5.1994  | valid_mse: 27.03376|  0:32:06s\n",
      "epoch 473| loss: 0.00822 | train_mae: 0.23458 | train_rmse: 5.50523 | train_mse: 30.30759| valid_mae: 0.3245  | valid_rmse: 6.98679 | valid_mse: 48.81518|  0:32:10s\n",
      "epoch 474| loss: 0.00863 | train_mae: 0.23512 | train_rmse: 5.62254 | train_mse: 31.61295| valid_mae: 0.33815 | valid_rmse: 7.44731 | valid_mse: 55.46238|  0:32:15s\n",
      "epoch 475| loss: 0.00831 | train_mae: 0.22794 | train_rmse: 4.83652 | train_mse: 23.39194| valid_mae: 0.26414 | valid_rmse: 5.44287 | valid_mse: 29.62488|  0:32:19s\n",
      "epoch 476| loss: 0.00846 | train_mae: 0.13823 | train_rmse: 2.50842 | train_mse: 6.29218 | valid_mae: 0.1651  | valid_rmse: 2.98235 | valid_mse: 8.8944  |  0:32:24s\n",
      "epoch 477| loss: 0.00841 | train_mae: 0.16265 | train_rmse: 3.34854 | train_mse: 11.21274| valid_mae: 0.22558 | valid_rmse: 4.53262 | valid_mse: 20.5446 |  0:32:28s\n",
      "epoch 478| loss: 0.00858 | train_mae: 0.25097 | train_rmse: 6.17634 | train_mse: 38.14714| valid_mae: 0.295   | valid_rmse: 7.02483 | valid_mse: 49.34829|  0:32:32s\n",
      "epoch 479| loss: 0.00899 | train_mae: 0.1304  | train_rmse: 2.58741 | train_mse: 6.69471 | valid_mae: 0.13793 | valid_rmse: 2.73853 | valid_mse: 7.49955 |  0:32:36s\n",
      "epoch 480| loss: 0.00845 | train_mae: 0.11648 | train_rmse: 2.3739  | train_mse: 5.63542 | valid_mae: 0.12696 | valid_rmse: 2.58972 | valid_mse: 6.70663 |  0:32:40s\n",
      "epoch 481| loss: 0.00809 | train_mae: 0.09067 | train_rmse: 1.11038 | train_mse: 1.23293 | valid_mae: 0.09456 | valid_rmse: 1.20842 | valid_mse: 1.46028 |  0:32:44s\n",
      "epoch 482| loss: 0.00793 | train_mae: 0.17868 | train_rmse: 4.22354 | train_mse: 17.83833| valid_mae: 0.24579 | valid_rmse: 5.54072 | valid_mse: 30.69958|  0:32:48s\n",
      "epoch 483| loss: 0.00852 | train_mae: 0.13576 | train_rmse: 2.00327 | train_mse: 4.0131  | valid_mae: 0.14328 | valid_rmse: 2.08286 | valid_mse: 4.33829 |  0:32:52s\n",
      "epoch 484| loss: 0.00849 | train_mae: 0.10817 | train_rmse: 1.55625 | train_mse: 2.42192 | valid_mae: 0.12901 | valid_rmse: 1.97733 | valid_mse: 3.90985 |  0:32:57s\n",
      "epoch 485| loss: 0.00846 | train_mae: 0.16073 | train_rmse: 2.67397 | train_mse: 7.15012 | valid_mae: 0.2075  | valid_rmse: 3.47411 | valid_mse: 12.06943|  0:33:01s\n",
      "epoch 486| loss: 0.00869 | train_mae: 0.13806 | train_rmse: 2.38693 | train_mse: 5.69744 | valid_mae: 0.17994 | valid_rmse: 3.2795  | valid_mse: 10.75509|  0:33:05s\n",
      "epoch 487| loss: 0.00921 | train_mae: 0.09054 | train_rmse: 0.69954 | train_mse: 0.48935 | valid_mae: 0.10385 | valid_rmse: 0.90869 | valid_mse: 0.82572 |  0:33:09s\n",
      "epoch 488| loss: 0.00866 | train_mae: 0.14108 | train_rmse: 2.03537 | train_mse: 4.14272 | valid_mae: 0.17707 | valid_rmse: 2.60703 | valid_mse: 6.79658 |  0:33:13s\n",
      "epoch 489| loss: 0.00906 | train_mae: 0.09891 | train_rmse: 1.3249  | train_mse: 1.75536 | valid_mae: 0.10496 | valid_rmse: 1.491   | valid_mse: 2.22309 |  0:33:18s\n",
      "epoch 490| loss: 0.00842 | train_mae: 0.09902 | train_rmse: 0.56247 | train_mse: 0.31638 | valid_mae: 0.1007  | valid_rmse: 0.59892 | valid_mse: 0.35871 |  0:33:21s\n",
      "epoch 491| loss: 0.0087  | train_mae: 0.09687 | train_rmse: 1.05405 | train_mse: 1.11101 | valid_mae: 0.09741 | valid_rmse: 1.05277 | valid_mse: 1.10833 |  0:33:26s\n",
      "epoch 492| loss: 0.00886 | train_mae: 0.08918 | train_rmse: 0.1486  | train_mse: 0.02208 | valid_mae: 0.0907  | valid_rmse: 0.15243 | valid_mse: 0.02324 |  0:33:30s\n",
      "epoch 493| loss: 0.00898 | train_mae: 0.08886 | train_rmse: 0.65372 | train_mse: 0.42735 | valid_mae: 0.08939 | valid_rmse: 0.64616 | valid_mse: 0.41753 |  0:33:34s\n",
      "epoch 494| loss: 0.0083  | train_mae: 0.09012 | train_rmse: 0.78342 | train_mse: 0.61375 | valid_mae: 0.09291 | valid_rmse: 0.86172 | valid_mse: 0.74256 |  0:33:38s\n",
      "epoch 495| loss: 0.00827 | train_mae: 0.09379 | train_rmse: 1.11919 | train_mse: 1.25259 | valid_mae: 0.09996 | valid_rmse: 1.23041 | valid_mse: 1.5139  |  0:33:42s\n",
      "epoch 496| loss: 0.00808 | train_mae: 0.1004  | train_rmse: 0.41389 | train_mse: 0.17131 | valid_mae: 0.09947 | valid_rmse: 0.40322 | valid_mse: 0.16259 |  0:33:46s\n",
      "epoch 497| loss: 0.00862 | train_mae: 0.07773 | train_rmse: 0.1174  | train_mse: 0.01378 | valid_mae: 0.07863 | valid_rmse: 0.11878 | valid_mse: 0.01411 |  0:33:50s\n",
      "epoch 498| loss: 0.00866 | train_mae: 0.10473 | train_rmse: 1.05147 | train_mse: 1.10559 | valid_mae: 0.10398 | valid_rmse: 1.02596 | valid_mse: 1.05259 |  0:33:54s\n",
      "epoch 499| loss: 0.00822 | train_mae: 0.07144 | train_rmse: 0.11165 | train_mse: 0.01246 | valid_mae: 0.07194 | valid_rmse: 0.11317 | valid_mse: 0.01281 |  0:33:58s\n",
      "Stop training because you reached max_epochs = 500 with best_epoch = 367 and best_valid_mse = 0.01122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\buita\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 500   \n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train_normal,\n",
    "    eval_set=[(X_train, y_train_normal), (X_test, y_test_normal)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['mae', 'rmse', 'mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=500,\n",
    "    batch_size=512, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    loss_fn= nn.HuberLoss()\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at PyTorch_Model/TabNet_Huber_500Epoch.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PyTorch_Model/TabNet_Huber_500Epoch.zip'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.save_model(\"PyTorch_Model/TabNet_Huber_500Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network scores in actual distribution: r2 = 0.9861227142018549, mape = 0.03889483199309503, rmse = 5.7975093001371585\n"
     ]
    }
   ],
   "source": [
    "y_predNN_normal = clf.predict(X_test)\n",
    "\n",
    "y_predNN = quantile.inverse_transform(y_predNN_normal)\n",
    "mape = mean_absolute_percentage_error(y_test, y_predNN)\n",
    "r2 = r2_score(y_test, y_predNN)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_predNN))\n",
    "\n",
    "print(f\"Neural Network scores in actual distribution: r2 = {r2}, mape = {mape}, rmse = {rmse}\")   # Same as best weights from best epoch are automatically used!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuberLoss - 200 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train_normal,\n",
    "    eval_set=[(X_train, y_train_normal), (X_test, y_test_normal)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['mae', 'rmse', 'mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=200,\n",
    "    batch_size=512, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    loss_fn= nn.HuberLoss()\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.save_model(\"PyTorch_Model/TabNet_Huber_200Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\buita\\anaconda3\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:75: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "#### HOW TO LOAD SAVED MODEL#######\n",
    "\n",
    "clf2 = TabNetRegressor(cat_dims=[2], cat_emb_dim=[2], cat_idxs=[9])\n",
    "clf2.load_model(\"PyTorch_Model/TabNet_Huber_200Epoch.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network scores in normal distribution: r2 = 0.9885674225329073, mape = 0.48165472926467534\n",
      "Neural Network scores in actual distribution: r2 = 0.9861227142018549, mape = 0.03889483199309503, rmse = 5.7975093001371585\n"
     ]
    }
   ],
   "source": [
    "y_predNN_normal = clf.predict(X_test)\n",
    "\n",
    "mape = mean_absolute_percentage_error(y_test_normal, y_predNN_normal)\n",
    "r2 = r2_score(y_test_normal, y_predNN_normal)\n",
    "print(f\"Neural Network scores in normal distribution: r2 = {r2}, mape = {mape}\")\n",
    "\n",
    "\n",
    "y_predNN = quantile.inverse_transform(y_predNN_normal)\n",
    "mape = mean_absolute_percentage_error(y_test, y_predNN)\n",
    "r2 = r2_score(y_test, y_predNN)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_predNN))\n",
    "\n",
    "print(f\"Neural Network scores in actual distribution: r2 = {r2}, mape = {mape}, rmse = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predNN[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predNN_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Pressure\n",
    "print(\"Negative Pressure\")\n",
    "print(y_predNN[:,3])\n",
    "print()\n",
    "\n",
    "\n",
    "# Positive Impulse\n",
    "print(\"Positive Impulse\")\n",
    "print(y_predNN[:,5])\n",
    "print()\n",
    "\n",
    "# Positive Pressure\n",
    "print(\"Positive Pressure\")\n",
    "print(y_predNN[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
