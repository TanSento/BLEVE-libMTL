{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import timeit\n",
    "#import shap\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read-in and One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tank Failure Pressure (bar)</th>\n",
       "      <th>Liquid Ratio (%)</th>\n",
       "      <th>Tank Width (m)</th>\n",
       "      <th>Tank Length (m)</th>\n",
       "      <th>Tank Height (m)</th>\n",
       "      <th>Height of BLEVE (m)</th>\n",
       "      <th>Tank Height with Gas (m)</th>\n",
       "      <th>Vapour Temerature (K)</th>\n",
       "      <th>Liquid Temerature (K)</th>\n",
       "      <th>Status</th>\n",
       "      <th>Stand-off Distance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.010208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.012350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.014577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.016878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.019250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.021671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.024145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.026658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.029206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.031778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.034377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.037001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.039638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.042292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>Subcooled</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.044965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  Tank Failure Pressure (bar)  Liquid Ratio (%)  Tank Width (m)  \\\n",
       "0   B1                         24.5          0.519805             2.2   \n",
       "1   B1                         24.5          0.519805             2.2   \n",
       "2   B1                         24.5          0.519805             2.2   \n",
       "3   B1                         24.5          0.519805             2.2   \n",
       "4   B1                         24.5          0.519805             2.2   \n",
       "5   B1                         24.5          0.519805             2.2   \n",
       "6   B1                         24.5          0.519805             2.2   \n",
       "7   B1                         24.5          0.519805             2.2   \n",
       "8   B1                         24.5          0.519805             2.2   \n",
       "9   B1                         24.5          0.519805             2.2   \n",
       "10  B1                         24.5          0.519805             2.2   \n",
       "11  B1                         24.5          0.519805             2.2   \n",
       "12  B1                         24.5          0.519805             2.2   \n",
       "13  B1                         24.5          0.519805             2.2   \n",
       "14  B1                         24.5          0.519805             2.2   \n",
       "\n",
       "    Tank Length (m)  Tank Height (m)  Height of BLEVE (m)  \\\n",
       "0               6.0              1.0                  1.6   \n",
       "1               6.0              1.0                  1.6   \n",
       "2               6.0              1.0                  1.6   \n",
       "3               6.0              1.0                  1.6   \n",
       "4               6.0              1.0                  1.6   \n",
       "5               6.0              1.0                  1.6   \n",
       "6               6.0              1.0                  1.6   \n",
       "7               6.0              1.0                  1.6   \n",
       "8               6.0              1.0                  1.6   \n",
       "9               6.0              1.0                  1.6   \n",
       "10              6.0              1.0                  1.6   \n",
       "11              6.0              1.0                  1.6   \n",
       "12              6.0              1.0                  1.6   \n",
       "13              6.0              1.0                  1.6   \n",
       "14              6.0              1.0                  1.6   \n",
       "\n",
       "    Tank Height with Gas (m)   Vapour Temerature (K)   Liquid Temerature (K)  \\\n",
       "0                        0.4                   307.8                   339.0   \n",
       "1                        0.4                   307.8                   339.0   \n",
       "2                        0.4                   307.8                   339.0   \n",
       "3                        0.4                   307.8                   339.0   \n",
       "4                        0.4                   307.8                   339.0   \n",
       "5                        0.4                   307.8                   339.0   \n",
       "6                        0.4                   307.8                   339.0   \n",
       "7                        0.4                   307.8                   339.0   \n",
       "8                        0.4                   307.8                   339.0   \n",
       "9                        0.4                   307.8                   339.0   \n",
       "10                       0.4                   307.8                   339.0   \n",
       "11                       0.4                   307.8                   339.0   \n",
       "12                       0.4                   307.8                   339.0   \n",
       "13                       0.4                   307.8                   339.0   \n",
       "14                       0.4                   307.8                   339.0   \n",
       "\n",
       "       Status  Stand-off Distance    Target  \n",
       "0   Subcooled                 5.0  0.010208  \n",
       "1   Subcooled                 6.0  0.012350  \n",
       "2   Subcooled                 7.0  0.014577  \n",
       "3   Subcooled                 8.0  0.016878  \n",
       "4   Subcooled                 9.0  0.019250  \n",
       "5   Subcooled                10.0  0.021671  \n",
       "6   Subcooled                11.0  0.024145  \n",
       "7   Subcooled                12.0  0.026658  \n",
       "8   Subcooled                13.0  0.029206  \n",
       "9   Subcooled                14.0  0.031778  \n",
       "10  Subcooled                15.0  0.034377  \n",
       "11  Subcooled                16.0  0.037001  \n",
       "12  Subcooled                17.0  0.039638  \n",
       "13  Subcooled                18.0  0.042292  \n",
       "14  Subcooled                19.0  0.044965  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"data/arrival_time_COMPLETE.csv\")\n",
    "df2 = pd.read_csv(\"data/negative_duration_COMPLETE.csv\")\n",
    "df3 = pd.read_csv(\"data/negative_peak_time_COMPLETE.csv\")\n",
    "df4 = pd.read_csv(\"data/negative_pressure_COMPLETE.csv\")\n",
    "df5 = pd.read_csv(\"data/positive_duration_COMPLETE.csv\")\n",
    "df6 = pd.read_csv(\"data/positive_impulse_COMPLETE.csv\")\n",
    "df7 = pd.read_csv(\"data/positive_peak_time_COMPLETE.csv\")\n",
    "df8 = pd.read_csv(\"data/positive_pressure_COMPLETE.csv\")\n",
    "\n",
    "df1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tank Failure Pressure (bar)</th>\n",
       "      <th>Liquid Ratio (%)</th>\n",
       "      <th>Tank Width (m)</th>\n",
       "      <th>Tank Length (m)</th>\n",
       "      <th>Tank Height (m)</th>\n",
       "      <th>Height of BLEVE (m)</th>\n",
       "      <th>Tank Height with Gas (m)</th>\n",
       "      <th>Vapour Temerature (K)</th>\n",
       "      <th>Liquid Temerature (K)</th>\n",
       "      <th>Status</th>\n",
       "      <th>Stand-off Distance</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.006817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.007816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.008326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B1</td>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.008817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35995</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35996</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35997</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35998</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>P500</td>\n",
       "      <td>11.40239</td>\n",
       "      <td>0.442321</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>388.2</td>\n",
       "      <td>366.7</td>\n",
       "      <td>1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  Tank Failure Pressure (bar)  Liquid Ratio (%)  Tank Width (m)  \\\n",
       "0        B1                     24.50000          0.519805             2.2   \n",
       "1        B1                     24.50000          0.519805             2.2   \n",
       "2        B1                     24.50000          0.519805             2.2   \n",
       "3        B1                     24.50000          0.519805             2.2   \n",
       "4        B1                     24.50000          0.519805             2.2   \n",
       "...     ...                          ...               ...             ...   \n",
       "35995  P500                     11.40239          0.442321             1.4   \n",
       "35996  P500                     11.40239          0.442321             1.4   \n",
       "35997  P500                     11.40239          0.442321             1.4   \n",
       "35998  P500                     11.40239          0.442321             1.4   \n",
       "35999  P500                     11.40239          0.442321             1.4   \n",
       "\n",
       "       Tank Length (m)  Tank Height (m)  Height of BLEVE (m)  \\\n",
       "0                  6.0              1.0                  1.6   \n",
       "1                  6.0              1.0                  1.6   \n",
       "2                  6.0              1.0                  1.6   \n",
       "3                  6.0              1.0                  1.6   \n",
       "4                  6.0              1.0                  1.6   \n",
       "...                ...              ...                  ...   \n",
       "35995              2.8              1.2                  1.4   \n",
       "35996              2.8              1.2                  1.4   \n",
       "35997              2.8              1.2                  1.4   \n",
       "35998              2.8              1.2                  1.4   \n",
       "35999              2.8              1.2                  1.4   \n",
       "\n",
       "       Tank Height with Gas (m)   Vapour Temerature (K)  \\\n",
       "0                           0.4                   307.8   \n",
       "1                           0.4                   307.8   \n",
       "2                           0.4                   307.8   \n",
       "3                           0.4                   307.8   \n",
       "4                           0.4                   307.8   \n",
       "...                         ...                     ...   \n",
       "35995                       0.8                   388.2   \n",
       "35996                       0.8                   388.2   \n",
       "35997                       0.8                   388.2   \n",
       "35998                       0.8                   388.2   \n",
       "35999                       0.8                   388.2   \n",
       "\n",
       "        Liquid Temerature (K)  Status  Stand-off Distance    Target  \n",
       "0                       339.0       0                 5.0  0.006817  \n",
       "1                       339.0       0                 6.0  0.007302  \n",
       "2                       339.0       0                 7.0  0.007816  \n",
       "3                       339.0       0                 8.0  0.008326  \n",
       "4                       339.0       0                 9.0  0.008817  \n",
       "...                       ...     ...                 ...       ...  \n",
       "35995                   366.7       1                36.0       NaN  \n",
       "35996                   366.7       1                37.0       NaN  \n",
       "35997                   366.7       1                38.0       NaN  \n",
       "35998                   366.7       1                39.0       NaN  \n",
       "35999                   366.7       1                40.0       NaN  \n",
       "\n",
       "[36000 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding 'Status' feature into 0 and 1 \n",
    "# 0 for Subcooled and 1 for Superheated\n",
    "# Doing Similarly for ID (Do we need dummy encoding ??)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()\n",
    "\n",
    "df5['Status'] = LE.fit_transform(df5['Status'])\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tank Failure Pressure (bar)</th>\n",
       "      <th>Liquid Ratio (%)</th>\n",
       "      <th>Tank Width (m)</th>\n",
       "      <th>Tank Length (m)</th>\n",
       "      <th>Tank Height (m)</th>\n",
       "      <th>Height of BLEVE (m)</th>\n",
       "      <th>Tank Height with Gas (m)</th>\n",
       "      <th>Vapour Temerature (K)</th>\n",
       "      <th>Liquid Temerature (K)</th>\n",
       "      <th>Status</th>\n",
       "      <th>Stand-off Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.50000</td>\n",
       "      <td>0.519805</td>\n",
       "      <td>2.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>307.8</td>\n",
       "      <td>339.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28795</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28796</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28797</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28798</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28799</th>\n",
       "      <td>33.17377</td>\n",
       "      <td>0.372041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>312.7</td>\n",
       "      <td>318.2</td>\n",
       "      <td>0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28800 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Tank Failure Pressure (bar)  Liquid Ratio (%)  Tank Width (m)  \\\n",
       "0                         24.50000          0.519805             2.2   \n",
       "1                         24.50000          0.519805             2.2   \n",
       "2                         24.50000          0.519805             2.2   \n",
       "3                         24.50000          0.519805             2.2   \n",
       "4                         24.50000          0.519805             2.2   \n",
       "...                            ...               ...             ...   \n",
       "28795                     33.17377          0.372041             1.0   \n",
       "28796                     33.17377          0.372041             1.0   \n",
       "28797                     33.17377          0.372041             1.0   \n",
       "28798                     33.17377          0.372041             1.0   \n",
       "28799                     33.17377          0.372041             1.0   \n",
       "\n",
       "       Tank Length (m)  Tank Height (m)  Height of BLEVE (m)  \\\n",
       "0                  6.0              1.0                  1.6   \n",
       "1                  6.0              1.0                  1.6   \n",
       "2                  6.0              1.0                  1.6   \n",
       "3                  6.0              1.0                  1.6   \n",
       "4                  6.0              1.0                  1.6   \n",
       "...                ...              ...                  ...   \n",
       "28795              2.2              0.6                  0.2   \n",
       "28796              2.2              0.6                  0.2   \n",
       "28797              2.2              0.6                  0.2   \n",
       "28798              2.2              0.6                  0.2   \n",
       "28799              2.2              0.6                  0.2   \n",
       "\n",
       "       Tank Height with Gas (m)   Vapour Temerature (K)  \\\n",
       "0                           0.4                   307.8   \n",
       "1                           0.4                   307.8   \n",
       "2                           0.4                   307.8   \n",
       "3                           0.4                   307.8   \n",
       "4                           0.4                   307.8   \n",
       "...                         ...                     ...   \n",
       "28795                       0.4                   312.7   \n",
       "28796                       0.4                   312.7   \n",
       "28797                       0.4                   312.7   \n",
       "28798                       0.4                   312.7   \n",
       "28799                       0.4                   312.7   \n",
       "\n",
       "        Liquid Temerature (K)  Status  Stand-off Distance  \n",
       "0                       339.0       0                 5.0  \n",
       "1                       339.0       0                 6.0  \n",
       "2                       339.0       0                 7.0  \n",
       "3                       339.0       0                 8.0  \n",
       "4                       339.0       0                 9.0  \n",
       "...                       ...     ...                 ...  \n",
       "28795                   318.2       0                36.0  \n",
       "28796                   318.2       0                37.0  \n",
       "28797                   318.2       0                38.0  \n",
       "28798                   318.2       0                39.0  \n",
       "28799                   318.2       0                40.0  \n",
       "\n",
       "[28800 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df5.drop(['ID','Target'], axis=1)[:28800]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.006817\n",
       "1        0.007302\n",
       "2        0.007816\n",
       "3        0.008326\n",
       "4        0.008817\n",
       "           ...   \n",
       "28795    0.012178\n",
       "28796    0.012275\n",
       "28797    0.012374\n",
       "28798    0.012477\n",
       "28799    0.012573\n",
       "Name: Target, Length: 28800, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y5 = df5['Target'][:28800]\n",
    "y5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = df1['Target'][:28800]\n",
    "y2 = df2['Target'][:28800]\n",
    "y3 = df3['Target'][:28800]\n",
    "y4 = df4['Target'][:28800]\n",
    "y6 = df6['Target'][:28800]\n",
    "y7 = df7['Target'][:28800]\n",
    "y8 = df8['Target'][:28800]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Set Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 11)\n",
      "(7200, 11)\n"
     ]
    }
   ],
   "source": [
    "X_traindf, X_testdf, y1_train, y1_test = train_test_split(X, y1, test_size=0.25, random_state=42)\n",
    "print(X_traindf.shape)\n",
    "print(X_testdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_train, y2_test = train_test_split(y2, test_size=0.25, random_state=42)\n",
    "y3_train, y3_test = train_test_split(y3, test_size=0.25, random_state=42)\n",
    "y4_train, y4_test = train_test_split(y4, test_size=0.25, random_state=42)\n",
    "y5_train, y5_test = train_test_split(y5, test_size=0.25, random_state=42)\n",
    "y6_train, y6_test = train_test_split(y6, test_size=0.25, random_state=42)\n",
    "y7_train, y7_test = train_test_split(y7, test_size=0.25, random_state=42)\n",
    "y8_train, y8_test = train_test_split(y8, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01284534, 0.01444022, 0.10035855, ..., 0.10251021, 0.07990494,\n",
       "       0.01143898])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y7_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1268162 , 0.62633157, 0.04595783, ..., 0.02130297, 0.13602383,\n",
       "       2.3361142 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y8_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((y1_train.values.reshape(-1,1), y2_train.values.reshape(-1,1), y3_train.values.reshape(-1,1), \n",
    "                          y4_train.values.reshape(-1,1), y5_train.values.reshape(-1,1), y6_train.values.reshape(-1,1),\n",
    "                          y7_train.values.reshape(-1,1), y8_train.values.reshape(-1,1)), axis=1)\n",
    "\n",
    "y_test = np.concatenate((y1_test.values.reshape(-1,1), y2_test.values.reshape(-1,1), y3_test.values.reshape(-1,1), \n",
    "                          y4_test.values.reshape(-1,1), y5_test.values.reshape(-1,1), y6_test.values.reshape(-1,1),\n",
    "                          y7_test.values.reshape(-1,1), y8_test.values.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21600, 8)\n",
      "(7200, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.11694350e-02, 1.56393350e-02, 2.44314220e-02, ...,\n",
       "        2.86203890e+02, 1.28453400e-02, 1.12681620e+00],\n",
       "       [1.24119570e-02, 1.88029450e-02, 2.82499930e-02, ...,\n",
       "        1.80425900e+02, 1.44402250e-02, 6.26331570e-01],\n",
       "       [9.50448220e-02, 1.51018300e-02, 1.09446822e-01, ...,\n",
       "        1.49908640e+01, 1.00358550e-01, 4.59578340e-02],\n",
       "       ...,\n",
       "       [9.67233260e-02, 9.91223000e-03, 1.08929812e-01, ...,\n",
       "        7.44885020e+00, 1.02510210e-01, 2.13029660e-02],\n",
       "       [7.58939240e-02, 2.09730370e-02, 9.80206378e-02, ...,\n",
       "        6.19014550e+01, 7.99049360e-02, 1.36023830e-01],\n",
       "       [1.00712810e-02, 3.38855160e-02, 3.30022184e-02, ...,\n",
       "        7.96478820e+02, 1.14389770e-02, 2.33611420e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.36676340e-02, 1.85481800e-02, 1.16362342e-01, ...,\n",
       "        3.87540700e+01, 9.85093640e-02, 8.02679810e-02],\n",
       "       [1.88341180e-02, 1.49853250e-02, 3.32267860e-02, ...,\n",
       "        1.10768800e+02, 2.12428740e-02, 3.99188070e-01],\n",
       "       [4.82065450e-02, 2.26087420e-02, 7.29268908e-02, ...,\n",
       "        9.14076770e+01, 5.16740420e-02, 1.84897880e-01],\n",
       "       ...,\n",
       "       [7.48656170e-02, 1.68266440e-02, 9.27517236e-02, ...,\n",
       "        3.51032030e+01, 7.93257060e-02, 9.26706940e-02],\n",
       "       [1.48923780e-02, 2.82288120e-02, 3.50396958e-02, ...,\n",
       "        3.61880250e+02, 1.65773410e-02, 1.17031230e+00],\n",
       "       [4.29282640e-02, 7.86490400e-03, 5.24556696e-02, ...,\n",
       "        1.25165350e+01, 4.73894250e-02, 4.77122370e-02]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization and Power Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing both X_train and X_test using standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_traindf)\n",
    "X_test = scaler.transform(X_testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check if it succeeded\n",
    "# df_stdscal = pd.DataFrame(X_train)\n",
    "# df_stdscal.hist(figsize = (20,20), bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "y_train_normal = quantile.fit_transform(y_train)\n",
    "y_test_normal = quantile.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0197092 ,  0.33428349, -0.71517926,  2.24232066,  1.86359402,\n",
       "       -0.74523355, -0.4648274 , -0.2268882 , -0.75649043, -2.09079023,\n",
       "       -0.55463441,  0.75038709, -0.03604441,  0.25850177,  1.95536698,\n",
       "        0.14818542,  0.53428356, -0.76673673,  0.53672479, -0.23066606,\n",
       "       -0.14865688,  0.41007522,  1.43976542,  0.18550623, -0.37067758,\n",
       "       -0.82626052,  1.35012437, -1.40802512, -2.21909141, -0.44441688,\n",
       "       -0.75462019,  1.19340742,  0.80011561,  0.54253131,  0.33034992,\n",
       "        1.98874773,  1.62697118, -0.53849423,  0.35250048,  2.31339542])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_normal[1:41, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39918807, 0.18489788, 0.07076792, 1.603528  , 1.0806105 ,\n",
       "       0.06908134, 0.08818348, 0.10897039, 0.06826332, 0.01860524,\n",
       "       0.08134046, 0.28987974, 0.12975076, 0.17156978, 1.2115381 ,\n",
       "       0.15436813, 0.22705919, 0.06763056, 0.22765867, 0.10868868,\n",
       "       0.11713055, 0.19952142, 0.66621888, 0.15975925, 0.09596858,\n",
       "       0.06414993, 0.59720933, 0.03660972, 0.01633277, 0.08981635,\n",
       "       0.06839496, 0.49123496, 0.30692583, 0.2290165 , 0.18374079,\n",
       "       1.2546521 , 0.83790588, 0.08270955, 0.18826026, 1.7111793 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_check = quantile.inverse_transform(y_test_normal)\n",
    "y_test_check[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39918807, 0.18489788, 0.07076792, 1.603528  , 1.0806105 ,\n",
       "       0.06908134, 0.08818348, 0.10897039, 0.06826332, 0.01860524,\n",
       "       0.08134046, 0.28987974, 0.12975076, 0.17156978, 1.2115381 ,\n",
       "       0.15436813, 0.22705919, 0.06763056, 0.22765867, 0.10868868,\n",
       "       0.11713055, 0.19952142, 0.66621888, 0.15975925, 0.09596858,\n",
       "       0.06414993, 0.59720933, 0.03660972, 0.01633277, 0.08981635,\n",
       "       0.06839496, 0.49123496, 0.30692583, 0.2290165 , 0.18374079,\n",
       "       1.2546521 , 0.83790588, 0.08270955, 0.18826026, 1.7111793 ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check if it succeeded\n",
    "# df_stdscal = pd.DataFrame(y_train)\n",
    "# df_stdscal.hist(figsize = (20,20), bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.backends.cudnn.version() , torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1781, -0.9747, -1.5140,  ..., -0.7230, -0.7424, -1.4851],\n",
      "        [-0.5207,  0.5219,  0.0684,  ..., -0.3853,  1.3470, -1.4851],\n",
      "        [ 1.3911,  1.5275, -0.7228,  ...,  1.1182, -0.7424,  1.3998],\n",
      "        ...,\n",
      "        [-1.6514,  0.5764, -1.2503,  ...,  0.7199,  1.3470,  1.3998],\n",
      "        [ 0.9835,  0.0136, -0.7228,  ..., -0.4908, -0.7424,  0.9190],\n",
      "        [ 1.3715,  0.0745,  0.3321,  ...,  0.5537,  1.3470, -1.4851]])\n"
     ]
    }
   ],
   "source": [
    "# convert data to torch.FloatTensor\n",
    "X_train_torch = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test_torch = torch.from_numpy(X_test.astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "y_train_torch = torch.from_numpy(y_train_normal.astype(np.float32))\n",
    "y_test_torch = torch.from_numpy(y_test_normal.astype(np.float32))\n",
    "\n",
    "\n",
    "print(X_train_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7290, -0.1825, -1.7822,  ...,  1.7153, -1.7731,  1.8945],\n",
       "        [-1.5605,  0.3168, -1.4323,  ...,  1.2409, -1.5663,  1.3875],\n",
       "        [ 1.4714, -0.2850,  1.1074,  ..., -1.2251,  1.4889, -1.1817],\n",
       "        ...,\n",
       "        [ 1.5913, -1.5754,  1.0800,  ..., -1.9514,  1.6349, -1.9590],\n",
       "        [ 0.6395,  0.5629,  0.6927,  ...,  0.1485,  0.6236,  0.0151],\n",
       "        [-1.9274,  2.0287, -1.1576,  ...,  2.8862, -2.0276,  2.6981]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3816,  0.2827,  1.4829,  ..., -0.3075,  1.3684, -0.5699],\n",
       "        [-1.0928, -0.3107, -1.1451,  ...,  0.7363, -1.0969,  1.0197],\n",
       "        [-0.1113,  0.7614,  0.0336,  ...,  0.5406, -0.1180,  0.3343],\n",
       "        ...,\n",
       "        [ 0.6086,  0.0336,  0.5384,  ..., -0.4086,  0.6064, -0.4105],\n",
       "        [-1.3440,  1.4611, -1.0647,  ...,  1.9779, -1.3778,  1.9265],\n",
       "        [-0.2490, -2.1335, -0.4783,  ..., -1.3961, -0.2252, -1.1426]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i in range(len(X_train_torch)):\n",
    "   train_data.append([X_train_torch[i],\n",
    "                      y_train_torch[i] \n",
    "                     ])\n",
    "\n",
    "test_data = []\n",
    "for i in range(len(X_test_torch)):\n",
    "   test_data.append([X_test_torch[i], \n",
    "                     y_test_torch[i]\n",
    "                     ])\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=512,               # batch_size could be smaller\n",
    "    num_workers=0)                                                                   # Increasing num_workers slow down the training because it does not use GPU at all\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=512,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7290, -0.1825, -1.7822,  ...,  1.7153, -1.7731,  1.8945],\n",
      "        [-1.5605,  0.3168, -1.4323,  ...,  1.2409, -1.5663,  1.3875],\n",
      "        [ 1.4714, -0.2850,  1.1074,  ..., -1.2251,  1.4889, -1.1817],\n",
      "        ...,\n",
      "        [ 0.8358, -0.0945,  0.8934,  ..., -0.7314,  0.8505, -1.0485],\n",
      "        [-0.8206,  0.8322, -0.6906,  ...,  1.3263, -0.8403,  1.2101],\n",
      "        [ 0.2861,  1.0838,  0.4119,  ...,  0.6527,  0.2623,  0.4181]])\n",
      "tensor([[-0.5300, -2.0834, -0.7918,  ..., -0.8972, -0.5132, -0.4180],\n",
      "        [-0.9009, -1.6717, -1.1690,  ..., -0.1349, -0.8938,  0.5568],\n",
      "        [ 1.4466, -0.0538,  1.5691,  ..., -0.8411,  1.4696, -1.2969],\n",
      "        ...,\n",
      "        [ 1.3805,  0.5386,  1.6099,  ..., -0.4543,  1.3940, -0.9163],\n",
      "        [ 2.4348, -0.6237,  1.6922,  ..., -2.1980,  2.4790, -2.2859],\n",
      "        [-0.1717,  0.7099, -0.0818,  ...,  0.8580, -0.1884,  0.6773]])\n",
      "tensor([[ 0.2020, -0.7115,  0.1271,  ..., -0.5994,  0.2110, -0.5918],\n",
      "        [-0.0059, -0.2115, -0.0668,  ..., -0.0512, -0.0085,  0.0518],\n",
      "        [-1.1355, -0.5121, -1.2322,  ...,  0.3827, -1.1245,  0.7504],\n",
      "        ...,\n",
      "        [-0.9340, -0.2810, -1.0251,  ...,  1.0621, -0.9535,  1.3148],\n",
      "        [ 0.1063,  0.2683,  0.1440,  ..., -0.0209,  0.1085, -0.2021],\n",
      "        [-2.6043,  0.3627, -2.1220,  ...,  1.7895, -2.6160,  2.1416]])\n",
      "tensor([[-0.3388,  1.8408, -0.1090,  ...,  1.3557, -0.3607,  0.9889],\n",
      "        [ 1.3040,  1.9205,  3.1201,  ...,  0.7790,  1.2458,  0.1796],\n",
      "        [-0.6991,  1.7344, -0.4734,  ...,  1.8379, -0.7287,  1.5371],\n",
      "        ...,\n",
      "        [ 1.0664,  0.9639,  1.3039,  ...,  0.1256,  1.0458, -0.1783],\n",
      "        [-1.6152,  0.5406, -1.4470,  ...,  1.2873, -1.6243,  1.5087],\n",
      "        [-0.8205, -1.0797, -0.9529,  ..., -0.1304, -0.8038,  0.2629]])\n",
      "tensor([[-0.1871,  0.1163, -0.2030,  ...,  0.3484, -0.1953,  0.4197],\n",
      "        [-1.9282, -1.7085, -2.7503,  ..., -0.2147, -1.8141,  0.5657],\n",
      "        [ 1.6561, -1.7856,  1.1190,  ..., -2.4656,  1.7176, -2.5107],\n",
      "        ...,\n",
      "        [ 0.1240, -0.1066,  0.0865,  ..., -0.5805,  0.1322, -0.5656],\n",
      "        [-1.1004, -0.5606, -1.3509,  ..., -0.8110, -1.0557, -0.2400],\n",
      "        [-2.3294, -1.0607, -2.6494,  ...,  0.9331, -2.2621,  1.4495]])\n",
      "tensor([[-1.2725, -1.3560, -1.4715,  ..., -0.4136, -1.2265,  0.2324],\n",
      "        [ 0.2726, -0.1055,  0.2604,  ..., -0.3200,  0.2767, -0.4037],\n",
      "        [ 0.0784, -0.8434, -0.0943,  ..., -1.1949,  0.0973, -1.0764],\n",
      "        ...,\n",
      "        [ 1.2081,  0.9339,  1.4344,  ..., -0.0339,  1.1875, -0.3515],\n",
      "        [ 1.1289,  0.6457,  1.3153,  ..., -0.1511,  1.1182, -0.5080],\n",
      "        [-1.1617,  1.2852, -0.9418,  ...,  2.0244, -1.2031,  1.9376]])\n",
      "tensor([[ 1.3987, -0.1906,  1.2403,  ..., -0.6114,  1.3924, -0.7222],\n",
      "        [-1.6078, -1.1778, -2.0383,  ...,  0.6294, -1.5989,  1.2185],\n",
      "        [ 0.4551, -0.6526,  0.3639,  ..., -0.7559,  0.4651, -0.7714],\n",
      "        ...,\n",
      "        [-0.3549,  0.6266, -0.2641,  ...,  0.3552, -0.3575,  0.3595],\n",
      "        [ 0.6362, -0.3029,  0.6720,  ..., -0.6820,  0.6502, -0.9269],\n",
      "        [ 2.4475, -0.4124,  2.3141,  ..., -1.0381,  2.4183, -1.3041]])\n",
      "tensor([[ 2.9108, -0.6557,  1.9890,  ..., -1.8382,  2.9180, -1.9071],\n",
      "        [ 0.1885,  0.1365,  0.2005,  ..., -0.1354,  0.1922, -0.2667],\n",
      "        [ 0.1587, -1.0545,  0.0785,  ..., -0.7173,  0.1676, -0.5293],\n",
      "        ...,\n",
      "        [ 1.4645,  0.9325,  1.8647,  ...,  0.1921,  1.4132, -0.1190],\n",
      "        [ 1.2970, -1.3034,  0.9128,  ..., -1.5339,  1.3310, -1.4931],\n",
      "        [-2.5630,  0.0574, -2.1333,  ...,  2.0786, -2.5857,  2.3142]])\n",
      "tensor([[ 1.6956, -0.4829,  1.4646,  ..., -0.9308,  1.6994, -1.0523],\n",
      "        [-0.1497,  0.5208, -0.0723,  ...,  0.1572, -0.1504,  0.0776],\n",
      "        [ 0.8126,  0.7227,  0.9107,  ...,  0.1571,  0.7949, -0.0750],\n",
      "        ...,\n",
      "        [-1.4106,  0.5814, -1.2664,  ...,  1.4496, -1.4338,  1.5557],\n",
      "        [-0.4861,  0.0064, -0.5092,  ...,  0.5575, -0.4952,  0.6921],\n",
      "        [-0.0593,  1.3126,  0.1377,  ...,  0.7906, -0.0698,  0.4424]])\n",
      "tensor([[ 0.2499,  1.2286,  0.4382,  ...,  0.7003,  0.2346,  0.3597],\n",
      "        [ 0.0554,  0.3985,  0.0756,  ...,  0.2358,  0.0460,  0.2188],\n",
      "        [ 0.3039,  0.7025,  0.3991,  ...,  0.2370,  0.2965,  0.0300],\n",
      "        ...,\n",
      "        [ 2.4251, -0.3371,  1.7513,  ..., -1.8156,  2.4546, -1.9481],\n",
      "        [-1.1340, -1.8389, -1.4767,  ...,  0.0348, -1.1220,  0.7326],\n",
      "        [-0.3463,  0.8507, -0.2517,  ...,  0.7581, -0.3583,  0.6651]])\n",
      "tensor([[ 0.8210, -0.8412,  0.5857,  ..., -1.6604,  0.8473, -1.6533],\n",
      "        [-0.1879,  1.2487, -0.0198,  ...,  0.8996, -0.2003,  0.6589],\n",
      "        [-2.0655, -0.8292, -2.3056,  ...,  0.7633, -1.9969,  1.2946],\n",
      "        ...,\n",
      "        [-1.6783,  0.4750, -1.5093,  ...,  1.5783, -1.7044,  1.7084],\n",
      "        [-0.1056, -0.3117, -0.1796,  ..., -0.1739, -0.1070,  0.0093],\n",
      "        [ 0.1554, -0.1702,  0.2061,  ..., -0.4836,  0.1682, -0.7610]])\n",
      "tensor([[-0.3311,  0.0352, -0.3874,  ...,  0.1207, -0.3339,  0.3569],\n",
      "        [-0.7734, -0.4677, -0.8631,  ...,  0.1763, -0.7687,  0.4988],\n",
      "        [-0.3564, -0.3715, -0.4202,  ...,  0.1869, -0.3576,  0.3620],\n",
      "        ...,\n",
      "        [ 0.5621, -0.4899,  0.5723,  ..., -0.7196,  0.5749, -0.9512],\n",
      "        [ 1.2769,  2.4338,  3.3132,  ...,  0.7417,  1.2302,  0.0962],\n",
      "        [ 0.9675,  0.2587,  1.1096,  ..., -0.5231,  0.9795, -0.9729]])\n",
      "tensor([[-1.3526, -0.8438, -1.5422,  ...,  1.0014, -1.3545,  1.3262],\n",
      "        [-0.9778, -0.0806, -0.9928,  ...,  0.6604, -0.9795,  0.9031],\n",
      "        [ 1.4065, -1.0322,  1.0220,  ..., -2.4085,  1.4624, -2.4920],\n",
      "        ...,\n",
      "        [ 0.1681,  0.0612,  0.1415,  ...,  0.1231,  0.1593,  0.1570],\n",
      "        [ 1.3938,  1.6960,  2.3638,  ...,  0.4070,  1.3454, -0.0662],\n",
      "        [-0.0358, -0.6146, -0.1862,  ..., -0.9617, -0.0163, -0.8670]])\n",
      "tensor([[-0.2956,  0.5126, -0.2530,  ...,  0.6693, -0.3073,  0.6376],\n",
      "        [ 0.2885,  0.9838,  0.4047,  ...,  0.3309,  0.2779,  0.0893],\n",
      "        [-0.6847, -1.2670, -0.8223,  ..., -0.1359, -0.6732,  0.3087],\n",
      "        ...,\n",
      "        [ 1.1546,  1.3612,  1.7817,  ...,  0.2849,  1.1379, -0.2696],\n",
      "        [ 1.0155,  0.5902,  1.1100,  ...,  0.0091,  0.9966, -0.2061],\n",
      "        [ 0.7617,  0.2805,  0.7904,  ..., -0.2251,  0.7570, -0.4078]])\n",
      "tensor([[ 0.3645,  1.6741,  0.5935,  ...,  0.6998,  0.3454,  0.3029],\n",
      "        [-1.4216,  0.9653, -1.1827,  ...,  1.8926, -1.4608,  1.8802],\n",
      "        [-0.6218, -0.6597, -0.7169,  ...,  0.2009, -0.6208,  0.5263],\n",
      "        ...,\n",
      "        [ 0.9638,  0.1302,  1.0033,  ..., -0.3687,  0.9630, -0.6595],\n",
      "        [-1.0725,  0.4626, -0.9924,  ...,  1.1670, -1.0820,  1.2211],\n",
      "        [-1.3247, -1.8796, -1.7083,  ...,  0.5978, -1.3119,  1.1039]])\n",
      "tensor([[ 0.0109, -0.5515, -0.1451,  ..., -0.8986,  0.0241, -0.7559],\n",
      "        [-0.6323,  0.2752, -0.6247,  ...,  0.5706, -0.6372,  0.6941],\n",
      "        [-0.0937, -0.5692, -0.2472,  ..., -1.4528, -0.0692, -1.3582],\n",
      "        ...,\n",
      "        [-0.5550, -0.4353, -0.6315,  ...,  0.4876, -0.5656,  0.7338],\n",
      "        [ 0.1660,  0.5654,  0.2624,  ...,  0.1290,  0.1676, -0.1125],\n",
      "        [ 1.1155,  2.6541,  2.1414,  ...,  0.8451,  1.0611,  0.3034]])\n",
      "tensor([[-1.3536, -2.3127, -2.0706,  ..., -0.2477, -1.3160,  0.5329],\n",
      "        [-0.6874,  0.3041, -0.6077,  ...,  0.2375, -0.6780,  0.2910],\n",
      "        [-0.0970, -0.5311, -0.1746,  ..., -0.1782, -0.0989,  0.0345],\n",
      "        ...,\n",
      "        [ 1.0591,  0.2038,  1.0540,  ..., -0.4803,  1.0622, -0.6976],\n",
      "        [-0.7811,  0.1883, -0.7849,  ...,  0.9757, -0.7939,  1.0631],\n",
      "        [ 0.5676,  0.6880,  0.7632,  ...,  0.1852,  0.5619, -0.2042]])\n",
      "tensor([[-2.7449,  1.0714, -1.6729,  ...,  2.5175, -2.7883,  2.5617],\n",
      "        [-0.7358,  0.6982, -0.5924,  ...,  0.8409, -0.7414,  0.7893],\n",
      "        [ 2.2284, -1.8601,  1.3913,  ..., -1.8968,  2.2570, -1.9007],\n",
      "        ...,\n",
      "        [-1.0721, -0.2398, -1.0838,  ...,  0.2576, -1.0570,  0.6189],\n",
      "        [-0.0409, -1.4045, -0.1648,  ..., -0.7006, -0.0356, -0.3591],\n",
      "        [-0.5866, -0.4511, -0.7580,  ..., -1.1064, -0.5595, -0.7678]])\n",
      "tensor([[-1.8517, -1.5220, -2.3642,  ...,  1.1435, -1.8639,  1.6400],\n",
      "        [-0.4773, -0.8146, -0.5913,  ...,  0.0347, -0.4807,  0.4003],\n",
      "        [-0.0383,  0.6963,  0.0240,  ...,  0.6592, -0.0541,  0.5716],\n",
      "        ...,\n",
      "        [ 0.8727,  0.8063,  1.0690,  ...,  0.0034,  0.8672, -0.3919],\n",
      "        [-1.2411, -0.3331, -1.3086,  ...,  0.6920, -1.2432,  1.0278],\n",
      "        [-0.1524, -1.9586, -0.3770,  ..., -0.9631, -0.1365, -0.5815]])\n",
      "tensor([[-1.9343,  0.0310, -1.9219,  ...,  0.8769, -1.9070,  1.3280],\n",
      "        [ 0.6895, -0.0666,  0.6209,  ..., -0.5262,  0.6915, -0.6318],\n",
      "        [ 0.3113, -0.0434,  0.3006,  ..., -0.2133,  0.3138, -0.2939],\n",
      "        ...,\n",
      "        [-0.2140, -0.5177, -0.2119,  ..., -0.7401, -0.1942, -0.8144],\n",
      "        [-0.3057, -0.6042, -0.3980,  ..., -0.2793, -0.3018, -0.0045],\n",
      "        [ 0.3184, -1.0200,  0.2079,  ..., -0.8826,  0.3288, -0.8324]])\n",
      "tensor([[-1.6011, -1.0743, -2.0459,  ...,  1.1845, -1.6279,  1.6971],\n",
      "        [ 0.7208, -0.3472,  0.5241,  ..., -0.9735,  0.7260, -0.7814],\n",
      "        [-0.2238, -0.7610, -0.2214,  ..., -0.8552, -0.2003, -0.9458],\n",
      "        ...,\n",
      "        [ 0.5886, -0.2630,  0.5064,  ..., -0.5731,  0.5918, -0.6273],\n",
      "        [-1.4981, -0.5277, -1.9166,  ..., -0.5201, -1.4292,  0.2520],\n",
      "        [-0.8545,  0.1763, -0.8291,  ...,  0.7497, -0.8625,  0.9123]])\n",
      "tensor([[-0.0228, -0.0502,  0.0348,  ..., -0.6179, -0.0030, -0.9238],\n",
      "        [ 0.9924,  0.3997,  1.0289,  ..., -0.3290,  0.9839, -0.5059],\n",
      "        [ 1.2853,  0.2660,  1.2340,  ..., -0.5157,  1.2885, -0.7205],\n",
      "        ...,\n",
      "        [-1.9209, -0.4898, -2.0058,  ...,  1.1784, -1.9006,  1.4399],\n",
      "        [-1.0356, -1.4830, -1.2831,  ...,  0.3070, -1.0342,  0.8982],\n",
      "        [ 1.1225,  1.1753,  1.5593,  ...,  0.3579,  1.0869, -0.0088]])\n",
      "tensor([[ 0.1340,  1.0373,  0.2574,  ...,  0.5575,  0.1213,  0.3411],\n",
      "        [-0.7614,  3.0974, -0.3176,  ...,  2.3485, -0.7968,  1.7527],\n",
      "        [ 2.5236,  0.2355,  1.9583,  ..., -1.5412,  2.5268, -1.7183],\n",
      "        ...,\n",
      "        [-0.7123, -0.3394, -0.7584,  ..., -0.0350, -0.7026,  0.2960],\n",
      "        [-0.7385, -0.2835, -0.7326,  ...,  0.1151, -0.7303,  0.3781],\n",
      "        [-0.8367,  1.5030, -0.5805,  ...,  1.4370, -0.8522,  1.1938]])\n",
      "tensor([[ 0.0234, -1.4986, -0.1748,  ..., -2.8211,  0.0491, -2.7541],\n",
      "        [-0.5151, -0.1739, -0.4993,  ...,  0.0301, -0.5029,  0.0813],\n",
      "        [ 0.2624, -0.4660,  0.1022,  ..., -1.9270,  0.2889, -1.8378],\n",
      "        ...,\n",
      "        [-0.1180,  0.6770, -0.0487,  ...,  0.7993, -0.1330,  0.6738],\n",
      "        [-0.7001,  0.0368, -0.7140,  ...,  0.7437, -0.7092,  0.8648],\n",
      "        [-0.7296,  1.3291, -0.5445,  ...,  1.5523, -0.7542,  1.3461]])\n",
      "tensor([[ 1.2966, -0.4327,  0.9659,  ..., -1.2194,  1.3122, -1.1380],\n",
      "        [-0.5392,  0.6357, -0.4519,  ...,  0.7883, -0.5476,  0.7229],\n",
      "        [-0.8394, -0.7186, -0.9089,  ...,  0.0401, -0.8276,  0.4103],\n",
      "        ...,\n",
      "        [ 1.2299,  0.0705,  1.1343,  ..., -0.5635,  1.2614, -0.6939],\n",
      "        [-1.3373, -0.5387, -1.3918,  ..., -0.3802, -1.2787,  0.1003],\n",
      "        [-0.4754,  0.7026, -0.3592,  ...,  0.7078, -0.4837,  0.6531]])\n",
      "tensor([[-0.4533,  0.7089, -0.3542,  ...,  1.0188, -0.4684,  0.8601],\n",
      "        [ 0.6210,  0.3359,  0.4800,  ..., -0.9799,  0.6345, -0.9340],\n",
      "        [-1.9078,  1.2272, -1.4182,  ...,  2.6316, -2.0394,  2.8650],\n",
      "        ...,\n",
      "        [ 0.0984, -0.1531,  0.0377,  ..., -0.3018,  0.1020, -0.2506],\n",
      "        [-0.9077, -0.7722, -1.0331,  ...,  0.3385, -0.9059,  0.7163],\n",
      "        [ 1.9929,  0.4469,  2.4168,  ..., -0.2497,  1.9340, -0.5447]])\n",
      "tensor([[-0.6588,  0.3586, -0.6072,  ...,  0.6019, -0.6587,  0.6200],\n",
      "        [-0.9232, -1.0259, -1.0210,  ..., -0.5158, -0.8933, -0.0904],\n",
      "        [ 0.4760,  1.9392,  0.8902,  ...,  1.0221,  0.4517,  0.4648],\n",
      "        ...,\n",
      "        [ 0.7508, -0.8698,  0.6291,  ..., -0.9854,  0.7667, -1.0532],\n",
      "        [ 0.8975,  0.2540,  0.8455,  ..., -0.2783,  0.8838, -0.3669],\n",
      "        [-1.3066,  0.1631, -1.3178,  ...,  1.0679, -1.3182,  1.3275]])\n",
      "tensor([[-1.1455,  2.5219, -0.7272,  ...,  2.2976, -1.1905,  2.0308],\n",
      "        [ 1.5139, -0.1603,  1.1722,  ..., -1.4231,  1.5599, -1.6342],\n",
      "        [-0.3655,  0.4244, -0.3028,  ...,  0.5995, -0.3748,  0.5775],\n",
      "        ...,\n",
      "        [-1.1985, -1.1113, -1.3541,  ...,  0.2917, -1.1777,  0.7361],\n",
      "        [ 0.7978,  0.5354,  0.8492,  ...,  0.1760,  0.7736,  0.0314],\n",
      "        [-0.0095,  1.7559,  0.2022,  ...,  1.0479, -0.0313,  0.7121]])\n",
      "tensor([[ 1.2398,  1.4567,  1.8545,  ...,  0.5062,  1.1894,  0.1060],\n",
      "        [-0.3397, -0.5375, -0.3296,  ..., -0.3499, -0.3271, -0.3199],\n",
      "        [ 0.3544, -1.4756,  0.1293,  ..., -1.1578,  0.3634, -0.9354],\n",
      "        ...,\n",
      "        [ 0.0645,  0.6035,  0.1616,  ...,  0.2600,  0.0579,  0.0604],\n",
      "        [-1.0365,  0.1375, -1.0373,  ...,  1.0754, -1.0503,  1.2267],\n",
      "        [ 1.9399, -0.2856,  1.3993,  ..., -1.2783,  1.9348, -1.2506]])\n",
      "tensor([[-0.3776,  1.8972, -0.1188,  ...,  1.4409, -0.3984,  1.0265],\n",
      "        [ 0.5235,  0.2052,  0.5647,  ..., -0.4502,  0.5271, -0.6387],\n",
      "        [ 2.2399, -1.4905,  1.4250,  ..., -1.7124,  2.2563, -1.6925],\n",
      "        ...,\n",
      "        [ 0.4465, -0.2122,  0.4591,  ..., -0.5625,  0.4559, -0.7951],\n",
      "        [ 1.3515, -0.3922,  1.0018,  ..., -1.0877,  1.3511, -0.9489],\n",
      "        [-1.9645,  0.5690, -1.6747,  ...,  1.7252, -2.0140,  1.9365]])\n",
      "tensor([[-1.2568,  0.9292, -1.0722,  ...,  1.8521, -1.2925,  1.8287],\n",
      "        [-0.0837, -1.3824, -0.1914,  ..., -0.8636, -0.0679, -0.7247],\n",
      "        [ 0.1163, -1.6361, -0.0875,  ..., -1.5605,  0.1400, -1.4112],\n",
      "        ...,\n",
      "        [ 0.7445,  0.9029,  0.8845,  ...,  0.1472,  0.7316, -0.0863],\n",
      "        [ 0.2592,  1.3346,  0.4509,  ...,  0.6333,  0.2426,  0.3510],\n",
      "        [ 0.9952,  1.1861,  1.3537,  ...,  0.5582,  0.9552,  0.1854]])\n",
      "tensor([[-1.1037,  2.3122, -0.6687,  ...,  2.1339, -1.1344,  1.7435],\n",
      "        [-1.0701,  0.1375, -1.0019,  ..., -0.1202, -1.0372,  0.1925],\n",
      "        [-0.1166, -0.5381, -0.2060,  ...,  0.0236, -0.1229,  0.3116],\n",
      "        ...,\n",
      "        [ 1.2650,  0.7816,  1.6290,  ...,  0.1509,  1.2365, -0.2484],\n",
      "        [-2.0744,  0.3092, -1.7118,  ...,  1.0170, -1.9608,  1.2683],\n",
      "        [-0.7960, -2.2102, -1.1113,  ..., -2.4022, -0.7546, -2.0563]])\n",
      "tensor([[ 0.9125, -1.0888,  0.6487,  ..., -2.6770,  0.9461, -2.6940],\n",
      "        [-0.4845, -0.6106, -0.5481,  ..., -0.4050, -0.4691, -0.2535],\n",
      "        [ 0.0830,  0.7796,  0.2321,  ...,  0.2735,  0.0791, -0.0334],\n",
      "        ...,\n",
      "        [ 0.3656,  1.3322,  0.5781,  ...,  0.5506,  0.3508,  0.1735],\n",
      "        [-0.2872, -1.4899, -0.3979,  ..., -0.9053, -0.2647, -0.7499],\n",
      "        [ 0.9368,  0.3363,  1.0423,  ..., -0.1402,  0.9327, -0.4787]])\n",
      "tensor([[ 0.5664, -0.1955,  0.5055,  ..., -0.6010,  0.5725, -0.7416],\n",
      "        [-0.0591, -1.1481, -0.1753,  ..., -0.3044, -0.0618,  0.0561],\n",
      "        [-0.3047, -0.1803, -0.3050,  ...,  0.0794, -0.3019,  0.0889],\n",
      "        ...,\n",
      "        [-0.5771, -0.5558, -0.7503,  ..., -0.8883, -0.5506, -0.5896],\n",
      "        [ 0.1995,  1.0592,  0.3328,  ...,  0.6990,  0.1807,  0.4536],\n",
      "        [-0.8628,  1.6318, -0.6090,  ...,  1.6820, -0.8879,  1.4327]])\n",
      "tensor([[ 0.8998,  0.2506,  0.8363,  ..., -0.5348,  0.8962, -0.5768],\n",
      "        [ 1.3773,  1.3054,  2.2117,  ...,  0.3177,  1.3346, -0.1334],\n",
      "        [-0.5534, -0.9646, -0.7432,  ..., -0.9317, -0.5337, -0.5247],\n",
      "        ...,\n",
      "        [-1.3392,  1.7097, -1.0125,  ...,  2.1613, -1.3771,  2.0542],\n",
      "        [ 0.6205, -0.0532,  0.5608,  ..., -0.2318,  0.6152, -0.2900],\n",
      "        [-2.8540, -1.3572, -3.2568,  ...,  1.2722, -2.8256,  1.8692]])\n",
      "tensor([[-0.8764,  0.4935, -0.7919,  ...,  1.2241, -0.8902,  1.1860],\n",
      "        [ 0.2573,  0.0419,  0.2198,  ..., -0.5134,  0.2641, -0.5961],\n",
      "        [-0.0150, -1.0052, -0.1912,  ..., -1.1593,  0.0020, -0.9339],\n",
      "        ...,\n",
      "        [ 0.0489, -0.8514, -0.1199,  ..., -0.9514,  0.0574, -0.6525],\n",
      "        [-0.2718,  0.5988, -0.1724,  ...,  0.5222, -0.2743,  0.3662],\n",
      "        [-1.2116,  1.4211, -0.9190,  ...,  1.9372, -1.2408,  1.7365]])\n",
      "tensor([[-1.3093, -0.4090, -1.4663,  ...,  1.4815, -1.3480,  1.7895],\n",
      "        [ 0.4268,  0.6519,  0.4957,  ...,  0.3520,  0.4104,  0.1859],\n",
      "        [ 0.2061, -0.9045,  0.0264,  ..., -1.0531,  0.2179, -0.8175],\n",
      "        ...,\n",
      "        [ 1.4597, -2.3226,  0.9530,  ..., -2.2896,  1.5098, -2.2860],\n",
      "        [-0.6347, -1.5841, -0.8085,  ...,  0.0487, -0.6353,  0.5653],\n",
      "        [ 1.2377, -0.3113,  1.3093,  ..., -0.6831,  1.2499, -1.0037]])\n",
      "tensor([[ 1.7941, -1.2752,  1.2149,  ..., -1.6552,  1.8181, -1.6271],\n",
      "        [ 0.8221, -0.1986,  0.7246,  ..., -0.6402,  0.8259, -0.6652],\n",
      "        [ 0.1412,  0.7628,  0.2843,  ...,  0.3068,  0.1378, -0.0271],\n",
      "        ...,\n",
      "        [ 0.3226, -1.9781,  0.0926,  ..., -1.3986,  0.3391, -1.2430],\n",
      "        [ 0.4296, -0.9098,  0.2255,  ..., -1.0690,  0.4376, -0.8466],\n",
      "        [ 0.5813, -0.1950,  0.5029,  ..., -0.5004,  0.5831, -0.5570]])\n",
      "tensor([[ 0.5487, -2.4819,  0.2845,  ..., -2.0599,  0.5756, -1.9891],\n",
      "        [ 1.6829,  0.6002,  2.0312,  ..., -0.4140,  1.6756, -0.8192],\n",
      "        [-0.5033,  2.4722, -0.1735,  ...,  1.4921, -0.5228,  1.0430],\n",
      "        ...,\n",
      "        [-0.6985,  1.0842, -0.5634,  ...,  1.3962, -0.7203,  1.2662],\n",
      "        [ 0.8270, -0.1216,  0.7350,  ..., -0.6816,  0.8357, -0.8037],\n",
      "        [-1.2039, -1.3831, -1.4681,  ...,  0.2342, -1.1931,  0.8187]])\n",
      "tensor([[ 0.5172,  0.4825,  0.5659,  ..., -0.1118,  0.5136, -0.3302],\n",
      "        [-1.4118,  0.4935, -1.2884,  ...,  1.2864, -1.4243,  1.4130],\n",
      "        [-1.7290,  0.6060, -1.5117,  ...,  1.2689, -1.7532,  1.5413],\n",
      "        ...,\n",
      "        [-1.0737, -0.2738, -1.1690,  ...,  1.1212, -1.0937,  1.3801],\n",
      "        [-1.4428, -1.8967, -2.1150,  ..., -0.8998, -1.3694, -0.2367],\n",
      "        [-2.7787,  0.1651, -2.2327,  ...,  1.9535, -2.7555,  2.1851]])\n",
      "tensor([[-1.3787, -1.4015, -1.7786,  ...,  0.5996, -1.3804,  1.1872],\n",
      "        [-1.6492,  1.5471, -1.1289,  ...,  1.8873, -1.6714,  1.7568],\n",
      "        [ 0.7320,  1.8953,  1.1017,  ...,  0.7631,  0.7024,  0.3117],\n",
      "        ...,\n",
      "        [-0.4444, -0.3008, -0.4911,  ...,  0.0877, -0.4443,  0.3666],\n",
      "        [ 2.0255, -0.0047,  1.8959,  ..., -0.7214,  2.0004, -0.9669],\n",
      "        [-0.9216, -1.7003, -1.1419,  ...,  0.2963, -0.9207,  0.8213]])\n",
      "tensor([[ 0.7164,  1.4127,  1.0518,  ...,  0.6468,  0.6891,  0.1912],\n",
      "        [-0.5452, -0.4068, -0.5827,  ...,  0.0358, -0.5423,  0.2622],\n",
      "        [-0.3901,  1.3668, -0.1542,  ...,  1.0264, -0.4009,  0.7055],\n",
      "        ...,\n",
      "        [-1.0855,  2.0898, -0.6992,  ...,  2.5755, -1.1208,  1.7760],\n",
      "        [ 1.7430, -0.0639,  1.6182,  ..., -0.7429,  1.7413, -0.9866],\n",
      "        [-0.1266, -2.0464, -0.3607,  ..., -0.8513, -0.1182, -0.4176]])\n",
      "tensor([[ 4.5677e-01, -6.9398e-01,  2.5792e-01,  8.0979e-01, -1.1577e+00,\n",
      "         -8.9502e-01,  4.6070e-01, -6.0003e-01],\n",
      "        [ 5.2010e-01,  3.5702e-02,  4.5809e-01,  5.3690e-01, -2.1315e-02,\n",
      "         -2.8098e-01,  5.1039e-01, -2.3048e-01],\n",
      "        [-3.1671e-01, -1.0489e+00, -4.2814e-01,  1.8315e-02, -4.2988e-01,\n",
      "         -3.1246e-01, -3.1237e-01, -4.0536e-02],\n",
      "        [-6.6989e-03,  2.7926e-01,  3.4704e-02, -3.5242e-01,  5.6999e-01,\n",
      "          1.4662e-01, -1.1299e-02,  9.3962e-02],\n",
      "        [ 3.0266e-01,  1.3209e+00,  5.2324e-01, -1.8917e-01,  1.6419e+00,\n",
      "          6.3988e-01,  2.8538e-01,  3.0161e-01],\n",
      "        [ 1.0967e+00,  1.3132e+00,  1.4393e+00,  2.3541e-01,  1.4193e+00,\n",
      "          3.3972e-01,  1.0606e+00, -8.6063e-03],\n",
      "        [ 1.4151e+00, -1.2904e+00,  1.0057e+00,  2.4269e+00, -6.1804e-01,\n",
      "         -2.2916e+00,  1.4670e+00, -2.3415e+00],\n",
      "        [-7.5863e-01,  7.4229e-01, -6.6182e-01, -1.0842e+00, -7.1677e-02,\n",
      "          1.2046e+00, -7.7546e-01,  1.1430e+00],\n",
      "        [ 1.0832e+00,  1.0059e+00,  1.3334e+00, -4.5899e-03,  1.3012e+00,\n",
      "          3.8832e-01,  1.0474e+00,  7.5263e-02],\n",
      "        [ 9.9803e-01,  1.1793e+00,  1.3653e+00,  3.9332e-01,  1.8998e+00,\n",
      "          3.3666e-01,  9.7631e-01, -1.3315e-01],\n",
      "        [ 4.7782e-01,  2.5527e-01,  4.7854e-01, -2.9001e-03,  4.5866e-01,\n",
      "          1.5188e-02,  4.6952e-01, -4.8617e-02],\n",
      "        [-9.4761e-01,  3.0227e-01, -9.0911e-01, -1.0402e+00, -3.8079e-01,\n",
      "          8.2134e-01, -9.5359e-01,  9.7767e-01],\n",
      "        [ 5.7313e-01,  1.3354e-01,  5.8162e-01,  3.4236e-01,  6.5250e-01,\n",
      "         -2.7462e-01,  5.6993e-01, -4.2213e-01],\n",
      "        [-4.6885e-01,  1.5412e+00, -2.5681e-01, -8.6594e-01,  8.3650e-01,\n",
      "          1.3328e+00, -4.8769e-01,  1.0134e+00],\n",
      "        [ 9.0237e-03,  1.4182e-01,  8.0167e-02,  9.7758e-02,  1.0044e+00,\n",
      "         -1.5446e-01,  1.2997e-02, -2.6410e-01],\n",
      "        [ 1.4736e+00,  1.7490e+00,  2.6832e+00,  5.3921e-01,  2.1248e+00,\n",
      "          4.4374e-01,  1.4281e+00, -1.0633e-01],\n",
      "        [ 2.0386e-01, -9.7458e-01,  1.5794e-02,  9.5013e-01, -1.2632e+00,\n",
      "         -1.0699e+00,  2.1469e-01, -8.1701e-01],\n",
      "        [-1.5466e+00, -1.5345e+00, -1.9479e+00, -7.5007e-01, -1.5480e+00,\n",
      "          8.7531e-02, -1.5034e+00,  7.2885e-01],\n",
      "        [ 1.6512e+00,  7.8727e-01,  2.0744e+00,  7.7689e-01,  1.1446e+00,\n",
      "         -1.2837e-01,  1.6150e+00, -4.5754e-01],\n",
      "        [-3.4816e-02,  1.1704e+00,  1.2115e-01, -5.5124e-01,  9.7791e-01,\n",
      "          1.0227e+00, -5.4877e-02,  7.3635e-01],\n",
      "        [ 1.3799e+00, -3.9568e-01,  1.2046e+00,  9.8535e-01,  2.7280e-01,\n",
      "         -7.4596e-01,  1.3749e+00, -8.0333e-01],\n",
      "        [ 8.7009e-02, -1.4986e-01,  1.2416e-01,  9.6755e-02,  8.7168e-01,\n",
      "         -4.2329e-01,  9.6297e-02, -5.2436e-01],\n",
      "        [-1.3656e+00, -2.0790e+00, -2.0365e+00, -3.2605e-01, -2.6518e+00,\n",
      "         -3.9647e-01, -1.3293e+00,  3.7490e-01],\n",
      "        [ 4.4367e-01,  4.5287e-01,  5.0395e-01,  1.7611e-01,  8.3896e-01,\n",
      "         -1.2117e-01,  4.4156e-01, -3.7620e-01],\n",
      "        [ 3.6375e-01,  1.1233e+00,  5.5642e-01, -1.4983e-01,  1.5507e+00,\n",
      "          5.1547e-01,  3.4861e-01,  1.6015e-01],\n",
      "        [ 4.5886e-02,  4.7462e-01,  7.9283e-02, -3.9450e-01,  3.4825e-01,\n",
      "          4.4693e-01,  3.3865e-02,  4.0023e-01],\n",
      "        [ 5.6200e-01,  1.0938e+00,  7.5191e-01,  2.6404e-02,  1.3637e+00,\n",
      "          3.5628e-01,  5.4634e-01,  5.7363e-02],\n",
      "        [-4.3040e-01, -9.1352e-01, -5.3999e-01, -2.5547e-01, -5.1382e-01,\n",
      "         -3.9780e-02, -4.3010e-01,  3.0501e-01],\n",
      "        [-3.4716e-02,  1.0594e+00,  1.2641e-01, -3.1754e-01,  1.1405e+00,\n",
      "          5.3803e-01, -4.0233e-02,  2.2585e-01],\n",
      "        [-4.5352e-01,  3.0182e-02, -4.5563e-01, -4.3337e-01, -4.7998e-02,\n",
      "          2.4896e-01, -4.5417e-01,  3.3881e-01],\n",
      "        [ 1.2962e-01,  1.3025e-02,  1.2445e-01,  1.1503e-01,  3.8744e-01,\n",
      "         -2.0211e-01,  1.3276e-01, -2.8748e-01],\n",
      "        [ 6.4783e-01, -1.0898e+00,  5.2524e-01,  1.2556e+00,  5.2306e-02,\n",
      "         -1.1319e+00,  6.6314e-01, -1.1461e+00],\n",
      "        [ 3.3937e-01, -1.4313e+00,  1.3332e-01,  2.1497e+00, -1.0301e+00,\n",
      "         -2.1562e+00,  3.6806e-01, -2.0911e+00],\n",
      "        [ 9.5448e-01,  2.7734e-02,  8.7355e-01,  7.2505e-01,  1.7125e-01,\n",
      "         -5.1438e-01,  9.5140e-01, -6.4574e-01],\n",
      "        [-1.7688e+00, -2.3093e+00, -2.8831e+00, -4.9999e-01, -3.1940e+00,\n",
      "         -3.6694e-01, -1.6693e+00,  4.8479e-01],\n",
      "        [-7.2875e-01,  9.4665e-01, -6.0031e-01, -9.3896e-01,  5.6507e-02,\n",
      "          1.1845e+00, -7.4491e-01,  1.0949e+00],\n",
      "        [-7.5016e-01,  1.3901e+00, -5.3725e-01, -1.2284e+00,  4.1797e-01,\n",
      "          1.5036e+00, -7.7043e-01,  1.2360e+00],\n",
      "        [ 7.4391e-01,  1.0543e+00,  9.1883e-01,  1.8293e-01,  1.1681e+00,\n",
      "          2.7877e-01,  7.2786e-01, -1.2398e-02],\n",
      "        [-6.5888e-01, -1.8022e+00, -9.2036e-01,  2.3903e-01, -1.9823e+00,\n",
      "         -8.4573e-01, -6.3654e-01, -3.1762e-01],\n",
      "        [-7.4801e-01,  2.8778e-01, -7.2587e-01, -8.2124e-01, -2.6759e-01,\n",
      "          7.5646e-01, -7.5512e-01,  8.4164e-01],\n",
      "        [-8.8858e-01,  1.4873e+00, -6.4510e-01, -1.2047e+00,  3.0641e-01,\n",
      "          1.4004e+00, -9.0459e-01,  1.2410e+00],\n",
      "        [ 1.2841e+00,  4.1371e-01,  1.3366e+00,  4.4886e-01,  7.9293e-01,\n",
      "         -1.9919e-01,  1.2625e+00, -3.5229e-01],\n",
      "        [ 8.3152e-01,  6.8821e-01,  9.1449e-01,  2.2631e-01,  8.0631e-01,\n",
      "          1.2888e-01,  8.1266e-01, -6.4555e-02],\n",
      "        [ 1.5376e+00, -5.1397e-01,  1.1215e+00,  1.3568e+00, -7.3975e-01,\n",
      "         -1.2872e+00,  1.5494e+00, -1.2314e+00],\n",
      "        [ 1.2687e-01, -1.3802e+00, -7.5602e-02,  1.0280e+00, -1.3418e+00,\n",
      "         -1.1710e+00,  1.4060e-01, -9.3355e-01],\n",
      "        [-1.3577e+00, -1.0402e+00, -1.5990e+00, -8.3988e-01, -1.3741e+00,\n",
      "          2.6816e-01, -1.3365e+00,  7.9551e-01],\n",
      "        [ 5.4419e-01,  1.6842e+00,  8.1125e-01, -2.3051e-01,  1.5367e+00,\n",
      "          8.3886e-01,  5.1722e-01,  4.5769e-01],\n",
      "        [ 1.6297e-01, -1.3015e+00, -2.5531e-02,  1.4221e+00, -1.1259e+00,\n",
      "         -1.5112e+00,  1.8627e-01, -1.3853e+00],\n",
      "        [ 2.5602e-01, -9.2690e-02,  2.2313e-01,  5.2400e-01,  2.2079e-01,\n",
      "         -5.3995e-01,  2.6530e-01, -6.6651e-01],\n",
      "        [-7.0450e-01, -1.3581e+00, -9.4654e-01,  3.8938e-02, -1.9499e+00,\n",
      "         -5.5023e-01, -6.9025e-01,  5.0073e-02],\n",
      "        [ 1.7671e+00, -1.0284e-01,  1.5741e+00,  9.6764e-01,  3.1075e-01,\n",
      "         -7.4658e-01,  1.7518e+00, -9.2995e-01],\n",
      "        [ 5.8782e-01,  1.4324e+00,  8.4226e-01,  1.3505e-01,  1.6130e+00,\n",
      "          4.7678e-01,  5.7006e-01,  2.8408e-02],\n",
      "        [ 1.0448e-01, -1.0563e+00,  9.6416e-03,  7.7252e-01, -1.3609e-01,\n",
      "         -8.3911e-01,  1.1935e-01, -8.5112e-01],\n",
      "        [ 8.2317e-02, -7.8980e-01, -9.6796e-02,  1.0596e+00, -1.2749e+00,\n",
      "         -1.2489e+00,  9.9516e-02, -1.0499e+00],\n",
      "        [-1.6599e+00, -1.9897e-01, -1.7877e+00, -2.1214e+00, -1.5078e+00,\n",
      "          1.7494e+00, -1.7209e+00,  2.0341e+00],\n",
      "        [-5.6749e-01,  3.3529e-01, -5.3448e-01, -6.1577e-01, -9.9810e-02,\n",
      "          2.6760e-01, -5.6703e-01,  4.6780e-01],\n",
      "        [ 8.3826e-01,  9.2891e-01,  9.8851e-01,  4.1843e-01,  1.0226e+00,\n",
      "          1.1857e-01,  8.2549e-01, -1.2651e-01],\n",
      "        [ 6.7524e-01,  2.0787e-01,  6.5759e-01,  5.3522e-01,  4.4543e-01,\n",
      "         -1.6354e-01,  6.6444e-01, -2.4737e-01],\n",
      "        [-4.5570e-01, -1.1508e-01, -4.4907e-01, -4.2626e-01,  1.3027e-01,\n",
      "          1.7876e-01, -4.5448e-01,  2.4068e-01],\n",
      "        [ 1.9508e+00,  1.7616e-01,  1.8389e+00,  5.9048e-01,  3.7272e-01,\n",
      "         -4.4024e-01,  1.8824e+00, -6.0486e-01],\n",
      "        [-9.8918e-01,  1.0097e-01, -9.0390e-01, -5.5705e-01, -1.5766e-02,\n",
      "          1.2081e-01, -9.6811e-01,  3.7636e-01],\n",
      "        [-1.0123e+00,  6.7503e-01, -8.9640e-01, -1.3071e+00, -2.7949e-01,\n",
      "          1.4050e+00, -1.0317e+00,  1.3762e+00],\n",
      "        [ 5.1369e-01,  4.5627e-01,  5.4014e-01, -1.7272e-01,  5.2489e-01,\n",
      "          1.9949e-01,  4.9398e-01,  1.2281e-01],\n",
      "        [ 5.5838e-01,  8.8979e-01,  6.9289e-01,  2.6377e-01,  1.0323e+00,\n",
      "          5.1335e-02,  5.5438e-01, -2.5293e-01],\n",
      "        [-3.9881e-01,  6.1481e-01, -3.0639e-01, -6.5402e-01,  5.3099e-01,\n",
      "          6.5265e-01, -4.0376e-01,  5.5602e-01],\n",
      "        [ 5.3711e-01, -2.7353e-02,  5.2605e-01,  2.1365e-01,  5.2247e-01,\n",
      "         -3.1318e-01,  5.3930e-01, -4.3523e-01],\n",
      "        [-1.7713e+00,  5.3683e-01, -1.4649e+00, -1.2228e+00, -5.7902e-01,\n",
      "          9.2380e-01, -1.7484e+00,  1.2016e+00],\n",
      "        [-9.3991e-01, -1.4762e+00, -1.2031e+00, -1.0642e-01, -1.7708e+00,\n",
      "         -4.9757e-01, -9.1510e-01,  1.1391e-01],\n",
      "        [ 4.2221e-01, -3.6004e-02,  3.5615e-01,  2.8996e-01, -6.4366e-02,\n",
      "         -2.7653e-01,  4.1825e-01, -2.6571e-01],\n",
      "        [-2.4106e+00,  4.6104e-01, -1.8178e+00, -2.4587e+00, -1.0296e+00,\n",
      "          2.1159e+00, -2.4158e+00,  2.0995e+00],\n",
      "        [-4.5351e-01,  8.9187e-01, -3.0153e-01, -7.2194e-01,  7.9383e-01,\n",
      "          8.6852e-01, -4.6339e-01,  6.5697e-01],\n",
      "        [-5.2617e-01,  6.2167e-01, -3.6833e-01, -5.8624e-01,  9.5182e-01,\n",
      "          7.4980e-01, -5.3145e-01,  5.9396e-01],\n",
      "        [ 7.6784e-02,  4.5483e-01,  2.1608e-01,  5.2734e-02,  1.5907e+00,\n",
      "          2.3378e-01,  7.5307e-02, -7.7614e-02],\n",
      "        [ 3.3749e-01, -1.4072e+00,  2.0305e-01,  6.5309e-01, -1.7859e-01,\n",
      "         -7.9493e-01,  3.4338e-01, -5.4529e-01],\n",
      "        [-1.1321e+00,  1.7304e+00, -8.2377e-01, -1.5102e+00, -4.3184e-02,\n",
      "          1.8124e+00, -1.1604e+00,  1.6442e+00],\n",
      "        [-1.1606e+00, -9.4793e-01, -1.2277e+00, -2.0467e-02, -4.5813e-01,\n",
      "         -6.5955e-01, -1.1112e+00, -2.7989e-01],\n",
      "        [-2.8388e-01, -1.4027e+00, -4.2539e-01,  2.0399e-01, -5.5275e-01,\n",
      "         -4.5141e-01, -2.8227e-01,  5.1930e-02],\n",
      "        [ 8.8379e-01,  5.8791e-01,  1.0091e+00,  3.5328e-02,  1.1793e+00,\n",
      "          8.3401e-02,  8.6755e-01, -2.1636e-01],\n",
      "        [-7.6156e-01, -9.3101e-01, -9.8013e-01,  7.2625e-02, -1.8493e+00,\n",
      "         -7.3181e-01, -7.3210e-01, -2.9452e-01],\n",
      "        [-1.2753e+00,  6.1762e-01, -1.1483e+00, -1.2656e+00, -5.3623e-01,\n",
      "          1.1098e+00, -1.2814e+00,  1.2696e+00],\n",
      "        [ 6.9584e-01,  7.1788e-01,  8.1495e-01,  1.3811e-01,  1.0728e+00,\n",
      "          9.2162e-02,  6.8293e-01, -1.8176e-01],\n",
      "        [ 2.6169e-01,  1.1203e+00,  4.1734e-01, -1.1203e-02,  1.1617e+00,\n",
      "          5.4426e-01,  2.4904e-01,  2.2890e-01],\n",
      "        [-4.2936e-01,  1.4172e+00, -2.2485e-01, -7.4033e-01,  9.1333e-01,\n",
      "          9.0125e-01, -4.3782e-01,  6.3915e-01],\n",
      "        [ 5.5262e-01, -1.3453e-01,  3.9073e-01,  1.5970e+00, -9.2208e-01,\n",
      "         -1.8679e+00,  5.8021e-01, -1.8088e+00],\n",
      "        [-1.3718e+00, -1.7243e-01, -1.3584e+00, -2.9469e-01, -5.7133e-01,\n",
      "         -1.8931e-01, -1.3290e+00,  2.8647e-01],\n",
      "        [ 1.4804e+00,  4.3416e-01,  1.4255e+00,  6.9158e-01,  3.5297e-01,\n",
      "         -4.2975e-01,  1.4589e+00, -5.8146e-01],\n",
      "        [-1.2631e+00, -1.2357e+00, -1.4964e+00, -1.1178e+00, -1.2649e+00,\n",
      "          4.5718e-01, -1.2492e+00,  9.4669e-01],\n",
      "        [-1.6068e+00,  2.0615e-01, -1.5764e+00, -1.6435e+00, -1.1510e+00,\n",
      "          1.3157e+00, -1.6151e+00,  1.5520e+00],\n",
      "        [-1.1346e-01, -2.6276e+00, -3.5117e-01,  1.4414e+00, -1.4607e+00,\n",
      "         -1.7775e+00, -9.0076e-02, -1.5463e+00],\n",
      "        [-3.8826e-02, -6.5915e-01, -5.4022e-02,  1.9191e-01,  5.0152e-01,\n",
      "         -3.8564e-01, -3.2775e-02, -3.5032e-01],\n",
      "        [-3.6053e-01, -2.9205e-01, -3.7221e-01, -1.3384e-01,  1.2523e-01,\n",
      "         -1.2984e-01, -3.4953e-01, -8.9009e-02],\n",
      "        [-5.2408e-01,  9.1943e-01, -4.1225e-01, -7.6789e-01,  2.1860e-01,\n",
      "          9.2797e-01, -5.3751e-01,  8.4185e-01],\n",
      "        [ 6.8502e-01, -4.4223e-01,  5.8763e-01,  9.5449e-01,  8.3896e-02,\n",
      "         -7.6743e-01,  6.9246e-01, -8.2903e-01],\n",
      "        [ 1.5913e+00, -1.5754e+00,  1.0800e+00,  2.3575e+00, -7.1979e-01,\n",
      "         -1.9514e+00,  1.6349e+00, -1.9590e+00],\n",
      "        [ 6.3946e-01,  5.6295e-01,  6.9273e-01,  1.2682e-01,  6.7558e-01,\n",
      "          1.4851e-01,  6.2364e-01,  1.5077e-02],\n",
      "        [-1.9274e+00,  2.0287e+00, -1.1576e+00, -2.6706e+00, -4.1009e-01,\n",
      "          2.8862e+00, -2.0276e+00,  2.6981e+00]])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(y)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BleveNet(\n",
      "  (fc1): Linear(in_features=11, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=8, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (mish): Mish()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "## Define the NN architecture\n",
    "## NN with 3 hidden layer, s=[26, 256, 256, 256, 8]\n",
    "\n",
    "class BleveNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BleveNet, self).__init__()\n",
    "        # The first hidden layer has 256 neurons\n",
    "        self.fc1 = nn.Linear(X_train_torch.shape[1], 256)\n",
    "        # The second hidden layer has 256 neurons\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        # The third hidden layer has 256 neurons\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        # The final layer has 8 output neuron\n",
    "        self.fc4 = nn.Linear(256, 8)\n",
    "\n",
    "\n",
    "        # Define proportion or neurons to dropout\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # Mish activation\n",
    "        self.mish = nn.Mish()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = self.mish(x)\n",
    "        x = self.dropout(x)\n",
    "        # add second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.mish(x)\n",
    "        x = self.dropout(x)\n",
    "        # add third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        x = self.mish(x)\n",
    "        x = self.dropout(x)\n",
    "        # add final fully connected layers\n",
    "        output = self.fc4(x)\n",
    "       \n",
    "        return output\n",
    "\n",
    "# initialize the NN\n",
    "model = BleveNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.HuberLoss()        # This is the best loss function for my model\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-5)     # This is the best optimizer for my model \n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/500 \tTraining Loss: 0.066347\n",
      "Epoch: 2/500 \tTraining Loss: 0.022474\n",
      "Epoch: 3/500 \tTraining Loss: 0.018219\n",
      "Epoch: 4/500 \tTraining Loss: 0.016469\n",
      "Epoch: 5/500 \tTraining Loss: 0.015355\n",
      "Epoch: 6/500 \tTraining Loss: 0.014302\n",
      "Epoch: 7/500 \tTraining Loss: 0.013555\n",
      "Epoch: 8/500 \tTraining Loss: 0.013139\n",
      "Epoch: 9/500 \tTraining Loss: 0.012535\n",
      "Epoch: 10/500 \tTraining Loss: 0.012545\n",
      "Epoch: 11/500 \tTraining Loss: 0.012102\n",
      "Epoch: 12/500 \tTraining Loss: 0.011653\n",
      "Epoch: 13/500 \tTraining Loss: 0.010708\n",
      "Epoch: 14/500 \tTraining Loss: 0.010907\n",
      "Epoch: 15/500 \tTraining Loss: 0.010800\n",
      "Epoch: 16/500 \tTraining Loss: 0.010549\n",
      "Epoch: 17/500 \tTraining Loss: 0.010662\n",
      "Epoch: 18/500 \tTraining Loss: 0.010267\n",
      "Epoch: 19/500 \tTraining Loss: 0.010528\n",
      "Epoch: 20/500 \tTraining Loss: 0.009349\n",
      "Epoch: 21/500 \tTraining Loss: 0.009568\n",
      "Epoch: 22/500 \tTraining Loss: 0.009563\n",
      "Epoch: 23/500 \tTraining Loss: 0.009252\n",
      "Epoch: 24/500 \tTraining Loss: 0.009471\n",
      "Epoch: 25/500 \tTraining Loss: 0.009247\n",
      "Epoch: 26/500 \tTraining Loss: 0.009137\n",
      "Epoch: 27/500 \tTraining Loss: 0.008933\n",
      "Epoch: 28/500 \tTraining Loss: 0.009418\n",
      "Epoch: 29/500 \tTraining Loss: 0.008983\n",
      "Epoch: 30/500 \tTraining Loss: 0.009394\n",
      "Epoch: 31/500 \tTraining Loss: 0.009695\n",
      "Epoch: 32/500 \tTraining Loss: 0.008795\n",
      "Epoch: 33/500 \tTraining Loss: 0.008608\n",
      "Epoch: 34/500 \tTraining Loss: 0.008810\n",
      "Epoch: 35/500 \tTraining Loss: 0.008356\n",
      "Epoch: 36/500 \tTraining Loss: 0.008531\n",
      "Epoch: 37/500 \tTraining Loss: 0.008817\n",
      "Epoch: 38/500 \tTraining Loss: 0.008497\n",
      "Epoch: 39/500 \tTraining Loss: 0.009073\n",
      "Epoch: 40/500 \tTraining Loss: 0.009150\n",
      "Epoch: 41/500 \tTraining Loss: 0.008662\n",
      "Epoch: 42/500 \tTraining Loss: 0.008699\n",
      "Epoch: 43/500 \tTraining Loss: 0.008333\n",
      "Epoch: 44/500 \tTraining Loss: 0.008568\n",
      "Epoch: 45/500 \tTraining Loss: 0.008228\n",
      "Epoch: 46/500 \tTraining Loss: 0.008826\n",
      "Epoch: 47/500 \tTraining Loss: 0.008640\n",
      "Epoch: 48/500 \tTraining Loss: 0.008252\n",
      "Epoch: 49/500 \tTraining Loss: 0.008411\n",
      "Epoch: 50/500 \tTraining Loss: 0.008568\n",
      "Epoch: 51/500 \tTraining Loss: 0.008556\n",
      "Epoch: 52/500 \tTraining Loss: 0.008044\n",
      "Epoch: 53/500 \tTraining Loss: 0.009007\n",
      "Epoch: 54/500 \tTraining Loss: 0.008269\n",
      "Epoch: 55/500 \tTraining Loss: 0.007834\n",
      "Epoch: 56/500 \tTraining Loss: 0.008477\n",
      "Epoch: 57/500 \tTraining Loss: 0.008316\n",
      "Epoch: 58/500 \tTraining Loss: 0.008225\n",
      "Epoch: 59/500 \tTraining Loss: 0.008666\n",
      "Epoch: 60/500 \tTraining Loss: 0.008286\n",
      "Epoch: 61/500 \tTraining Loss: 0.008560\n",
      "Epoch: 62/500 \tTraining Loss: 0.008690\n",
      "Epoch: 63/500 \tTraining Loss: 0.007936\n",
      "Epoch: 64/500 \tTraining Loss: 0.008343\n",
      "Epoch: 65/500 \tTraining Loss: 0.008346\n",
      "Epoch: 66/500 \tTraining Loss: 0.008215\n",
      "Epoch: 67/500 \tTraining Loss: 0.007897\n",
      "Epoch: 68/500 \tTraining Loss: 0.007909\n",
      "Epoch: 69/500 \tTraining Loss: 0.007710\n",
      "Epoch: 70/500 \tTraining Loss: 0.007970\n",
      "Epoch: 71/500 \tTraining Loss: 0.008549\n",
      "Epoch: 72/500 \tTraining Loss: 0.007813\n",
      "Epoch: 73/500 \tTraining Loss: 0.008132\n",
      "Epoch: 74/500 \tTraining Loss: 0.008118\n",
      "Epoch: 75/500 \tTraining Loss: 0.008055\n",
      "Epoch: 76/500 \tTraining Loss: 0.008432\n",
      "Epoch: 77/500 \tTraining Loss: 0.007826\n",
      "Epoch: 78/500 \tTraining Loss: 0.007806\n",
      "Epoch: 79/500 \tTraining Loss: 0.007734\n",
      "Epoch: 80/500 \tTraining Loss: 0.008145\n",
      "Epoch: 81/500 \tTraining Loss: 0.007700\n",
      "Epoch: 82/500 \tTraining Loss: 0.007534\n",
      "Epoch: 83/500 \tTraining Loss: 0.007740\n",
      "Epoch: 84/500 \tTraining Loss: 0.007897\n",
      "Epoch: 85/500 \tTraining Loss: 0.008057\n",
      "Epoch: 86/500 \tTraining Loss: 0.007815\n",
      "Epoch: 87/500 \tTraining Loss: 0.008007\n",
      "Epoch: 88/500 \tTraining Loss: 0.007963\n",
      "Epoch: 89/500 \tTraining Loss: 0.007913\n",
      "Epoch: 90/500 \tTraining Loss: 0.008934\n",
      "Epoch: 91/500 \tTraining Loss: 0.007571\n",
      "Epoch: 92/500 \tTraining Loss: 0.007804\n",
      "Epoch: 93/500 \tTraining Loss: 0.007589\n",
      "Epoch: 94/500 \tTraining Loss: 0.007690\n",
      "Epoch: 95/500 \tTraining Loss: 0.007313\n",
      "Epoch: 96/500 \tTraining Loss: 0.007682\n",
      "Epoch: 97/500 \tTraining Loss: 0.008093\n",
      "Epoch: 98/500 \tTraining Loss: 0.007749\n",
      "Epoch: 99/500 \tTraining Loss: 0.008400\n",
      "Epoch: 100/500 \tTraining Loss: 0.008138\n",
      "Epoch: 101/500 \tTraining Loss: 0.007880\n",
      "Epoch: 102/500 \tTraining Loss: 0.007810\n",
      "Epoch: 103/500 \tTraining Loss: 0.007626\n",
      "Epoch: 104/500 \tTraining Loss: 0.007558\n",
      "Epoch: 105/500 \tTraining Loss: 0.007772\n",
      "Epoch: 106/500 \tTraining Loss: 0.008027\n",
      "Epoch: 107/500 \tTraining Loss: 0.007411\n",
      "Epoch: 108/500 \tTraining Loss: 0.007334\n",
      "Epoch: 109/500 \tTraining Loss: 0.007290\n",
      "Epoch: 110/500 \tTraining Loss: 0.007571\n",
      "Epoch: 111/500 \tTraining Loss: 0.007509\n",
      "Epoch: 112/500 \tTraining Loss: 0.007908\n",
      "Epoch: 113/500 \tTraining Loss: 0.007726\n",
      "Epoch: 114/500 \tTraining Loss: 0.007715\n",
      "Epoch: 115/500 \tTraining Loss: 0.007995\n",
      "Epoch: 116/500 \tTraining Loss: 0.007852\n",
      "Epoch: 117/500 \tTraining Loss: 0.007357\n",
      "Epoch: 118/500 \tTraining Loss: 0.007935\n",
      "Epoch: 119/500 \tTraining Loss: 0.008012\n",
      "Epoch: 120/500 \tTraining Loss: 0.007618\n",
      "Epoch: 121/500 \tTraining Loss: 0.007972\n",
      "Epoch: 122/500 \tTraining Loss: 0.008122\n",
      "Epoch: 123/500 \tTraining Loss: 0.007762\n",
      "Epoch: 124/500 \tTraining Loss: 0.007922\n",
      "Epoch: 125/500 \tTraining Loss: 0.007423\n",
      "Epoch: 126/500 \tTraining Loss: 0.007392\n",
      "Epoch: 127/500 \tTraining Loss: 0.007485\n",
      "Epoch: 128/500 \tTraining Loss: 0.007487\n",
      "Epoch: 129/500 \tTraining Loss: 0.007700\n",
      "Epoch: 130/500 \tTraining Loss: 0.007947\n",
      "Epoch: 131/500 \tTraining Loss: 0.007708\n",
      "Epoch: 132/500 \tTraining Loss: 0.007529\n",
      "Epoch: 133/500 \tTraining Loss: 0.007478\n",
      "Epoch: 134/500 \tTraining Loss: 0.007478\n",
      "Epoch: 135/500 \tTraining Loss: 0.008095\n",
      "Epoch: 136/500 \tTraining Loss: 0.007875\n",
      "Epoch: 137/500 \tTraining Loss: 0.007875\n",
      "Epoch: 138/500 \tTraining Loss: 0.008170\n",
      "Epoch: 139/500 \tTraining Loss: 0.007566\n",
      "Epoch: 140/500 \tTraining Loss: 0.007551\n",
      "Epoch: 141/500 \tTraining Loss: 0.007366\n",
      "Epoch: 142/500 \tTraining Loss: 0.007780\n",
      "Epoch: 143/500 \tTraining Loss: 0.007935\n",
      "Epoch: 144/500 \tTraining Loss: 0.007606\n",
      "Epoch: 145/500 \tTraining Loss: 0.008114\n",
      "Epoch: 146/500 \tTraining Loss: 0.008265\n",
      "Epoch: 147/500 \tTraining Loss: 0.007971\n",
      "Epoch: 148/500 \tTraining Loss: 0.007307\n",
      "Epoch: 149/500 \tTraining Loss: 0.007324\n",
      "Epoch: 150/500 \tTraining Loss: 0.007325\n",
      "Epoch: 151/500 \tTraining Loss: 0.007399\n",
      "Epoch: 152/500 \tTraining Loss: 0.007431\n",
      "Epoch: 153/500 \tTraining Loss: 0.007329\n",
      "Epoch: 154/500 \tTraining Loss: 0.007362\n",
      "Epoch: 155/500 \tTraining Loss: 0.007311\n",
      "Epoch: 156/500 \tTraining Loss: 0.007576\n",
      "Epoch: 157/500 \tTraining Loss: 0.007538\n",
      "Epoch: 158/500 \tTraining Loss: 0.007805\n",
      "Epoch: 159/500 \tTraining Loss: 0.007618\n",
      "Epoch: 160/500 \tTraining Loss: 0.007593\n",
      "Epoch: 161/500 \tTraining Loss: 0.007176\n",
      "Epoch: 162/500 \tTraining Loss: 0.007788\n",
      "Epoch: 163/500 \tTraining Loss: 0.007215\n",
      "Epoch: 164/500 \tTraining Loss: 0.007335\n",
      "Epoch: 165/500 \tTraining Loss: 0.007217\n",
      "Epoch: 166/500 \tTraining Loss: 0.007259\n",
      "Epoch: 167/500 \tTraining Loss: 0.007861\n",
      "Epoch: 168/500 \tTraining Loss: 0.007770\n",
      "Epoch: 169/500 \tTraining Loss: 0.007909\n",
      "Epoch: 170/500 \tTraining Loss: 0.007351\n",
      "Epoch: 171/500 \tTraining Loss: 0.007731\n",
      "Epoch: 172/500 \tTraining Loss: 0.007580\n",
      "Epoch: 173/500 \tTraining Loss: 0.007904\n",
      "Epoch: 174/500 \tTraining Loss: 0.008049\n",
      "Epoch: 175/500 \tTraining Loss: 0.007211\n",
      "Epoch: 176/500 \tTraining Loss: 0.007609\n",
      "Epoch: 177/500 \tTraining Loss: 0.007508\n",
      "Epoch: 178/500 \tTraining Loss: 0.007286\n",
      "Epoch: 179/500 \tTraining Loss: 0.007461\n",
      "Epoch: 180/500 \tTraining Loss: 0.007883\n",
      "Epoch: 181/500 \tTraining Loss: 0.007672\n",
      "Epoch: 182/500 \tTraining Loss: 0.007513\n",
      "Epoch: 183/500 \tTraining Loss: 0.007363\n",
      "Epoch: 184/500 \tTraining Loss: 0.007618\n",
      "Epoch: 185/500 \tTraining Loss: 0.007759\n",
      "Epoch: 186/500 \tTraining Loss: 0.007633\n",
      "Epoch: 187/500 \tTraining Loss: 0.007934\n",
      "Epoch: 188/500 \tTraining Loss: 0.007470\n",
      "Epoch: 189/500 \tTraining Loss: 0.007102\n",
      "Epoch: 190/500 \tTraining Loss: 0.007151\n",
      "Epoch: 191/500 \tTraining Loss: 0.006953\n",
      "Epoch: 192/500 \tTraining Loss: 0.007620\n",
      "Epoch: 193/500 \tTraining Loss: 0.007387\n",
      "Epoch: 194/500 \tTraining Loss: 0.007189\n",
      "Epoch: 195/500 \tTraining Loss: 0.007970\n",
      "Epoch: 196/500 \tTraining Loss: 0.007371\n",
      "Epoch: 197/500 \tTraining Loss: 0.006962\n",
      "Epoch: 198/500 \tTraining Loss: 0.007851\n",
      "Epoch: 199/500 \tTraining Loss: 0.007200\n",
      "Epoch: 200/500 \tTraining Loss: 0.007218\n",
      "Epoch: 201/500 \tTraining Loss: 0.008185\n",
      "Epoch: 202/500 \tTraining Loss: 0.007929\n",
      "Epoch: 203/500 \tTraining Loss: 0.007894\n",
      "Epoch: 204/500 \tTraining Loss: 0.007863\n",
      "Epoch: 205/500 \tTraining Loss: 0.008860\n",
      "Epoch: 206/500 \tTraining Loss: 0.007456\n",
      "Epoch: 207/500 \tTraining Loss: 0.007089\n",
      "Epoch: 208/500 \tTraining Loss: 0.007183\n",
      "Epoch: 209/500 \tTraining Loss: 0.007149\n",
      "Epoch: 210/500 \tTraining Loss: 0.007385\n",
      "Epoch: 211/500 \tTraining Loss: 0.007529\n",
      "Epoch: 212/500 \tTraining Loss: 0.007696\n",
      "Epoch: 213/500 \tTraining Loss: 0.007551\n",
      "Epoch: 214/500 \tTraining Loss: 0.007531\n",
      "Epoch: 215/500 \tTraining Loss: 0.006868\n",
      "Epoch: 216/500 \tTraining Loss: 0.007300\n",
      "Epoch: 217/500 \tTraining Loss: 0.007315\n",
      "Epoch: 218/500 \tTraining Loss: 0.007708\n",
      "Epoch: 219/500 \tTraining Loss: 0.007234\n",
      "Epoch: 220/500 \tTraining Loss: 0.007237\n",
      "Epoch: 221/500 \tTraining Loss: 0.007615\n",
      "Epoch: 222/500 \tTraining Loss: 0.007383\n",
      "Epoch: 223/500 \tTraining Loss: 0.007209\n",
      "Epoch: 224/500 \tTraining Loss: 0.007675\n",
      "Epoch: 225/500 \tTraining Loss: 0.007380\n",
      "Epoch: 226/500 \tTraining Loss: 0.006935\n",
      "Epoch: 227/500 \tTraining Loss: 0.007891\n",
      "Epoch: 228/500 \tTraining Loss: 0.007580\n",
      "Epoch: 229/500 \tTraining Loss: 0.007692\n",
      "Epoch: 230/500 \tTraining Loss: 0.007418\n",
      "Epoch: 231/500 \tTraining Loss: 0.007386\n",
      "Epoch: 232/500 \tTraining Loss: 0.007635\n",
      "Epoch: 233/500 \tTraining Loss: 0.007383\n",
      "Epoch: 234/500 \tTraining Loss: 0.007141\n",
      "Epoch: 235/500 \tTraining Loss: 0.007463\n",
      "Epoch: 236/500 \tTraining Loss: 0.007636\n",
      "Epoch: 237/500 \tTraining Loss: 0.007343\n",
      "Epoch: 238/500 \tTraining Loss: 0.007532\n",
      "Epoch: 239/500 \tTraining Loss: 0.007136\n",
      "Epoch: 240/500 \tTraining Loss: 0.007429\n",
      "Epoch: 241/500 \tTraining Loss: 0.007531\n",
      "Epoch: 242/500 \tTraining Loss: 0.007255\n",
      "Epoch: 243/500 \tTraining Loss: 0.007350\n",
      "Epoch: 244/500 \tTraining Loss: 0.008004\n",
      "Epoch: 245/500 \tTraining Loss: 0.007520\n",
      "Epoch: 246/500 \tTraining Loss: 0.006888\n",
      "Epoch: 247/500 \tTraining Loss: 0.007201\n",
      "Epoch: 248/500 \tTraining Loss: 0.007393\n",
      "Epoch: 249/500 \tTraining Loss: 0.007096\n",
      "Epoch: 250/500 \tTraining Loss: 0.007344\n",
      "Epoch: 251/500 \tTraining Loss: 0.007716\n",
      "Epoch: 252/500 \tTraining Loss: 0.007130\n",
      "Epoch: 253/500 \tTraining Loss: 0.007335\n",
      "Epoch: 254/500 \tTraining Loss: 0.007737\n",
      "Epoch: 255/500 \tTraining Loss: 0.007135\n",
      "Epoch: 256/500 \tTraining Loss: 0.007568\n",
      "Epoch: 257/500 \tTraining Loss: 0.007447\n",
      "Epoch: 258/500 \tTraining Loss: 0.007309\n",
      "Epoch: 259/500 \tTraining Loss: 0.007031\n",
      "Epoch: 260/500 \tTraining Loss: 0.007363\n",
      "Epoch: 261/500 \tTraining Loss: 0.007528\n",
      "Epoch: 262/500 \tTraining Loss: 0.007414\n",
      "Epoch: 263/500 \tTraining Loss: 0.007267\n",
      "Epoch: 264/500 \tTraining Loss: 0.008035\n",
      "Epoch: 265/500 \tTraining Loss: 0.007420\n",
      "Epoch: 266/500 \tTraining Loss: 0.007455\n",
      "Epoch: 267/500 \tTraining Loss: 0.007117\n",
      "Epoch: 268/500 \tTraining Loss: 0.007688\n",
      "Epoch: 269/500 \tTraining Loss: 0.007618\n",
      "Epoch: 270/500 \tTraining Loss: 0.007485\n",
      "Epoch: 271/500 \tTraining Loss: 0.007162\n",
      "Epoch: 272/500 \tTraining Loss: 0.007341\n",
      "Epoch: 273/500 \tTraining Loss: 0.007714\n",
      "Epoch: 274/500 \tTraining Loss: 0.007717\n",
      "Epoch: 275/500 \tTraining Loss: 0.007532\n",
      "Epoch: 276/500 \tTraining Loss: 0.007484\n",
      "Epoch: 277/500 \tTraining Loss: 0.007038\n",
      "Epoch: 278/500 \tTraining Loss: 0.007278\n",
      "Epoch: 279/500 \tTraining Loss: 0.007310\n",
      "Epoch: 280/500 \tTraining Loss: 0.007144\n",
      "Epoch: 281/500 \tTraining Loss: 0.007628\n",
      "Epoch: 282/500 \tTraining Loss: 0.007590\n",
      "Epoch: 283/500 \tTraining Loss: 0.007718\n",
      "Epoch: 284/500 \tTraining Loss: 0.007535\n",
      "Epoch: 285/500 \tTraining Loss: 0.007620\n",
      "Epoch: 286/500 \tTraining Loss: 0.007905\n",
      "Epoch: 287/500 \tTraining Loss: 0.007057\n",
      "Epoch: 288/500 \tTraining Loss: 0.007090\n",
      "Epoch: 289/500 \tTraining Loss: 0.007238\n",
      "Epoch: 290/500 \tTraining Loss: 0.007487\n",
      "Epoch: 291/500 \tTraining Loss: 0.007215\n",
      "Epoch: 292/500 \tTraining Loss: 0.007471\n",
      "Epoch: 293/500 \tTraining Loss: 0.007162\n",
      "Epoch: 294/500 \tTraining Loss: 0.007140\n",
      "Epoch: 295/500 \tTraining Loss: 0.006966\n",
      "Epoch: 296/500 \tTraining Loss: 0.007855\n",
      "Epoch: 297/500 \tTraining Loss: 0.007282\n",
      "Epoch: 298/500 \tTraining Loss: 0.007397\n",
      "Epoch: 299/500 \tTraining Loss: 0.007596\n",
      "Epoch: 300/500 \tTraining Loss: 0.007584\n",
      "Epoch: 301/500 \tTraining Loss: 0.007811\n",
      "Epoch: 302/500 \tTraining Loss: 0.007678\n",
      "Epoch: 303/500 \tTraining Loss: 0.007030\n",
      "Epoch: 304/500 \tTraining Loss: 0.006911\n",
      "Epoch: 305/500 \tTraining Loss: 0.006914\n",
      "Epoch: 306/500 \tTraining Loss: 0.007524\n",
      "Epoch: 307/500 \tTraining Loss: 0.007110\n",
      "Epoch: 308/500 \tTraining Loss: 0.007524\n",
      "Epoch: 309/500 \tTraining Loss: 0.007474\n",
      "Epoch: 310/500 \tTraining Loss: 0.007455\n",
      "Epoch: 311/500 \tTraining Loss: 0.007438\n",
      "Epoch: 312/500 \tTraining Loss: 0.007395\n",
      "Epoch: 313/500 \tTraining Loss: 0.007156\n",
      "Epoch: 314/500 \tTraining Loss: 0.007309\n",
      "Epoch: 315/500 \tTraining Loss: 0.006968\n",
      "Epoch: 316/500 \tTraining Loss: 0.007166\n",
      "Epoch: 317/500 \tTraining Loss: 0.007407\n",
      "Epoch: 318/500 \tTraining Loss: 0.007032\n",
      "Epoch: 319/500 \tTraining Loss: 0.007782\n",
      "Epoch: 320/500 \tTraining Loss: 0.007717\n",
      "Epoch: 321/500 \tTraining Loss: 0.007622\n",
      "Epoch: 322/500 \tTraining Loss: 0.007295\n",
      "Epoch: 323/500 \tTraining Loss: 0.007202\n",
      "Epoch: 324/500 \tTraining Loss: 0.007225\n",
      "Epoch: 325/500 \tTraining Loss: 0.007181\n",
      "Epoch: 326/500 \tTraining Loss: 0.007302\n",
      "Epoch: 327/500 \tTraining Loss: 0.007379\n",
      "Epoch: 328/500 \tTraining Loss: 0.007062\n",
      "Epoch: 329/500 \tTraining Loss: 0.007210\n",
      "Epoch: 330/500 \tTraining Loss: 0.007127\n",
      "Epoch: 331/500 \tTraining Loss: 0.007082\n",
      "Epoch: 332/500 \tTraining Loss: 0.007113\n",
      "Epoch: 333/500 \tTraining Loss: 0.007144\n",
      "Epoch: 334/500 \tTraining Loss: 0.008307\n",
      "Epoch: 335/500 \tTraining Loss: 0.007397\n",
      "Epoch: 336/500 \tTraining Loss: 0.007271\n",
      "Epoch: 337/500 \tTraining Loss: 0.007180\n",
      "Epoch: 338/500 \tTraining Loss: 0.007812\n",
      "Epoch: 339/500 \tTraining Loss: 0.007780\n",
      "Epoch: 340/500 \tTraining Loss: 0.007791\n",
      "Epoch: 341/500 \tTraining Loss: 0.007901\n",
      "Epoch: 342/500 \tTraining Loss: 0.007371\n",
      "Epoch: 343/500 \tTraining Loss: 0.007286\n",
      "Epoch: 344/500 \tTraining Loss: 0.007361\n",
      "Epoch: 345/500 \tTraining Loss: 0.007240\n",
      "Epoch: 346/500 \tTraining Loss: 0.007342\n",
      "Epoch: 347/500 \tTraining Loss: 0.007212\n",
      "Epoch: 348/500 \tTraining Loss: 0.007126\n",
      "Epoch: 349/500 \tTraining Loss: 0.007310\n",
      "Epoch: 350/500 \tTraining Loss: 0.006861\n",
      "Epoch: 351/500 \tTraining Loss: 0.007783\n",
      "Epoch: 352/500 \tTraining Loss: 0.007020\n",
      "Epoch: 353/500 \tTraining Loss: 0.007739\n",
      "Epoch: 354/500 \tTraining Loss: 0.007877\n",
      "Epoch: 355/500 \tTraining Loss: 0.007155\n",
      "Epoch: 356/500 \tTraining Loss: 0.007423\n",
      "Epoch: 357/500 \tTraining Loss: 0.007451\n",
      "Epoch: 358/500 \tTraining Loss: 0.007251\n",
      "Epoch: 359/500 \tTraining Loss: 0.006959\n",
      "Epoch: 360/500 \tTraining Loss: 0.007029\n",
      "Epoch: 361/500 \tTraining Loss: 0.007331\n",
      "Epoch: 362/500 \tTraining Loss: 0.007183\n",
      "Epoch: 363/500 \tTraining Loss: 0.007310\n",
      "Epoch: 364/500 \tTraining Loss: 0.007280\n",
      "Epoch: 365/500 \tTraining Loss: 0.007114\n",
      "Epoch: 366/500 \tTraining Loss: 0.007870\n",
      "Epoch: 367/500 \tTraining Loss: 0.007375\n",
      "Epoch: 368/500 \tTraining Loss: 0.007051\n",
      "Epoch: 369/500 \tTraining Loss: 0.007160\n",
      "Epoch: 370/500 \tTraining Loss: 0.008044\n",
      "Epoch: 371/500 \tTraining Loss: 0.007163\n",
      "Epoch: 372/500 \tTraining Loss: 0.007064\n",
      "Epoch: 373/500 \tTraining Loss: 0.007717\n",
      "Epoch: 374/500 \tTraining Loss: 0.007534\n",
      "Epoch: 375/500 \tTraining Loss: 0.007093\n",
      "Epoch: 376/500 \tTraining Loss: 0.007082\n",
      "Epoch: 377/500 \tTraining Loss: 0.007088\n",
      "Epoch: 378/500 \tTraining Loss: 0.006862\n",
      "Epoch: 379/500 \tTraining Loss: 0.007495\n",
      "Epoch: 380/500 \tTraining Loss: 0.007555\n",
      "Epoch: 381/500 \tTraining Loss: 0.007216\n",
      "Epoch: 382/500 \tTraining Loss: 0.007476\n",
      "Epoch: 383/500 \tTraining Loss: 0.007604\n",
      "Epoch: 384/500 \tTraining Loss: 0.007392\n",
      "Epoch: 385/500 \tTraining Loss: 0.007801\n",
      "Epoch: 386/500 \tTraining Loss: 0.007153\n",
      "Epoch: 387/500 \tTraining Loss: 0.007114\n",
      "Epoch: 388/500 \tTraining Loss: 0.007228\n",
      "Epoch: 389/500 \tTraining Loss: 0.007402\n",
      "Epoch: 390/500 \tTraining Loss: 0.007559\n",
      "Epoch: 391/500 \tTraining Loss: 0.007281\n",
      "Epoch: 392/500 \tTraining Loss: 0.006843\n",
      "Epoch: 393/500 \tTraining Loss: 0.006978\n",
      "Epoch: 394/500 \tTraining Loss: 0.007094\n",
      "Epoch: 395/500 \tTraining Loss: 0.006908\n",
      "Epoch: 396/500 \tTraining Loss: 0.006969\n",
      "Epoch: 397/500 \tTraining Loss: 0.007769\n",
      "Epoch: 398/500 \tTraining Loss: 0.007091\n",
      "Epoch: 399/500 \tTraining Loss: 0.007087\n",
      "Epoch: 400/500 \tTraining Loss: 0.007201\n",
      "Epoch: 401/500 \tTraining Loss: 0.007400\n",
      "Epoch: 402/500 \tTraining Loss: 0.007397\n",
      "Epoch: 403/500 \tTraining Loss: 0.007098\n",
      "Epoch: 404/500 \tTraining Loss: 0.007113\n",
      "Epoch: 405/500 \tTraining Loss: 0.007321\n",
      "Epoch: 406/500 \tTraining Loss: 0.006976\n",
      "Epoch: 407/500 \tTraining Loss: 0.007295\n",
      "Epoch: 408/500 \tTraining Loss: 0.007257\n",
      "Epoch: 409/500 \tTraining Loss: 0.007233\n",
      "Epoch: 410/500 \tTraining Loss: 0.007396\n",
      "Epoch: 411/500 \tTraining Loss: 0.007106\n",
      "Epoch: 412/500 \tTraining Loss: 0.007290\n",
      "Epoch: 413/500 \tTraining Loss: 0.007362\n",
      "Epoch: 414/500 \tTraining Loss: 0.007732\n",
      "Epoch: 415/500 \tTraining Loss: 0.007262\n",
      "Epoch: 416/500 \tTraining Loss: 0.007207\n",
      "Epoch: 417/500 \tTraining Loss: 0.007215\n",
      "Epoch: 418/500 \tTraining Loss: 0.007559\n",
      "Epoch: 419/500 \tTraining Loss: 0.007296\n",
      "Epoch: 420/500 \tTraining Loss: 0.006996\n",
      "Epoch: 421/500 \tTraining Loss: 0.006989\n",
      "Epoch: 422/500 \tTraining Loss: 0.006989\n",
      "Epoch: 423/500 \tTraining Loss: 0.007799\n",
      "Epoch: 424/500 \tTraining Loss: 0.006926\n",
      "Epoch: 425/500 \tTraining Loss: 0.006869\n",
      "Epoch: 426/500 \tTraining Loss: 0.007099\n",
      "Epoch: 427/500 \tTraining Loss: 0.007086\n",
      "Epoch: 428/500 \tTraining Loss: 0.007054\n",
      "Epoch: 429/500 \tTraining Loss: 0.007544\n",
      "Epoch: 430/500 \tTraining Loss: 0.007215\n",
      "Epoch: 431/500 \tTraining Loss: 0.006907\n",
      "Epoch: 432/500 \tTraining Loss: 0.007179\n",
      "Epoch: 433/500 \tTraining Loss: 0.007448\n",
      "Epoch: 434/500 \tTraining Loss: 0.007069\n",
      "Epoch: 435/500 \tTraining Loss: 0.007319\n",
      "Epoch: 436/500 \tTraining Loss: 0.006945\n",
      "Epoch: 437/500 \tTraining Loss: 0.006959\n",
      "Epoch: 438/500 \tTraining Loss: 0.007502\n",
      "Epoch: 439/500 \tTraining Loss: 0.007472\n",
      "Epoch: 440/500 \tTraining Loss: 0.007226\n",
      "Epoch: 441/500 \tTraining Loss: 0.007812\n",
      "Epoch: 442/500 \tTraining Loss: 0.007229\n",
      "Epoch: 443/500 \tTraining Loss: 0.007227\n",
      "Epoch: 444/500 \tTraining Loss: 0.007443\n",
      "Epoch: 445/500 \tTraining Loss: 0.007496\n",
      "Epoch: 446/500 \tTraining Loss: 0.007294\n",
      "Epoch: 447/500 \tTraining Loss: 0.007422\n",
      "Epoch: 448/500 \tTraining Loss: 0.007230\n",
      "Epoch: 449/500 \tTraining Loss: 0.007746\n",
      "Epoch: 450/500 \tTraining Loss: 0.007255\n",
      "Epoch: 451/500 \tTraining Loss: 0.006690\n",
      "Epoch: 452/500 \tTraining Loss: 0.006959\n",
      "Epoch: 453/500 \tTraining Loss: 0.007745\n",
      "Epoch: 454/500 \tTraining Loss: 0.007197\n",
      "Epoch: 455/500 \tTraining Loss: 0.006979\n",
      "Epoch: 456/500 \tTraining Loss: 0.007454\n",
      "Epoch: 457/500 \tTraining Loss: 0.007307\n",
      "Epoch: 458/500 \tTraining Loss: 0.007743\n",
      "Epoch: 459/500 \tTraining Loss: 0.007089\n",
      "Epoch: 460/500 \tTraining Loss: 0.007168\n",
      "Epoch: 461/500 \tTraining Loss: 0.007291\n",
      "Epoch: 462/500 \tTraining Loss: 0.007052\n",
      "Epoch: 463/500 \tTraining Loss: 0.006818\n",
      "Epoch: 464/500 \tTraining Loss: 0.006838\n",
      "Epoch: 465/500 \tTraining Loss: 0.007087\n",
      "Epoch: 466/500 \tTraining Loss: 0.007215\n",
      "Epoch: 467/500 \tTraining Loss: 0.007266\n",
      "Epoch: 468/500 \tTraining Loss: 0.007049\n",
      "Epoch: 469/500 \tTraining Loss: 0.007862\n",
      "Epoch: 470/500 \tTraining Loss: 0.007195\n",
      "Epoch: 471/500 \tTraining Loss: 0.007473\n",
      "Epoch: 472/500 \tTraining Loss: 0.007278\n",
      "Epoch: 473/500 \tTraining Loss: 0.007165\n",
      "Epoch: 474/500 \tTraining Loss: 0.007558\n",
      "Epoch: 475/500 \tTraining Loss: 0.007410\n",
      "Epoch: 476/500 \tTraining Loss: 0.007235\n",
      "Epoch: 477/500 \tTraining Loss: 0.007097\n",
      "Epoch: 478/500 \tTraining Loss: 0.007245\n",
      "Epoch: 479/500 \tTraining Loss: 0.007044\n",
      "Epoch: 480/500 \tTraining Loss: 0.007586\n",
      "Epoch: 481/500 \tTraining Loss: 0.007646\n",
      "Epoch: 482/500 \tTraining Loss: 0.007996\n",
      "Epoch: 483/500 \tTraining Loss: 0.006890\n",
      "Epoch: 484/500 \tTraining Loss: 0.006918\n",
      "Epoch: 485/500 \tTraining Loss: 0.007153\n",
      "Epoch: 486/500 \tTraining Loss: 0.007124\n",
      "Epoch: 487/500 \tTraining Loss: 0.007387\n",
      "Epoch: 488/500 \tTraining Loss: 0.007168\n",
      "Epoch: 489/500 \tTraining Loss: 0.007132\n",
      "Epoch: 490/500 \tTraining Loss: 0.007404\n",
      "Epoch: 491/500 \tTraining Loss: 0.006766\n",
      "Epoch: 492/500 \tTraining Loss: 0.007059\n",
      "Epoch: 493/500 \tTraining Loss: 0.007385\n",
      "Epoch: 494/500 \tTraining Loss: 0.007440\n",
      "Epoch: 495/500 \tTraining Loss: 0.007160\n",
      "Epoch: 496/500 \tTraining Loss: 0.006955\n",
      "Epoch: 497/500 \tTraining Loss: 0.007164\n",
      "Epoch: 498/500 \tTraining Loss: 0.007421\n",
      "Epoch: 499/500 \tTraining Loss: 0.007639\n",
      "Epoch: 500/500 \tTraining Loss: 0.007225\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200 \n",
    "\n",
    "model.to(device)    # bring the model to gpu\n",
    "model.train()       # prep model for training\n",
    "\n",
    "train_losses = []\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        #bring data and target to gpu\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    print('Epoch: {}/{} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1,\n",
    "        n_epochs, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "\n",
    "# Save the model  -- I already saved this and submitted\n",
    "# torch.save(model.state_dict(), 'PyTorch_Model/NN_mish_Drop_L2_Huber_200Epoch_Quantile.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the saved state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.3745447   0.39098334  1.541328   ... -0.3063252   1.3605843\n",
      "  -0.5498472 ]\n",
      " [-1.0866694  -0.32501316 -1.165355   ...  0.7668543  -1.1000178\n",
      "   1.0265232 ]\n",
      " [-0.08239491  0.81565875  0.05113338 ...  0.5583257  -0.08981794\n",
      "   0.34370404]\n",
      " ...\n",
      " [ 0.6009814  -0.14309058  0.5339893  ... -0.49559617  0.6055057\n",
      "  -0.4972145 ]\n",
      " [-1.2923795   1.4716097  -1.0610684  ...  1.9973941  -1.3589212\n",
      "   1.9229813 ]\n",
      " [-0.2747004  -1.994422   -0.48124322 ... -1.3523613  -0.23567753\n",
      "  -1.0565274 ]]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('PyTorch_Model/NN_mish_Drop_L2_Huber_200Epoch_Quantile.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()           # prep model for *evaluation*\n",
    "model.to(device)\n",
    "with torch.no_grad():  # turn off gradient to save memory\n",
    "    y_predNN_torch = model(X_test_torch.to(device))\n",
    "\n",
    "y_predNN_normal = y_predNN_torch.cpu().numpy()     # convert to numpy array\n",
    "y_test_normal = y_test_torch.cpu().numpy()\n",
    "print(y_predNN_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network scores in normal distribution: r2 = 0.9912283571357272, mape = 0.44284647703170776\n"
     ]
    }
   ],
   "source": [
    "mape = mean_absolute_percentage_error(y_test_normal, y_predNN_normal)\n",
    "r2 = r2_score(y_test_normal, y_predNN_normal)\n",
    "print(f\"Neural Network scores in normal distribution: r2 = {r2}, mape = {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network scores in actual distribution: r2 = 0.9915125354390546, mape = 0.03392672898142321\n"
     ]
    }
   ],
   "source": [
    "y_predNN = quantile.inverse_transform(y_predNN_normal)\n",
    "mape = mean_absolute_percentage_error(y_test, y_predNN.astype('float64'))\n",
    "r2 = r2_score(y_test, y_predNN)\n",
    "\n",
    "print(f\"Neural Network scores in actual distribution: r2 = {r2}, mape = {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.40293893, 0.18692414, 0.06679379, 1.5731983 , 1.1409516 ,\n",
       "       0.07116825, 0.08229017, 0.11094648, 0.0684045 , 0.01790491,\n",
       "       0.076277  , 0.28509498, 0.13568424, 0.17459093, 1.2293346 ,\n",
       "       0.14248492, 0.21850143, 0.06556914, 0.22037481, 0.10233872,\n",
       "       0.11252213, 0.19777906, 0.69064593, 0.15893647, 0.09778114,\n",
       "       0.06329427, 0.52257925, 0.03382068, 0.0137039 , 0.08567823,\n",
       "       0.06398399, 0.49113005, 0.31454965, 0.22651668, 0.17856087,\n",
       "       1.3262647 , 0.9491288 , 0.08730866, 0.17905883, 1.7284456 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predNN[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39918807, 0.18489788, 0.07076792, 1.603528  , 1.0806105 ,\n",
       "       0.06908134, 0.08818348, 0.10897039, 0.06826332, 0.01860524,\n",
       "       0.08134046, 0.28987974, 0.12975076, 0.17156978, 1.2115381 ,\n",
       "       0.15436813, 0.22705919, 0.06763056, 0.22765867, 0.10868868,\n",
       "       0.11713055, 0.19952142, 0.66621888, 0.15975925, 0.09596858,\n",
       "       0.06414993, 0.59720933, 0.03660972, 0.01633277, 0.08981635,\n",
       "       0.06839496, 0.49123496, 0.30692583, 0.2290165 , 0.18374079,\n",
       "       1.2546521 , 0.83790588, 0.08270955, 0.18826026, 1.7111793 ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1:41,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.36676340e-02, 1.85481800e-02, 1.16362342e-01, ...,\n",
       "        3.87540700e+01, 9.85093640e-02, 8.02679810e-02],\n",
       "       [1.88341180e-02, 1.49853250e-02, 3.32267860e-02, ...,\n",
       "        1.10768800e+02, 2.12428740e-02, 3.99188070e-01],\n",
       "       [4.82065450e-02, 2.26087420e-02, 7.29268908e-02, ...,\n",
       "        9.14076770e+01, 5.16740420e-02, 1.84897880e-01],\n",
       "       ...,\n",
       "       [7.48656170e-02, 1.68266440e-02, 9.27517236e-02, ...,\n",
       "        3.51032030e+01, 7.93257060e-02, 9.26706940e-02],\n",
       "       [1.48923780e-02, 2.82288120e-02, 3.50396958e-02, ...,\n",
       "        3.61880250e+02, 1.65773410e-02, 1.17031230e+00],\n",
       "       [4.29282640e-02, 7.86490400e-03, 5.24556696e-02, ...,\n",
       "        1.25165350e+01, 4.73894250e-02, 4.77122370e-02]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3816304 ,  0.28268605,  1.4829322 , ..., -0.30745506,\n",
       "         1.3684012 , -0.5699484 ],\n",
       "       [-1.0927829 , -0.31069657, -1.1450505 , ...,  0.7362669 ,\n",
       "        -1.0969453 ,  1.0197092 ],\n",
       "       [-0.11125448,  0.76144725,  0.03355394, ...,  0.54064226,\n",
       "        -0.11799674,  0.33428347],\n",
       "       ...,\n",
       "       [ 0.60861874,  0.03356267,  0.5383628 , ..., -0.40858847,\n",
       "         0.60639787, -0.4104572 ],\n",
       "       [-1.3439791 ,  1.4611413 , -1.0646684 , ...,  1.977942  ,\n",
       "        -1.3777595 ,  1.9264768 ],\n",
       "       [-0.24901262, -2.1335402 , -0.47832558, ..., -1.3961294 ,\n",
       "        -0.22524492, -1.1426171 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3745447 ,  0.39098334,  1.541328  , ..., -0.3063252 ,\n",
       "         1.3605843 , -0.5498472 ],\n",
       "       [-1.0866694 , -0.32501316, -1.165355  , ...,  0.7668543 ,\n",
       "        -1.1000178 ,  1.0265232 ],\n",
       "       [-0.08239491,  0.81565875,  0.05113338, ...,  0.5583257 ,\n",
       "        -0.08981794,  0.34370404],\n",
       "       ...,\n",
       "       [ 0.6009814 , -0.14309058,  0.5339893 , ..., -0.49559617,\n",
       "         0.6055057 , -0.4972145 ],\n",
       "       [-1.2923795 ,  1.4716097 , -1.0610684 , ...,  1.9973941 ,\n",
       "        -1.3589212 ,  1.9229813 ],\n",
       "       [-0.2747004 , -1.994422  , -0.48124322, ..., -1.3523613 ,\n",
       "        -0.23567753, -1.0565274 ]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predNN_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Pressure\n",
      "[-0.05803021 -0.18981701 -0.11518781 ... -0.05726141 -0.302009\n",
      " -0.04012128]\n",
      "\n",
      "Positive Impulse\n",
      "[ 38.80099  113.936646  92.900665 ...  32.103645 366.63266   13.11972 ]\n",
      "\n",
      "Positive Pressure\n",
      "[0.08168865 0.40293893 0.18692414 ... 0.08577602 1.1673032  0.0519053 ]\n"
     ]
    }
   ],
   "source": [
    "# Negative Pressure\n",
    "print(\"Negative Pressure\")\n",
    "print(y_predNN[:,3])\n",
    "print()\n",
    "\n",
    "\n",
    "# Positive Impulse\n",
    "print(\"Positive Impulse\")\n",
    "print(y_predNN[:,5])\n",
    "print()\n",
    "\n",
    "# Positive Pressure\n",
    "print(\"Positive Pressure\")\n",
    "print(y_predNN[:,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
